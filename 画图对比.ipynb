{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5642, Val: 0.5226, Test: 0.5265\n",
      "Epoch: 02, Loss: 0.5260, Val: 0.5356, Test: 0.5402\n",
      "Epoch: 03, Loss: 0.5091, Val: 0.5633, Test: 0.5698\n",
      "Epoch: 04, Loss: 0.4815, Val: 0.5752, Test: 0.5868\n",
      "Epoch: 05, Loss: 0.4448, Val: 0.6393, Test: 0.6531\n",
      "Epoch: 06, Loss: 0.3985, Val: 0.6711, Test: 0.6893\n",
      "Epoch: 07, Loss: 0.3601, Val: 0.7191, Test: 0.7394\n",
      "Epoch: 08, Loss: 0.3266, Val: 0.7362, Test: 0.7579\n",
      "Epoch: 09, Loss: 0.2806, Val: 0.7825, Test: 0.8060\n",
      "Epoch: 10, Loss: 0.2492, Val: 0.8030, Test: 0.8273\n",
      "Epoch: 11, Loss: 0.2281, Val: 0.8292, Test: 0.8532\n",
      "Epoch: 12, Loss: 0.2016, Val: 0.8455, Test: 0.8707\n",
      "Epoch: 13, Loss: 0.1927, Val: 0.8282, Test: 0.8524\n",
      "Epoch: 14, Loss: 0.1856, Val: 0.8585, Test: 0.8832\n",
      "Epoch: 15, Loss: 0.1664, Val: 0.8723, Test: 0.8959\n",
      "Epoch: 16, Loss: 0.1580, Val: 0.8573, Test: 0.8805\n",
      "Epoch: 17, Loss: 0.1752, Val: 0.8580, Test: 0.8817\n",
      "Epoch: 18, Loss: 0.1652, Val: 0.8687, Test: 0.8924\n",
      "Epoch: 19, Loss: 0.1428, Val: 0.8887, Test: 0.9102\n",
      "Epoch: 20, Loss: 0.1257, Val: 0.9017, Test: 0.9212\n",
      "Epoch: 21, Loss: 0.1219, Val: 0.8766, Test: 0.8976\n",
      "Epoch: 22, Loss: 0.1488, Val: 0.8650, Test: 0.8887\n",
      "Epoch: 23, Loss: 0.1541, Val: 0.8676, Test: 0.8941\n",
      "Epoch: 24, Loss: 0.1434, Val: 0.8803, Test: 0.9044\n",
      "Epoch: 25, Loss: 0.1283, Val: 0.8861, Test: 0.9114\n",
      "Epoch: 26, Loss: 0.1348, Val: 0.8728, Test: 0.8965\n",
      "Epoch: 27, Loss: 0.1376, Val: 0.8798, Test: 0.9054\n",
      "Epoch: 28, Loss: 0.1296, Val: 0.8907, Test: 0.9146\n",
      "Epoch: 29, Loss: 0.1271, Val: 0.8804, Test: 0.9036\n",
      "Epoch: 30, Loss: 0.1316, Val: 0.8832, Test: 0.9073\n",
      "Epoch: 31, Loss: 0.1372, Val: 0.8832, Test: 0.9070\n",
      "Epoch: 32, Loss: 0.1289, Val: 0.8849, Test: 0.9085\n",
      "Epoch: 33, Loss: 0.1342, Val: 0.8761, Test: 0.8991\n",
      "Epoch: 34, Loss: 0.1453, Val: 0.8677, Test: 0.8946\n",
      "Epoch: 35, Loss: 0.1403, Val: 0.8797, Test: 0.9047\n",
      "Epoch: 36, Loss: 0.1259, Val: 0.8871, Test: 0.9105\n",
      "Epoch: 37, Loss: 0.1186, Val: 0.9007, Test: 0.9225\n",
      "Epoch: 38, Loss: 0.1075, Val: 0.9035, Test: 0.9256\n",
      "Epoch: 39, Loss: 0.1032, Val: 0.9101, Test: 0.9307\n",
      "Epoch: 40, Loss: 0.0959, Val: 0.9088, Test: 0.9300\n",
      "Epoch: 41, Loss: 0.0996, Val: 0.9101, Test: 0.9314\n",
      "Epoch: 42, Loss: 0.1023, Val: 0.9114, Test: 0.9332\n",
      "Epoch: 43, Loss: 0.1099, Val: 0.8960, Test: 0.9165\n",
      "Epoch: 44, Loss: 0.1265, Val: 0.8670, Test: 0.8908\n",
      "Epoch: 45, Loss: 0.1527, Val: 0.8614, Test: 0.8871\n",
      "Epoch: 46, Loss: 0.1490, Val: 0.8788, Test: 0.9036\n",
      "Epoch: 47, Loss: 0.1326, Val: 0.8821, Test: 0.9074\n",
      "Epoch: 48, Loss: 0.1289, Val: 0.8796, Test: 0.9024\n",
      "Epoch: 49, Loss: 0.1355, Val: 0.8894, Test: 0.9114\n",
      "Epoch: 50, Loss: 0.1202, Val: 0.9030, Test: 0.9235\n",
      "Epoch: 51, Loss: 0.1169, Val: 0.9000, Test: 0.9213\n",
      "Epoch: 52, Loss: 0.1221, Val: 0.8805, Test: 0.9029\n",
      "Epoch: 53, Loss: 0.1278, Val: 0.8914, Test: 0.9147\n",
      "Epoch: 54, Loss: 0.1182, Val: 0.8869, Test: 0.9086\n",
      "Epoch: 55, Loss: 0.1145, Val: 0.9062, Test: 0.9268\n",
      "Epoch: 56, Loss: 0.0964, Val: 0.9154, Test: 0.9352\n",
      "Epoch: 57, Loss: 0.0861, Val: 0.9199, Test: 0.9388\n",
      "Epoch: 58, Loss: 0.0857, Val: 0.9099, Test: 0.9310\n",
      "Epoch: 59, Loss: 0.0918, Val: 0.9128, Test: 0.9339\n",
      "Epoch: 60, Loss: 0.1035, Val: 0.8942, Test: 0.9147\n",
      "Epoch: 61, Loss: 0.1154, Val: 0.8954, Test: 0.9180\n",
      "Epoch: 62, Loss: 0.1167, Val: 0.9024, Test: 0.9266\n",
      "Epoch: 63, Loss: 0.0972, Val: 0.9094, Test: 0.9320\n",
      "Epoch: 64, Loss: 0.1150, Val: 0.8893, Test: 0.9147\n",
      "Epoch: 65, Loss: 0.1168, Val: 0.8944, Test: 0.9179\n",
      "Epoch: 66, Loss: 0.1132, Val: 0.9010, Test: 0.9233\n",
      "Epoch: 67, Loss: 0.1115, Val: 0.8936, Test: 0.9169\n",
      "Epoch: 68, Loss: 0.1333, Val: 0.8834, Test: 0.9084\n",
      "Epoch: 69, Loss: 0.1154, Val: 0.9016, Test: 0.9242\n",
      "Epoch: 70, Loss: 0.1035, Val: 0.8980, Test: 0.9216\n",
      "Epoch: 71, Loss: 0.1070, Val: 0.9040, Test: 0.9241\n",
      "Epoch: 72, Loss: 0.1034, Val: 0.9117, Test: 0.9319\n",
      "Epoch: 73, Loss: 0.0915, Val: 0.9128, Test: 0.9330\n",
      "Epoch: 74, Loss: 0.0851, Val: 0.9172, Test: 0.9357\n",
      "Epoch: 75, Loss: 0.0963, Val: 0.8957, Test: 0.9161\n",
      "Epoch: 76, Loss: 0.1221, Val: 0.8940, Test: 0.9146\n",
      "Epoch: 77, Loss: 0.1224, Val: 0.8950, Test: 0.9168\n",
      "Epoch: 78, Loss: 0.1109, Val: 0.8931, Test: 0.9148\n",
      "Epoch: 79, Loss: 0.1142, Val: 0.8925, Test: 0.9133\n",
      "Epoch: 80, Loss: 0.1163, Val: 0.8951, Test: 0.9179\n",
      "Epoch: 81, Loss: 0.1119, Val: 0.8945, Test: 0.9163\n",
      "Epoch: 82, Loss: 0.1121, Val: 0.9068, Test: 0.9286\n",
      "Epoch: 83, Loss: 0.1056, Val: 0.9066, Test: 0.9280\n",
      "Epoch: 84, Loss: 0.0946, Val: 0.9153, Test: 0.9345\n",
      "Epoch: 85, Loss: 0.0936, Val: 0.9127, Test: 0.9310\n",
      "Epoch: 86, Loss: 0.0894, Val: 0.9129, Test: 0.9335\n",
      "Epoch: 87, Loss: 0.0918, Val: 0.9138, Test: 0.9337\n",
      "Epoch: 88, Loss: 0.1303, Val: 0.8906, Test: 0.9115\n",
      "Epoch: 89, Loss: 0.1196, Val: 0.8903, Test: 0.9134\n",
      "Epoch: 90, Loss: 0.1223, Val: 0.8909, Test: 0.9117\n",
      "Epoch: 91, Loss: 0.1148, Val: 0.9025, Test: 0.9259\n",
      "Epoch: 92, Loss: 0.1045, Val: 0.8984, Test: 0.9245\n",
      "Epoch: 93, Loss: 0.1047, Val: 0.8987, Test: 0.9217\n",
      "Epoch: 94, Loss: 0.1306, Val: 0.8698, Test: 0.8943\n",
      "Epoch: 95, Loss: 0.1469, Val: 0.8523, Test: 0.8821\n",
      "Epoch: 96, Loss: 0.1436, Val: 0.8718, Test: 0.8978\n",
      "Epoch: 97, Loss: 0.1358, Val: 0.8764, Test: 0.9051\n",
      "Epoch: 98, Loss: 0.1299, Val: 0.8787, Test: 0.9036\n",
      "Epoch: 99, Loss: 0.1187, Val: 0.8983, Test: 0.9214\n",
      "Epoch: 100, Loss: 0.1161, Val: 0.8975, Test: 0.9190\n",
      "CPU times: user 51min 23s, sys: 4min 23s, total: 55min 47s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = GATConv(in_dim, out_dim, heads=1)#这里in_dim和out_dim都=dim=256\n",
    "        # self.gatconv = GATConv(256, 256, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist_ppigeniepath=[]\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist_ppigeniepath.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5691, Val: 0.4398, Test: 0.4421\n",
      "Epoch: 02, Loss: 0.5221, Val: 0.4487, Test: 0.4532\n",
      "Epoch: 03, Loss: 0.4901, Val: 0.5271, Test: 0.5366\n",
      "Epoch: 04, Loss: 0.4518, Val: 0.6028, Test: 0.6152\n",
      "Epoch: 05, Loss: 0.4052, Val: 0.6536, Test: 0.6685\n",
      "Epoch: 06, Loss: 0.3490, Val: 0.7333, Test: 0.7513\n",
      "Epoch: 07, Loss: 0.2857, Val: 0.7859, Test: 0.8067\n",
      "Epoch: 08, Loss: 0.2249, Val: 0.8328, Test: 0.8542\n",
      "Epoch: 09, Loss: 0.1759, Val: 0.8657, Test: 0.8869\n",
      "Epoch: 10, Loss: 0.1385, Val: 0.8973, Test: 0.9167\n",
      "Epoch: 11, Loss: 0.1089, Val: 0.9182, Test: 0.9368\n",
      "Epoch: 12, Loss: 0.0870, Val: 0.9296, Test: 0.9476\n",
      "Epoch: 13, Loss: 0.0727, Val: 0.9413, Test: 0.9567\n",
      "Epoch: 14, Loss: 0.0617, Val: 0.9472, Test: 0.9619\n",
      "Epoch: 15, Loss: 0.0537, Val: 0.9501, Test: 0.9649\n",
      "Epoch: 16, Loss: 0.0470, Val: 0.9546, Test: 0.9680\n",
      "Epoch: 17, Loss: 0.0422, Val: 0.9566, Test: 0.9702\n",
      "Epoch: 18, Loss: 0.0385, Val: 0.9591, Test: 0.9718\n",
      "Epoch: 19, Loss: 0.0358, Val: 0.9590, Test: 0.9719\n",
      "Epoch: 20, Loss: 0.0341, Val: 0.9596, Test: 0.9723\n",
      "Epoch: 21, Loss: 0.0315, Val: 0.9618, Test: 0.9741\n",
      "Epoch: 22, Loss: 0.0300, Val: 0.9622, Test: 0.9740\n",
      "Epoch: 23, Loss: 0.0283, Val: 0.9631, Test: 0.9747\n",
      "Epoch: 24, Loss: 0.0272, Val: 0.9640, Test: 0.9752\n",
      "Epoch: 25, Loss: 0.0264, Val: 0.9637, Test: 0.9752\n",
      "Epoch: 26, Loss: 0.0254, Val: 0.9640, Test: 0.9751\n",
      "Epoch: 27, Loss: 0.0247, Val: 0.9644, Test: 0.9754\n",
      "Epoch: 28, Loss: 0.0241, Val: 0.9648, Test: 0.9756\n",
      "Epoch: 29, Loss: 0.0239, Val: 0.9643, Test: 0.9756\n",
      "Epoch: 30, Loss: 0.0236, Val: 0.9645, Test: 0.9755\n",
      "Epoch: 31, Loss: 0.0229, Val: 0.9652, Test: 0.9763\n",
      "Epoch: 32, Loss: 0.0224, Val: 0.9660, Test: 0.9765\n",
      "Epoch: 33, Loss: 0.0222, Val: 0.9652, Test: 0.9759\n",
      "Epoch: 34, Loss: 0.0216, Val: 0.9665, Test: 0.9769\n",
      "Epoch: 35, Loss: 0.0213, Val: 0.9664, Test: 0.9771\n",
      "Epoch: 36, Loss: 0.0211, Val: 0.9654, Test: 0.9760\n",
      "Epoch: 37, Loss: 0.0209, Val: 0.9665, Test: 0.9768\n",
      "Epoch: 38, Loss: 0.0205, Val: 0.9661, Test: 0.9767\n",
      "Epoch: 39, Loss: 0.0203, Val: 0.9666, Test: 0.9773\n",
      "Epoch: 40, Loss: 0.0198, Val: 0.9664, Test: 0.9772\n",
      "Epoch: 41, Loss: 0.0201, Val: 0.9658, Test: 0.9765\n",
      "Epoch: 42, Loss: 0.0206, Val: 0.9655, Test: 0.9767\n",
      "Epoch: 43, Loss: 0.0208, Val: 0.9652, Test: 0.9757\n",
      "Epoch: 44, Loss: 0.0226, Val: 0.9633, Test: 0.9750\n",
      "Epoch: 45, Loss: 0.0232, Val: 0.9622, Test: 0.9737\n",
      "Epoch: 46, Loss: 0.0276, Val: 0.9578, Test: 0.9700\n",
      "Epoch: 47, Loss: 0.0368, Val: 0.9507, Test: 0.9654\n",
      "Epoch: 48, Loss: 0.0474, Val: 0.9394, Test: 0.9555\n",
      "Epoch: 49, Loss: 0.0490, Val: 0.9436, Test: 0.9585\n",
      "Epoch: 50, Loss: 0.0442, Val: 0.9505, Test: 0.9659\n",
      "Epoch: 51, Loss: 0.0386, Val: 0.9549, Test: 0.9690\n",
      "Epoch: 52, Loss: 0.0308, Val: 0.9618, Test: 0.9740\n",
      "Epoch: 53, Loss: 0.0257, Val: 0.9641, Test: 0.9758\n",
      "Epoch: 54, Loss: 0.0227, Val: 0.9665, Test: 0.9769\n",
      "Epoch: 55, Loss: 0.0209, Val: 0.9663, Test: 0.9774\n",
      "Epoch: 56, Loss: 0.0199, Val: 0.9669, Test: 0.9777\n",
      "Epoch: 57, Loss: 0.0194, Val: 0.9672, Test: 0.9779\n",
      "Epoch: 58, Loss: 0.0192, Val: 0.9674, Test: 0.9780\n",
      "Epoch: 59, Loss: 0.0188, Val: 0.9669, Test: 0.9778\n",
      "Epoch: 60, Loss: 0.0185, Val: 0.9672, Test: 0.9781\n",
      "Epoch: 61, Loss: 0.0184, Val: 0.9675, Test: 0.9779\n",
      "Epoch: 62, Loss: 0.0182, Val: 0.9673, Test: 0.9780\n",
      "Epoch: 63, Loss: 0.0179, Val: 0.9676, Test: 0.9783\n",
      "Epoch: 64, Loss: 0.0178, Val: 0.9677, Test: 0.9783\n",
      "Epoch: 65, Loss: 0.0177, Val: 0.9679, Test: 0.9785\n",
      "Epoch: 66, Loss: 0.0176, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 67, Loss: 0.0175, Val: 0.9678, Test: 0.9785\n",
      "Epoch: 68, Loss: 0.0174, Val: 0.9681, Test: 0.9786\n",
      "Epoch: 69, Loss: 0.0173, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 70, Loss: 0.0173, Val: 0.9678, Test: 0.9785\n",
      "Epoch: 71, Loss: 0.0173, Val: 0.9676, Test: 0.9784\n",
      "Epoch: 72, Loss: 0.0172, Val: 0.9680, Test: 0.9786\n",
      "Epoch: 73, Loss: 0.0173, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 74, Loss: 0.0172, Val: 0.9680, Test: 0.9787\n",
      "Epoch: 75, Loss: 0.0171, Val: 0.9675, Test: 0.9782\n",
      "Epoch: 76, Loss: 0.0171, Val: 0.9678, Test: 0.9785\n",
      "Epoch: 77, Loss: 0.0172, Val: 0.9678, Test: 0.9782\n",
      "Epoch: 78, Loss: 0.0171, Val: 0.9677, Test: 0.9782\n",
      "Epoch: 79, Loss: 0.0171, Val: 0.9680, Test: 0.9782\n",
      "Epoch: 80, Loss: 0.0168, Val: 0.9679, Test: 0.9786\n",
      "Epoch: 81, Loss: 0.0168, Val: 0.9676, Test: 0.9785\n",
      "Epoch: 82, Loss: 0.0169, Val: 0.9679, Test: 0.9786\n",
      "Epoch: 83, Loss: 0.0171, Val: 0.9681, Test: 0.9784\n",
      "Epoch: 84, Loss: 0.0171, Val: 0.9677, Test: 0.9782\n",
      "Epoch: 85, Loss: 0.0172, Val: 0.9679, Test: 0.9784\n",
      "Epoch: 86, Loss: 0.0173, Val: 0.9679, Test: 0.9782\n",
      "Epoch: 87, Loss: 0.0172, Val: 0.9677, Test: 0.9782\n",
      "Epoch: 88, Loss: 0.0172, Val: 0.9679, Test: 0.9783\n",
      "Epoch: 89, Loss: 0.0170, Val: 0.9680, Test: 0.9784\n",
      "Epoch: 90, Loss: 0.0171, Val: 0.9680, Test: 0.9783\n",
      "Epoch: 91, Loss: 0.0170, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 92, Loss: 0.0179, Val: 0.9663, Test: 0.9770\n",
      "Epoch: 93, Loss: 0.0183, Val: 0.9667, Test: 0.9779\n",
      "Epoch: 94, Loss: 0.0181, Val: 0.9669, Test: 0.9779\n",
      "Epoch: 95, Loss: 0.0181, Val: 0.9668, Test: 0.9781\n",
      "Epoch: 96, Loss: 0.0179, Val: 0.9671, Test: 0.9781\n",
      "Epoch: 97, Loss: 0.0178, Val: 0.9674, Test: 0.9783\n",
      "Epoch: 98, Loss: 0.0179, Val: 0.9666, Test: 0.9780\n",
      "Epoch: 99, Loss: 0.0177, Val: 0.9670, Test: 0.9780\n",
      "Epoch: 100, Loss: 0.0176, Val: 0.9671, Test: 0.9780\n",
      "CPU times: user 39min 4s, sys: 3min 24s, total: 42min 28s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "# class agnnn(torch.nn.Module):\n",
    "#     def __init__(self,in_dim,out_dim):\n",
    "#         super(agnnn, self).__init__()\n",
    "#         self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "#         self.prop1 = AGNNConv(requires_grad=False)\n",
    "#         self.prop2 = AGNNConv(requires_grad=True)\n",
    "#         self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x = F.dropout(data.x, training=self.training)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = self.prop1(x, edge_index)\n",
    "#         x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist_agnn=[]\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist_agnn.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3189885"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losslist_ppigeniepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0069c03ab65e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosslist_ppigeniepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"geniepath\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosslist_agnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mymodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losslist_ppigeniepath' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losslist_ppigeniepath,label=\"geniepath\")\n",
    "\n",
    "plt.plot(losslist_agnn,label=\"mymodel\")\n",
    "\n",
    "\n",
    "plt.legend(loc=0, ncol=1) \n",
    "plt.savefig('./agnn_ppi对比.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
