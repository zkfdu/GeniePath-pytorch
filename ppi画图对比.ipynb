{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5630, Val: 0.4852, Test: 0.4873\n",
      "Epoch: 02, Loss: 0.5201, Val: 0.5218, Test: 0.5275\n",
      "Epoch: 03, Loss: 0.4997, Val: 0.5411, Test: 0.5490\n",
      "Epoch: 04, Loss: 0.4687, Val: 0.5469, Test: 0.5601\n",
      "Epoch: 05, Loss: 0.4363, Val: 0.6446, Test: 0.6601\n",
      "Epoch: 06, Loss: 0.3932, Val: 0.6743, Test: 0.6909\n",
      "Epoch: 07, Loss: 0.3630, Val: 0.6897, Test: 0.7072\n",
      "Epoch: 08, Loss: 0.3205, Val: 0.7445, Test: 0.7641\n",
      "Epoch: 09, Loss: 0.2753, Val: 0.7942, Test: 0.8128\n",
      "Epoch: 10, Loss: 0.2427, Val: 0.7981, Test: 0.8196\n",
      "Epoch: 11, Loss: 0.2212, Val: 0.8278, Test: 0.8462\n",
      "Epoch: 12, Loss: 0.2024, Val: 0.8436, Test: 0.8614\n",
      "Epoch: 13, Loss: 0.1851, Val: 0.8635, Test: 0.8835\n",
      "Epoch: 14, Loss: 0.1683, Val: 0.8674, Test: 0.8880\n",
      "Epoch: 15, Loss: 0.1474, Val: 0.8811, Test: 0.9006\n",
      "Epoch: 16, Loss: 0.1284, Val: 0.9051, Test: 0.9222\n",
      "Epoch: 17, Loss: 0.1126, Val: 0.9120, Test: 0.9296\n",
      "Epoch: 18, Loss: 0.1106, Val: 0.9131, Test: 0.9280\n",
      "Epoch: 19, Loss: 0.1034, Val: 0.9153, Test: 0.9304\n",
      "Epoch: 20, Loss: 0.0963, Val: 0.9240, Test: 0.9391\n",
      "Epoch: 21, Loss: 0.0939, Val: 0.9180, Test: 0.9336\n",
      "Epoch: 22, Loss: 0.0984, Val: 0.9246, Test: 0.9376\n",
      "Epoch: 23, Loss: 0.0881, Val: 0.9284, Test: 0.9416\n",
      "Epoch: 24, Loss: 0.0893, Val: 0.9169, Test: 0.9310\n",
      "Epoch: 25, Loss: 0.1060, Val: 0.9074, Test: 0.9239\n",
      "Epoch: 26, Loss: 0.1113, Val: 0.9119, Test: 0.9272\n",
      "Epoch: 27, Loss: 0.1088, Val: 0.8976, Test: 0.9134\n",
      "Epoch: 28, Loss: 0.1120, Val: 0.9089, Test: 0.9242\n",
      "Epoch: 29, Loss: 0.0989, Val: 0.9197, Test: 0.9353\n",
      "Epoch: 30, Loss: 0.0860, Val: 0.9289, Test: 0.9435\n",
      "Epoch: 31, Loss: 0.0833, Val: 0.9216, Test: 0.9361\n",
      "Epoch: 32, Loss: 0.0786, Val: 0.9324, Test: 0.9458\n",
      "Epoch: 33, Loss: 0.0690, Val: 0.9340, Test: 0.9478\n",
      "Epoch: 34, Loss: 0.0649, Val: 0.9331, Test: 0.9467\n",
      "Epoch: 35, Loss: 0.0780, Val: 0.9283, Test: 0.9435\n",
      "Epoch: 36, Loss: 0.0679, Val: 0.9354, Test: 0.9483\n",
      "Epoch: 37, Loss: 0.0637, Val: 0.9387, Test: 0.9532\n",
      "Epoch: 38, Loss: 0.0620, Val: 0.9392, Test: 0.9535\n",
      "Epoch: 39, Loss: 0.0587, Val: 0.9366, Test: 0.9499\n",
      "Epoch: 40, Loss: 0.0651, Val: 0.9358, Test: 0.9508\n",
      "Epoch: 41, Loss: 0.0628, Val: 0.9416, Test: 0.9554\n",
      "Epoch: 42, Loss: 0.0607, Val: 0.9339, Test: 0.9481\n",
      "Epoch: 43, Loss: 0.0669, Val: 0.9316, Test: 0.9464\n",
      "Epoch: 44, Loss: 0.0688, Val: 0.9340, Test: 0.9486\n",
      "Epoch: 45, Loss: 0.0710, Val: 0.9269, Test: 0.9424\n",
      "Epoch: 46, Loss: 0.0743, Val: 0.9284, Test: 0.9420\n",
      "Epoch: 47, Loss: 0.0663, Val: 0.9356, Test: 0.9500\n",
      "Epoch: 48, Loss: 0.0628, Val: 0.9326, Test: 0.9467\n",
      "Epoch: 49, Loss: 0.0662, Val: 0.9382, Test: 0.9517\n",
      "Epoch: 50, Loss: 0.0563, Val: 0.9432, Test: 0.9557\n",
      "Epoch: 51, Loss: 0.0558, Val: 0.9456, Test: 0.9579\n",
      "Epoch: 52, Loss: 0.0509, Val: 0.9470, Test: 0.9600\n",
      "Epoch: 53, Loss: 0.0480, Val: 0.9488, Test: 0.9611\n",
      "Epoch: 54, Loss: 0.0496, Val: 0.9485, Test: 0.9600\n",
      "Epoch: 55, Loss: 0.0525, Val: 0.9410, Test: 0.9544\n",
      "Epoch: 56, Loss: 0.0737, Val: 0.9235, Test: 0.9375\n",
      "Epoch: 57, Loss: 0.0783, Val: 0.9342, Test: 0.9466\n",
      "Epoch: 58, Loss: 0.0785, Val: 0.9264, Test: 0.9412\n",
      "Epoch: 59, Loss: 0.0986, Val: 0.9093, Test: 0.9265\n",
      "Epoch: 60, Loss: 0.0962, Val: 0.9109, Test: 0.9258\n",
      "Epoch: 61, Loss: 0.0896, Val: 0.9168, Test: 0.9331\n",
      "Epoch: 62, Loss: 0.0955, Val: 0.8966, Test: 0.9114\n",
      "Epoch: 63, Loss: 0.1055, Val: 0.9116, Test: 0.9256\n",
      "Epoch: 64, Loss: 0.0888, Val: 0.9231, Test: 0.9379\n",
      "Epoch: 65, Loss: 0.0828, Val: 0.9245, Test: 0.9390\n",
      "Epoch: 66, Loss: 0.0763, Val: 0.9337, Test: 0.9458\n",
      "Epoch: 67, Loss: 0.0673, Val: 0.9392, Test: 0.9518\n",
      "Epoch: 68, Loss: 0.0624, Val: 0.9389, Test: 0.9526\n",
      "Epoch: 69, Loss: 0.0552, Val: 0.9453, Test: 0.9579\n",
      "Epoch: 70, Loss: 0.0505, Val: 0.9467, Test: 0.9590\n",
      "Epoch: 71, Loss: 0.0494, Val: 0.9451, Test: 0.9575\n",
      "Epoch: 72, Loss: 0.0504, Val: 0.9449, Test: 0.9573\n",
      "Epoch: 73, Loss: 0.0486, Val: 0.9499, Test: 0.9613\n",
      "Epoch: 74, Loss: 0.0439, Val: 0.9518, Test: 0.9622\n",
      "Epoch: 75, Loss: 0.0434, Val: 0.9524, Test: 0.9630\n",
      "Epoch: 76, Loss: 0.0410, Val: 0.9537, Test: 0.9637\n",
      "Epoch: 77, Loss: 0.0404, Val: 0.9540, Test: 0.9647\n",
      "Epoch: 78, Loss: 0.0406, Val: 0.9538, Test: 0.9638\n",
      "Epoch: 79, Loss: 0.0415, Val: 0.9531, Test: 0.9632\n",
      "Epoch: 80, Loss: 0.0414, Val: 0.9499, Test: 0.9617\n",
      "Epoch: 81, Loss: 0.0456, Val: 0.9492, Test: 0.9600\n",
      "Epoch: 82, Loss: 0.0489, Val: 0.9464, Test: 0.9577\n",
      "Epoch: 83, Loss: 0.0518, Val: 0.9440, Test: 0.9550\n",
      "Epoch: 84, Loss: 0.0514, Val: 0.9448, Test: 0.9565\n",
      "Epoch: 85, Loss: 0.0502, Val: 0.9465, Test: 0.9577\n",
      "Epoch: 86, Loss: 0.0463, Val: 0.9515, Test: 0.9624\n",
      "Epoch: 87, Loss: 0.0440, Val: 0.9484, Test: 0.9597\n",
      "Epoch: 88, Loss: 0.0449, Val: 0.9505, Test: 0.9616\n",
      "Epoch: 89, Loss: 0.0443, Val: 0.9483, Test: 0.9601\n",
      "Epoch: 90, Loss: 0.0426, Val: 0.9507, Test: 0.9624\n",
      "Epoch: 91, Loss: 0.0428, Val: 0.9490, Test: 0.9599\n",
      "Epoch: 92, Loss: 0.0428, Val: 0.9514, Test: 0.9617\n",
      "Epoch: 93, Loss: 0.0438, Val: 0.9516, Test: 0.9617\n",
      "Epoch: 94, Loss: 0.0416, Val: 0.9536, Test: 0.9639\n",
      "Epoch: 95, Loss: 0.0382, Val: 0.9555, Test: 0.9653\n",
      "Epoch: 96, Loss: 0.0373, Val: 0.9556, Test: 0.9653\n",
      "Epoch: 97, Loss: 0.0391, Val: 0.9532, Test: 0.9635\n",
      "Epoch: 98, Loss: 0.0393, Val: 0.9555, Test: 0.9658\n",
      "Epoch: 99, Loss: 0.0369, Val: 0.9536, Test: 0.9647\n",
      "Epoch: 100, Loss: 0.0381, Val: 0.9529, Test: 0.9643\n",
      "CPU times: user 45min 25s, sys: 4min, total: 49min 26s\n",
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3455097"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = GATConv(in_dim, out_dim, heads=1)#这里in_dim和out_dim都=dim=256\n",
    "        # self.gatconv = GATConv(256, 256, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist_ppigeniepath=[]\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist_ppigeniepath.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n",
    "sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5722, Val: 0.4580, Test: 0.4604\n",
      "Epoch: 02, Loss: 0.5275, Val: 0.4911, Test: 0.4983\n",
      "Epoch: 03, Loss: 0.4927, Val: 0.5555, Test: 0.5663\n",
      "Epoch: 04, Loss: 0.4489, Val: 0.6030, Test: 0.6157\n",
      "Epoch: 05, Loss: 0.3979, Val: 0.6771, Test: 0.6943\n",
      "Epoch: 06, Loss: 0.3384, Val: 0.7380, Test: 0.7580\n",
      "Epoch: 07, Loss: 0.2798, Val: 0.7862, Test: 0.8069\n",
      "Epoch: 08, Loss: 0.2263, Val: 0.8340, Test: 0.8554\n",
      "Epoch: 09, Loss: 0.1794, Val: 0.8663, Test: 0.8877\n",
      "Epoch: 10, Loss: 0.1437, Val: 0.8936, Test: 0.9135\n",
      "Epoch: 11, Loss: 0.1127, Val: 0.9150, Test: 0.9335\n",
      "Epoch: 12, Loss: 0.0910, Val: 0.9256, Test: 0.9432\n",
      "Epoch: 13, Loss: 0.0749, Val: 0.9358, Test: 0.9511\n",
      "Epoch: 14, Loss: 0.0655, Val: 0.9428, Test: 0.9585\n",
      "Epoch: 15, Loss: 0.0561, Val: 0.9487, Test: 0.9632\n",
      "Epoch: 16, Loss: 0.0490, Val: 0.9533, Test: 0.9659\n",
      "Epoch: 17, Loss: 0.0441, Val: 0.9549, Test: 0.9688\n",
      "Epoch: 18, Loss: 0.0398, Val: 0.9581, Test: 0.9713\n",
      "Epoch: 19, Loss: 0.0367, Val: 0.9598, Test: 0.9718\n",
      "Epoch: 20, Loss: 0.0336, Val: 0.9600, Test: 0.9727\n",
      "Epoch: 21, Loss: 0.0314, Val: 0.9621, Test: 0.9738\n",
      "Epoch: 22, Loss: 0.0298, Val: 0.9624, Test: 0.9743\n",
      "Epoch: 23, Loss: 0.0286, Val: 0.9635, Test: 0.9746\n",
      "Epoch: 24, Loss: 0.0279, Val: 0.9642, Test: 0.9754\n",
      "Epoch: 25, Loss: 0.0267, Val: 0.9638, Test: 0.9752\n",
      "Epoch: 26, Loss: 0.0256, Val: 0.9648, Test: 0.9757\n",
      "Epoch: 27, Loss: 0.0248, Val: 0.9645, Test: 0.9760\n",
      "Epoch: 28, Loss: 0.0243, Val: 0.9644, Test: 0.9756\n",
      "Epoch: 29, Loss: 0.0236, Val: 0.9652, Test: 0.9762\n",
      "Epoch: 30, Loss: 0.0229, Val: 0.9651, Test: 0.9761\n",
      "Epoch: 31, Loss: 0.0224, Val: 0.9654, Test: 0.9764\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "# class agnnn(torch.nn.Module):\n",
    "#     def __init__(self,in_dim,out_dim):\n",
    "#         super(agnnn, self).__init__()\n",
    "#         self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "#         self.prop1 = AGNNConv(requires_grad=False)\n",
    "#         self.prop2 = AGNNConv(requires_grad=True)\n",
    "#         self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x = F.dropout(data.x, training=self.training)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = self.prop1(x, edge_index)\n",
    "#         x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist_sgenieagnn=[]\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist_sgenieagnn.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n",
    "sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losslist_ppigeniepath,label=\"geniepath\")\n",
    "\n",
    "plt.plot(losslist_sgenieagnn,label=\"mymodel\")\n",
    "\n",
    "\n",
    "plt.legend(loc=0, ncol=1) \n",
    "plt.savefig('./ppi_genie_sgenie对比.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
