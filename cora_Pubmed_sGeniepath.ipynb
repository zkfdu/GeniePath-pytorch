{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cora sgeniepath agnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.9464262, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
      "Epoch: 002,train_loss:1.9461441, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
      "Epoch: 003,train_loss:1.9459273, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
      "Epoch: 004,train_loss:1.9456735, Train: 0.1714, Val: 0.1220, Test: 0.1330\n",
      "Epoch: 005,train_loss:1.9455956, Train: 0.2286, Val: 0.1260, Test: 0.1430\n",
      "Epoch: 006,train_loss:1.9455638, Train: 0.3214, Val: 0.2140, Test: 0.2160\n",
      "Epoch: 007,train_loss:1.9453977, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 008,train_loss:1.9451876, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 009,train_loss:1.9451252, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 010,train_loss:1.9450195, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 011,train_loss:1.9448973, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 012,train_loss:1.9448318, Train: 0.1429, Val: 0.1640, Test: 0.1500\n",
      "Epoch: 013,train_loss:1.9448287, Train: 0.2429, Val: 0.3320, Test: 0.3340\n",
      "Epoch: 014,train_loss:1.9447016, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 015,train_loss:1.9442562, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 016,train_loss:1.9439119, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 017,train_loss:1.9434733, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 018,train_loss:1.9427390, Train: 0.1429, Val: 0.3160, Test: 0.3200\n",
      "Epoch: 019,train_loss:1.9417843, Train: 0.1857, Val: 0.3260, Test: 0.3280\n",
      "Epoch: 020,train_loss:1.9397346, Train: 0.3714, Val: 0.3780, Test: 0.3700\n",
      "Epoch: 021,train_loss:1.9365418, Train: 0.5071, Val: 0.4840, Test: 0.4550\n",
      "Epoch: 022,train_loss:1.9330307, Train: 0.5000, Val: 0.4520, Test: 0.4500\n",
      "Epoch: 023,train_loss:1.9224631, Train: 0.4571, Val: 0.4280, Test: 0.4220\n",
      "Epoch: 024,train_loss:1.9118966, Train: 0.4500, Val: 0.4080, Test: 0.3970\n",
      "Epoch: 025,train_loss:1.8943305, Train: 0.4643, Val: 0.3560, Test: 0.3710\n",
      "Epoch: 026,train_loss:1.8582345, Train: 0.4286, Val: 0.3180, Test: 0.3300\n",
      "Epoch: 027,train_loss:1.8241228, Train: 0.4071, Val: 0.3020, Test: 0.2940\n",
      "Epoch: 028,train_loss:1.7530470, Train: 0.3714, Val: 0.2920, Test: 0.2880\n",
      "Epoch: 029,train_loss:1.6877141, Train: 0.3500, Val: 0.2720, Test: 0.2700\n",
      "Epoch: 030,train_loss:1.5978520, Train: 0.3429, Val: 0.2560, Test: 0.2500\n",
      "Epoch: 031,train_loss:1.5085084, Train: 0.3357, Val: 0.2400, Test: 0.2390\n",
      "Epoch: 032,train_loss:1.4133627, Train: 0.3286, Val: 0.2340, Test: 0.2260\n",
      "Epoch: 033,train_loss:1.3783914, Train: 0.3286, Val: 0.2200, Test: 0.2130\n",
      "Epoch: 034,train_loss:1.3216063, Train: 0.3786, Val: 0.2320, Test: 0.2340\n",
      "Epoch: 035,train_loss:1.3216925, Train: 0.4000, Val: 0.2520, Test: 0.2430\n",
      "Epoch: 036,train_loss:1.2934715, Train: 0.4071, Val: 0.2680, Test: 0.2590\n",
      "Epoch: 037,train_loss:1.2463232, Train: 0.4000, Val: 0.2660, Test: 0.2580\n",
      "Epoch: 038,train_loss:1.2449268, Train: 0.3643, Val: 0.2540, Test: 0.2620\n",
      "Epoch: 039,train_loss:1.2118152, Train: 0.4214, Val: 0.2680, Test: 0.2680\n",
      "Epoch: 040,train_loss:1.1770728, Train: 0.4429, Val: 0.2840, Test: 0.2780\n",
      "Epoch: 041,train_loss:1.1229866, Train: 0.5857, Val: 0.3020, Test: 0.3160\n",
      "Epoch: 042,train_loss:1.0918984, Train: 0.7000, Val: 0.3520, Test: 0.3500\n",
      "Epoch: 043,train_loss:1.0967308, Train: 0.7143, Val: 0.3320, Test: 0.3630\n",
      "Epoch: 044,train_loss:1.0196384, Train: 0.7214, Val: 0.3320, Test: 0.3620\n",
      "Epoch: 045,train_loss:0.9863206, Train: 0.8000, Val: 0.3380, Test: 0.3640\n",
      "Epoch: 046,train_loss:0.9256872, Train: 0.7286, Val: 0.3260, Test: 0.3470\n",
      "Epoch: 047,train_loss:0.8828688, Train: 0.7286, Val: 0.3340, Test: 0.3660\n",
      "Epoch: 048,train_loss:0.8272696, Train: 0.7571, Val: 0.3680, Test: 0.4070\n",
      "Epoch: 049,train_loss:0.8258811, Train: 0.8071, Val: 0.4100, Test: 0.4410\n",
      "Epoch: 050,train_loss:0.7437068, Train: 0.8143, Val: 0.4380, Test: 0.4520\n",
      "Epoch: 051,train_loss:0.7554442, Train: 0.8286, Val: 0.4680, Test: 0.4810\n",
      "Epoch: 052,train_loss:0.6370498, Train: 0.8429, Val: 0.4920, Test: 0.5060\n",
      "Epoch: 053,train_loss:0.6638510, Train: 0.9000, Val: 0.5180, Test: 0.5060\n",
      "Epoch: 054,train_loss:0.5965431, Train: 0.9571, Val: 0.5140, Test: 0.5180\n",
      "Epoch: 055,train_loss:0.5502414, Train: 0.9214, Val: 0.5220, Test: 0.5380\n",
      "Epoch: 056,train_loss:0.5254185, Train: 0.9214, Val: 0.5180, Test: 0.5410\n",
      "Epoch: 057,train_loss:0.5821495, Train: 0.9643, Val: 0.5240, Test: 0.5450\n",
      "Epoch: 058,train_loss:0.5048789, Train: 0.9714, Val: 0.5380, Test: 0.5390\n",
      "Epoch: 059,train_loss:0.4830102, Train: 0.9571, Val: 0.5420, Test: 0.5440\n",
      "Epoch: 060,train_loss:0.4625658, Train: 0.9500, Val: 0.5440, Test: 0.5610\n",
      "Epoch: 061,train_loss:0.4243068, Train: 0.9643, Val: 0.5580, Test: 0.5740\n",
      "Epoch: 062,train_loss:0.4325849, Train: 0.9643, Val: 0.5480, Test: 0.5480\n",
      "Epoch: 063,train_loss:0.4370578, Train: 0.9857, Val: 0.5360, Test: 0.5460\n",
      "Epoch: 064,train_loss:0.3998028, Train: 0.9857, Val: 0.5220, Test: 0.5240\n",
      "Epoch: 065,train_loss:0.3898641, Train: 0.9643, Val: 0.5120, Test: 0.5050\n",
      "Epoch: 066,train_loss:0.4341523, Train: 0.9857, Val: 0.5340, Test: 0.5410\n",
      "Epoch: 067,train_loss:0.3845738, Train: 0.9786, Val: 0.5740, Test: 0.5660\n",
      "Epoch: 068,train_loss:0.3459053, Train: 0.9714, Val: 0.5760, Test: 0.5870\n",
      "Epoch: 069,train_loss:0.4048089, Train: 0.9714, Val: 0.6020, Test: 0.6160\n",
      "Epoch: 070,train_loss:0.3172727, Train: 0.9857, Val: 0.6040, Test: 0.6190\n",
      "Epoch: 071,train_loss:0.3119454, Train: 0.9857, Val: 0.5940, Test: 0.6140\n",
      "Epoch: 072,train_loss:0.3280499, Train: 0.9857, Val: 0.5840, Test: 0.6070\n",
      "Epoch: 073,train_loss:0.2955283, Train: 0.9857, Val: 0.5660, Test: 0.5910\n",
      "Epoch: 074,train_loss:0.3587125, Train: 0.9857, Val: 0.5640, Test: 0.5840\n",
      "Epoch: 075,train_loss:0.2955534, Train: 0.9857, Val: 0.6000, Test: 0.5980\n",
      "Epoch: 076,train_loss:0.2752090, Train: 0.9857, Val: 0.6020, Test: 0.6100\n",
      "Epoch: 077,train_loss:0.2724130, Train: 0.9857, Val: 0.6160, Test: 0.6190\n",
      "Epoch: 078,train_loss:0.2893703, Train: 0.9929, Val: 0.6080, Test: 0.6110\n",
      "Epoch: 079,train_loss:0.2831057, Train: 0.9929, Val: 0.5920, Test: 0.6020\n",
      "Epoch: 080,train_loss:0.2299045, Train: 0.9929, Val: 0.6040, Test: 0.6040\n",
      "Epoch: 081,train_loss:0.2458570, Train: 0.9929, Val: 0.5920, Test: 0.5950\n",
      "Epoch: 082,train_loss:0.2441114, Train: 0.9929, Val: 0.5960, Test: 0.5970\n",
      "Epoch: 083,train_loss:0.2481980, Train: 0.9929, Val: 0.6000, Test: 0.6140\n",
      "Epoch: 084,train_loss:0.3035756, Train: 0.9929, Val: 0.6200, Test: 0.6470\n",
      "Epoch: 085,train_loss:0.2612965, Train: 0.9929, Val: 0.6320, Test: 0.6520\n",
      "Epoch: 086,train_loss:0.2140605, Train: 1.0000, Val: 0.6340, Test: 0.6590\n",
      "Epoch: 087,train_loss:0.2411556, Train: 1.0000, Val: 0.6300, Test: 0.6540\n",
      "Epoch: 088,train_loss:0.2684528, Train: 0.9929, Val: 0.5960, Test: 0.6260\n",
      "Epoch: 089,train_loss:0.2146271, Train: 0.9929, Val: 0.5920, Test: 0.6110\n",
      "Epoch: 090,train_loss:0.1943962, Train: 1.0000, Val: 0.5960, Test: 0.6220\n",
      "Epoch: 091,train_loss:0.1938347, Train: 1.0000, Val: 0.6020, Test: 0.6220\n",
      "Epoch: 092,train_loss:0.2395489, Train: 1.0000, Val: 0.6240, Test: 0.6430\n",
      "Epoch: 093,train_loss:0.1664297, Train: 1.0000, Val: 0.6500, Test: 0.6600\n",
      "Epoch: 094,train_loss:0.1982592, Train: 1.0000, Val: 0.6580, Test: 0.6680\n",
      "Epoch: 095,train_loss:0.2131442, Train: 1.0000, Val: 0.6280, Test: 0.6490\n",
      "Epoch: 096,train_loss:0.1726215, Train: 1.0000, Val: 0.6200, Test: 0.6220\n",
      "Epoch: 097,train_loss:0.1569208, Train: 0.9929, Val: 0.5880, Test: 0.6030\n",
      "Epoch: 098,train_loss:0.1513844, Train: 0.9929, Val: 0.5940, Test: 0.6140\n",
      "Epoch: 099,train_loss:0.1728940, Train: 1.0000, Val: 0.6080, Test: 0.6390\n",
      "Epoch: 100,train_loss:0.1912719, Train: 1.0000, Val: 0.6440, Test: 0.6730\n",
      "Epoch: 101,train_loss:0.1120603, Train: 1.0000, Val: 0.6720, Test: 0.6910\n",
      "Epoch: 102,train_loss:0.1418077, Train: 1.0000, Val: 0.6740, Test: 0.6990\n",
      "Epoch: 103,train_loss:0.1754940, Train: 1.0000, Val: 0.6820, Test: 0.7000\n",
      "Epoch: 104,train_loss:0.1670173, Train: 1.0000, Val: 0.6640, Test: 0.6850\n",
      "Epoch: 105,train_loss:0.1195990, Train: 1.0000, Val: 0.6480, Test: 0.6690\n",
      "Epoch: 106,train_loss:0.1223830, Train: 1.0000, Val: 0.6280, Test: 0.6460\n",
      "Epoch: 107,train_loss:0.1473373, Train: 1.0000, Val: 0.6220, Test: 0.6490\n",
      "Epoch: 108,train_loss:0.1355707, Train: 1.0000, Val: 0.6500, Test: 0.6500\n",
      "Epoch: 109,train_loss:0.1187073, Train: 1.0000, Val: 0.6620, Test: 0.6630\n",
      "Epoch: 110,train_loss:0.1175912, Train: 1.0000, Val: 0.6700, Test: 0.6860\n",
      "Epoch: 111,train_loss:0.1084107, Train: 1.0000, Val: 0.6740, Test: 0.6960\n",
      "Epoch: 112,train_loss:0.1136803, Train: 1.0000, Val: 0.6700, Test: 0.6900\n",
      "Epoch: 113,train_loss:0.1236826, Train: 1.0000, Val: 0.6700, Test: 0.6920\n",
      "Epoch: 114,train_loss:0.1401045, Train: 1.0000, Val: 0.6540, Test: 0.6740\n",
      "Epoch: 115,train_loss:0.1158062, Train: 1.0000, Val: 0.6440, Test: 0.6470\n",
      "Epoch: 116,train_loss:0.1432478, Train: 1.0000, Val: 0.6380, Test: 0.6460\n",
      "Epoch: 117,train_loss:0.1323799, Train: 1.0000, Val: 0.6540, Test: 0.6630\n",
      "Epoch: 118,train_loss:0.1342696, Train: 1.0000, Val: 0.6620, Test: 0.6980\n",
      "Epoch: 119,train_loss:0.1295527, Train: 1.0000, Val: 0.6600, Test: 0.7010\n",
      "Epoch: 120,train_loss:0.1435611, Train: 1.0000, Val: 0.6800, Test: 0.7140\n",
      "Epoch: 121,train_loss:0.1273734, Train: 1.0000, Val: 0.6780, Test: 0.7220\n",
      "Epoch: 122,train_loss:0.1165244, Train: 1.0000, Val: 0.6820, Test: 0.6950\n",
      "Epoch: 123,train_loss:0.1039335, Train: 1.0000, Val: 0.6660, Test: 0.6830\n",
      "Epoch: 124,train_loss:0.1261183, Train: 1.0000, Val: 0.6560, Test: 0.6790\n",
      "Epoch: 125,train_loss:0.1273566, Train: 1.0000, Val: 0.6700, Test: 0.6810\n",
      "Epoch: 126,train_loss:0.1091375, Train: 1.0000, Val: 0.6660, Test: 0.6840\n",
      "Epoch: 127,train_loss:0.1091575, Train: 1.0000, Val: 0.6700, Test: 0.6880\n",
      "Epoch: 128,train_loss:0.1509515, Train: 1.0000, Val: 0.6780, Test: 0.6990\n",
      "Epoch: 129,train_loss:0.1069901, Train: 1.0000, Val: 0.6880, Test: 0.7120\n",
      "Epoch: 130,train_loss:0.0844724, Train: 0.9929, Val: 0.6740, Test: 0.7010\n",
      "Epoch: 131,train_loss:0.1590606, Train: 0.9929, Val: 0.6580, Test: 0.6940\n",
      "Epoch: 132,train_loss:0.1249023, Train: 1.0000, Val: 0.6780, Test: 0.6920\n",
      "Epoch: 133,train_loss:0.1167765, Train: 1.0000, Val: 0.6780, Test: 0.6880\n",
      "Epoch: 134,train_loss:0.1123740, Train: 1.0000, Val: 0.6700, Test: 0.6740\n",
      "Epoch: 135,train_loss:0.0961715, Train: 1.0000, Val: 0.6640, Test: 0.6770\n",
      "Epoch: 136,train_loss:0.1493505, Train: 1.0000, Val: 0.6720, Test: 0.6880\n",
      "Epoch: 137,train_loss:0.0932568, Train: 1.0000, Val: 0.6760, Test: 0.7050\n",
      "Epoch: 138,train_loss:0.0847982, Train: 1.0000, Val: 0.6700, Test: 0.7060\n",
      "Epoch: 139,train_loss:0.1004217, Train: 1.0000, Val: 0.6700, Test: 0.7050\n",
      "Epoch: 140,train_loss:0.0857713, Train: 1.0000, Val: 0.6760, Test: 0.7140\n",
      "Epoch: 141,train_loss:0.1010255, Train: 1.0000, Val: 0.6860, Test: 0.7170\n",
      "Epoch: 142,train_loss:0.1497707, Train: 1.0000, Val: 0.6880, Test: 0.7190\n",
      "Epoch: 143,train_loss:0.1177466, Train: 1.0000, Val: 0.6940, Test: 0.7100\n",
      "Epoch: 144,train_loss:0.0667378, Train: 1.0000, Val: 0.6960, Test: 0.6950\n",
      "Epoch: 145,train_loss:0.0866616, Train: 1.0000, Val: 0.6940, Test: 0.6890\n",
      "Epoch: 146,train_loss:0.0724717, Train: 1.0000, Val: 0.6940, Test: 0.6920\n",
      "Epoch: 147,train_loss:0.0891639, Train: 1.0000, Val: 0.6920, Test: 0.6960\n",
      "Epoch: 148,train_loss:0.0734488, Train: 1.0000, Val: 0.6880, Test: 0.6980\n",
      "Epoch: 149,train_loss:0.0952926, Train: 1.0000, Val: 0.6880, Test: 0.6990\n",
      "Epoch: 150,train_loss:0.0788685, Train: 1.0000, Val: 0.6880, Test: 0.7080\n",
      "Epoch: 151,train_loss:0.0740889, Train: 1.0000, Val: 0.6900, Test: 0.7230\n",
      "Epoch: 152,train_loss:0.0559872, Train: 1.0000, Val: 0.6900, Test: 0.7210\n",
      "Epoch: 153,train_loss:0.0762241, Train: 1.0000, Val: 0.6980, Test: 0.7170\n",
      "Epoch: 154,train_loss:0.0855724, Train: 1.0000, Val: 0.7020, Test: 0.7190\n",
      "Epoch: 155,train_loss:0.0981824, Train: 1.0000, Val: 0.6880, Test: 0.7270\n",
      "Epoch: 156,train_loss:0.0749549, Train: 1.0000, Val: 0.6860, Test: 0.7260\n",
      "Epoch: 157,train_loss:0.0817123, Train: 1.0000, Val: 0.6900, Test: 0.7270\n",
      "Epoch: 158,train_loss:0.0609763, Train: 1.0000, Val: 0.6960, Test: 0.7170\n",
      "Epoch: 159,train_loss:0.0660052, Train: 1.0000, Val: 0.6940, Test: 0.7050\n",
      "Epoch: 160,train_loss:0.1025560, Train: 1.0000, Val: 0.6780, Test: 0.6810\n",
      "Epoch: 161,train_loss:0.0655341, Train: 1.0000, Val: 0.6740, Test: 0.6760\n",
      "Epoch: 162,train_loss:0.0758800, Train: 1.0000, Val: 0.6780, Test: 0.6850\n",
      "Epoch: 163,train_loss:0.0762293, Train: 1.0000, Val: 0.6780, Test: 0.6940\n",
      "Epoch: 164,train_loss:0.0772285, Train: 1.0000, Val: 0.6840, Test: 0.7130\n",
      "Epoch: 165,train_loss:0.0469019, Train: 1.0000, Val: 0.6920, Test: 0.7230\n",
      "Epoch: 166,train_loss:0.0589029, Train: 1.0000, Val: 0.6980, Test: 0.7230\n",
      "Epoch: 167,train_loss:0.0825883, Train: 1.0000, Val: 0.6960, Test: 0.7270\n",
      "Epoch: 168,train_loss:0.0963656, Train: 1.0000, Val: 0.6960, Test: 0.7290\n",
      "Epoch: 169,train_loss:0.0656141, Train: 1.0000, Val: 0.6940, Test: 0.7310\n",
      "Epoch: 170,train_loss:0.0632589, Train: 1.0000, Val: 0.6920, Test: 0.7250\n",
      "Epoch: 171,train_loss:0.0889383, Train: 1.0000, Val: 0.6880, Test: 0.7250\n",
      "Epoch: 172,train_loss:0.0913798, Train: 1.0000, Val: 0.6920, Test: 0.7240\n",
      "Epoch: 173,train_loss:0.0669049, Train: 1.0000, Val: 0.6960, Test: 0.7260\n",
      "Epoch: 174,train_loss:0.0896555, Train: 1.0000, Val: 0.6960, Test: 0.7310\n",
      "Epoch: 175,train_loss:0.0496903, Train: 1.0000, Val: 0.6980, Test: 0.7230\n",
      "Epoch: 176,train_loss:0.0563042, Train: 1.0000, Val: 0.6840, Test: 0.7170\n",
      "Epoch: 177,train_loss:0.0935830, Train: 1.0000, Val: 0.6800, Test: 0.7020\n",
      "Epoch: 178,train_loss:0.0847102, Train: 1.0000, Val: 0.6860, Test: 0.7070\n",
      "Epoch: 179,train_loss:0.0723696, Train: 1.0000, Val: 0.6860, Test: 0.7210\n",
      "Epoch: 180,train_loss:0.0696963, Train: 1.0000, Val: 0.6940, Test: 0.7180\n",
      "Epoch: 181,train_loss:0.0629137, Train: 1.0000, Val: 0.7080, Test: 0.7280\n",
      "Epoch: 182,train_loss:0.0724957, Train: 1.0000, Val: 0.7080, Test: 0.7380\n",
      "Epoch: 183,train_loss:0.0704547, Train: 1.0000, Val: 0.6840, Test: 0.7320\n",
      "Epoch: 184,train_loss:0.0678422, Train: 1.0000, Val: 0.6820, Test: 0.7250\n",
      "Epoch: 185,train_loss:0.0643434, Train: 1.0000, Val: 0.6860, Test: 0.7160\n",
      "Epoch: 186,train_loss:0.0959174, Train: 1.0000, Val: 0.6920, Test: 0.7250\n",
      "Epoch: 187,train_loss:0.0500247, Train: 1.0000, Val: 0.6980, Test: 0.7270\n",
      "Epoch: 188,train_loss:0.0529020, Train: 1.0000, Val: 0.7020, Test: 0.7320\n",
      "Epoch: 189,train_loss:0.0644475, Train: 1.0000, Val: 0.7020, Test: 0.7270\n",
      "Epoch: 190,train_loss:0.0523894, Train: 1.0000, Val: 0.6900, Test: 0.7170\n",
      "Epoch: 191,train_loss:0.0679870, Train: 1.0000, Val: 0.6900, Test: 0.7140\n",
      "Epoch: 192,train_loss:0.0550718, Train: 1.0000, Val: 0.6980, Test: 0.7280\n",
      "Epoch: 193,train_loss:0.0409118, Train: 1.0000, Val: 0.6960, Test: 0.7220\n",
      "Epoch: 194,train_loss:0.0628359, Train: 1.0000, Val: 0.7000, Test: 0.7280\n",
      "Epoch: 195,train_loss:0.0597333, Train: 1.0000, Val: 0.6980, Test: 0.7290\n",
      "Epoch: 196,train_loss:0.0518408, Train: 1.0000, Val: 0.6980, Test: 0.7330\n",
      "Epoch: 197,train_loss:0.0601748, Train: 1.0000, Val: 0.6900, Test: 0.7280\n",
      "Epoch: 198,train_loss:0.0935018, Train: 1.0000, Val: 0.6940, Test: 0.7330\n",
      "Epoch: 199,train_loss:0.0818265, Train: 1.0000, Val: 0.6940, Test: 0.7360\n",
      "Epoch: 200,train_loss:0.0590489, Train: 1.0000, Val: 0.6920, Test: 0.7180\n",
      "Epoch: 201,train_loss:0.0607434, Train: 1.0000, Val: 0.6980, Test: 0.7110\n",
      "Epoch: 202,train_loss:0.0940497, Train: 1.0000, Val: 0.6940, Test: 0.7200\n",
      "Epoch: 203,train_loss:0.1423700, Train: 1.0000, Val: 0.7060, Test: 0.7270\n",
      "Epoch: 204,train_loss:0.0573468, Train: 1.0000, Val: 0.7160, Test: 0.7350\n",
      "Epoch: 205,train_loss:0.0723924, Train: 1.0000, Val: 0.7180, Test: 0.7370\n",
      "Epoch: 206,train_loss:0.0936387, Train: 1.0000, Val: 0.7120, Test: 0.7320\n",
      "Epoch: 207,train_loss:0.0714700, Train: 1.0000, Val: 0.7040, Test: 0.7240\n",
      "Epoch: 208,train_loss:0.0594442, Train: 1.0000, Val: 0.7040, Test: 0.7080\n",
      "Epoch: 209,train_loss:0.0482924, Train: 1.0000, Val: 0.6880, Test: 0.6970\n",
      "Epoch: 210,train_loss:0.1084666, Train: 1.0000, Val: 0.6980, Test: 0.7120\n",
      "Epoch: 211,train_loss:0.0623482, Train: 1.0000, Val: 0.6980, Test: 0.7270\n",
      "Epoch: 212,train_loss:0.0384444, Train: 1.0000, Val: 0.7200, Test: 0.7360\n",
      "Epoch: 213,train_loss:0.0707546, Train: 1.0000, Val: 0.7320, Test: 0.7420\n",
      "Epoch: 214,train_loss:0.0476684, Train: 1.0000, Val: 0.7280, Test: 0.7540\n",
      "Epoch: 215,train_loss:0.0818767, Train: 1.0000, Val: 0.7240, Test: 0.7550\n",
      "Epoch: 216,train_loss:0.0631476, Train: 1.0000, Val: 0.7160, Test: 0.7510\n",
      "Epoch: 217,train_loss:0.0979322, Train: 1.0000, Val: 0.7100, Test: 0.7310\n",
      "Epoch: 218,train_loss:0.0842486, Train: 1.0000, Val: 0.7080, Test: 0.7220\n",
      "Epoch: 219,train_loss:0.0903172, Train: 1.0000, Val: 0.7100, Test: 0.7260\n",
      "Epoch: 220,train_loss:0.0847946, Train: 1.0000, Val: 0.6960, Test: 0.7230\n",
      "Epoch: 221,train_loss:0.0592729, Train: 1.0000, Val: 0.7040, Test: 0.7300\n",
      "Epoch: 222,train_loss:0.0798193, Train: 1.0000, Val: 0.7100, Test: 0.7410\n",
      "Epoch: 223,train_loss:0.0537674, Train: 1.0000, Val: 0.7220, Test: 0.7380\n",
      "Epoch: 224,train_loss:0.0639971, Train: 1.0000, Val: 0.7200, Test: 0.7400\n",
      "Epoch: 225,train_loss:0.0562731, Train: 1.0000, Val: 0.7160, Test: 0.7340\n",
      "Epoch: 226,train_loss:0.0622825, Train: 1.0000, Val: 0.7140, Test: 0.7280\n",
      "Epoch: 227,train_loss:0.0417862, Train: 1.0000, Val: 0.7140, Test: 0.7290\n",
      "Epoch: 228,train_loss:0.0378121, Train: 1.0000, Val: 0.7000, Test: 0.7300\n",
      "Epoch: 229,train_loss:0.0353346, Train: 1.0000, Val: 0.6940, Test: 0.7150\n",
      "Epoch: 230,train_loss:0.0738715, Train: 1.0000, Val: 0.7020, Test: 0.7360\n",
      "Epoch: 231,train_loss:0.0585886, Train: 1.0000, Val: 0.7220, Test: 0.7530\n",
      "Epoch: 232,train_loss:0.0638091, Train: 1.0000, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 233,train_loss:0.0826391, Train: 1.0000, Val: 0.7280, Test: 0.7440\n",
      "Epoch: 234,train_loss:0.0671989, Train: 1.0000, Val: 0.7120, Test: 0.7300\n",
      "Epoch: 235,train_loss:0.0328182, Train: 1.0000, Val: 0.6940, Test: 0.7220\n",
      "Epoch: 236,train_loss:0.0791700, Train: 1.0000, Val: 0.6960, Test: 0.7200\n",
      "Epoch: 237,train_loss:0.0694382, Train: 1.0000, Val: 0.7000, Test: 0.7320\n",
      "Epoch: 238,train_loss:0.0503779, Train: 1.0000, Val: 0.7080, Test: 0.7490\n",
      "Epoch: 239,train_loss:0.0472777, Train: 1.0000, Val: 0.7260, Test: 0.7480\n",
      "Epoch: 240,train_loss:0.0788871, Train: 1.0000, Val: 0.7260, Test: 0.7530\n",
      "Epoch: 241,train_loss:0.0532041, Train: 1.0000, Val: 0.7280, Test: 0.7420\n",
      "Epoch: 242,train_loss:0.0873282, Train: 1.0000, Val: 0.7180, Test: 0.7290\n",
      "Epoch: 243,train_loss:0.0482357, Train: 1.0000, Val: 0.6840, Test: 0.7030\n",
      "Epoch: 244,train_loss:0.0730870, Train: 1.0000, Val: 0.6800, Test: 0.6990\n",
      "Epoch: 245,train_loss:0.0501687, Train: 1.0000, Val: 0.6860, Test: 0.7000\n",
      "Epoch: 246,train_loss:0.0939299, Train: 1.0000, Val: 0.7260, Test: 0.7280\n",
      "Epoch: 247,train_loss:0.0573554, Train: 1.0000, Val: 0.7340, Test: 0.7470\n",
      "Epoch: 248,train_loss:0.0593140, Train: 1.0000, Val: 0.7360, Test: 0.7490\n",
      "Epoch: 249,train_loss:0.0949275, Train: 1.0000, Val: 0.7400, Test: 0.7520\n",
      "Epoch: 250,train_loss:0.0773873, Train: 1.0000, Val: 0.7280, Test: 0.7350\n",
      "Epoch: 251,train_loss:0.0474171, Train: 1.0000, Val: 0.6900, Test: 0.7130\n",
      "Epoch: 252,train_loss:0.0876002, Train: 1.0000, Val: 0.6760, Test: 0.7080\n",
      "Epoch: 253,train_loss:0.0612991, Train: 1.0000, Val: 0.6880, Test: 0.7170\n",
      "Epoch: 254,train_loss:0.0608906, Train: 1.0000, Val: 0.7140, Test: 0.7350\n",
      "Epoch: 255,train_loss:0.0514916, Train: 1.0000, Val: 0.7280, Test: 0.7400\n",
      "Epoch: 256,train_loss:0.0921809, Train: 1.0000, Val: 0.7240, Test: 0.7440\n",
      "Epoch: 257,train_loss:0.0375727, Train: 1.0000, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 258,train_loss:0.0563261, Train: 1.0000, Val: 0.7100, Test: 0.7360\n",
      "Epoch: 259,train_loss:0.0833429, Train: 1.0000, Val: 0.7100, Test: 0.7350\n",
      "Epoch: 260,train_loss:0.0615521, Train: 1.0000, Val: 0.7060, Test: 0.7340\n",
      "Epoch: 261,train_loss:0.0722323, Train: 1.0000, Val: 0.7120, Test: 0.7330\n",
      "Epoch: 262,train_loss:0.0649540, Train: 1.0000, Val: 0.7260, Test: 0.7390\n",
      "Epoch: 263,train_loss:0.0443812, Train: 1.0000, Val: 0.7300, Test: 0.7380\n",
      "Epoch: 264,train_loss:0.0510241, Train: 1.0000, Val: 0.7320, Test: 0.7460\n",
      "Epoch: 265,train_loss:0.0569160, Train: 1.0000, Val: 0.7320, Test: 0.7520\n",
      "Epoch: 266,train_loss:0.0343673, Train: 1.0000, Val: 0.7260, Test: 0.7530\n",
      "Epoch: 267,train_loss:0.0520140, Train: 1.0000, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 268,train_loss:0.0525069, Train: 1.0000, Val: 0.7340, Test: 0.7560\n",
      "Epoch: 269,train_loss:0.0360394, Train: 1.0000, Val: 0.7320, Test: 0.7600\n",
      "Epoch: 270,train_loss:0.0641559, Train: 1.0000, Val: 0.7360, Test: 0.7610\n",
      "Epoch: 271,train_loss:0.0774675, Train: 1.0000, Val: 0.7380, Test: 0.7570\n",
      "Epoch: 272,train_loss:0.0674468, Train: 1.0000, Val: 0.7260, Test: 0.7420\n",
      "Epoch: 273,train_loss:0.0608042, Train: 1.0000, Val: 0.7180, Test: 0.7320\n",
      "Epoch: 274,train_loss:0.0648442, Train: 1.0000, Val: 0.7140, Test: 0.7220\n",
      "Epoch: 275,train_loss:0.0448919, Train: 1.0000, Val: 0.7160, Test: 0.7220\n",
      "Epoch: 276,train_loss:0.0614630, Train: 1.0000, Val: 0.7220, Test: 0.7390\n",
      "Epoch: 277,train_loss:0.0408456, Train: 1.0000, Val: 0.7340, Test: 0.7490\n",
      "Epoch: 278,train_loss:0.0341391, Train: 1.0000, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 279,train_loss:0.0364939, Train: 1.0000, Val: 0.7320, Test: 0.7560\n",
      "Epoch: 280,train_loss:0.0735863, Train: 1.0000, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 281,train_loss:0.0596471, Train: 1.0000, Val: 0.7320, Test: 0.7520\n",
      "Epoch: 282,train_loss:0.0489700, Train: 1.0000, Val: 0.7280, Test: 0.7510\n",
      "Epoch: 283,train_loss:0.0308697, Train: 1.0000, Val: 0.7180, Test: 0.7550\n",
      "Epoch: 284,train_loss:0.0512918, Train: 1.0000, Val: 0.7260, Test: 0.7570\n",
      "Epoch: 285,train_loss:0.0432761, Train: 1.0000, Val: 0.7380, Test: 0.7650\n",
      "Epoch: 286,train_loss:0.0417021, Train: 1.0000, Val: 0.7400, Test: 0.7680\n",
      "Epoch: 287,train_loss:0.0575006, Train: 1.0000, Val: 0.7480, Test: 0.7630\n",
      "Epoch: 288,train_loss:0.0379999, Train: 1.0000, Val: 0.7380, Test: 0.7610\n",
      "Epoch: 289,train_loss:0.0675901, Train: 1.0000, Val: 0.7340, Test: 0.7540\n",
      "Epoch: 290,train_loss:0.0539289, Train: 1.0000, Val: 0.7320, Test: 0.7510\n",
      "Epoch: 291,train_loss:0.0439912, Train: 1.0000, Val: 0.7340, Test: 0.7510\n",
      "Epoch: 292,train_loss:0.0409966, Train: 1.0000, Val: 0.7360, Test: 0.7510\n",
      "Epoch: 293,train_loss:0.0682400, Train: 1.0000, Val: 0.7380, Test: 0.7520\n",
      "Epoch: 294,train_loss:0.0539125, Train: 1.0000, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 295,train_loss:0.0576835, Train: 1.0000, Val: 0.7360, Test: 0.7580\n",
      "Epoch: 296,train_loss:0.0463513, Train: 1.0000, Val: 0.7260, Test: 0.7490\n",
      "Epoch: 297,train_loss:0.0582245, Train: 1.0000, Val: 0.7200, Test: 0.7470\n",
      "Epoch: 298,train_loss:0.0587635, Train: 1.0000, Val: 0.7020, Test: 0.7460\n",
      "Epoch: 299,train_loss:0.0803971, Train: 1.0000, Val: 0.7080, Test: 0.7450\n",
      "Epoch: 300,train_loss:0.0505749, Train: 1.0000, Val: 0.7040, Test: 0.7400\n",
      "Epoch: 301,train_loss:0.0550749, Train: 1.0000, Val: 0.7120, Test: 0.7390\n",
      "Epoch: 302,train_loss:0.0720424, Train: 1.0000, Val: 0.7160, Test: 0.7410\n",
      "Epoch: 303,train_loss:0.0505125, Train: 1.0000, Val: 0.7220, Test: 0.7430\n",
      "Epoch: 304,train_loss:0.0400912, Train: 1.0000, Val: 0.7240, Test: 0.7490\n",
      "Epoch: 305,train_loss:0.0483704, Train: 1.0000, Val: 0.7220, Test: 0.7490\n",
      "Epoch: 306,train_loss:0.0538529, Train: 1.0000, Val: 0.7180, Test: 0.7420\n",
      "Epoch: 307,train_loss:0.0410304, Train: 1.0000, Val: 0.7080, Test: 0.7370\n",
      "Epoch: 308,train_loss:0.0538335, Train: 1.0000, Val: 0.7020, Test: 0.7330\n",
      "Epoch: 309,train_loss:0.0362699, Train: 1.0000, Val: 0.7020, Test: 0.7360\n",
      "Epoch: 310,train_loss:0.0503508, Train: 1.0000, Val: 0.7280, Test: 0.7450\n",
      "Epoch: 311,train_loss:0.0886950, Train: 1.0000, Val: 0.7120, Test: 0.7480\n",
      "Epoch: 312,train_loss:0.0435961, Train: 1.0000, Val: 0.7220, Test: 0.7510\n",
      "Epoch: 313,train_loss:0.0375606, Train: 1.0000, Val: 0.7260, Test: 0.7550\n",
      "Epoch: 314,train_loss:0.0463722, Train: 1.0000, Val: 0.7320, Test: 0.7600\n",
      "Epoch: 315,train_loss:0.0344798, Train: 1.0000, Val: 0.7320, Test: 0.7630\n",
      "Epoch: 316,train_loss:0.0545473, Train: 1.0000, Val: 0.7360, Test: 0.7580\n",
      "Epoch: 317,train_loss:0.0347495, Train: 1.0000, Val: 0.7360, Test: 0.7590\n",
      "Epoch: 318,train_loss:0.0475990, Train: 1.0000, Val: 0.7240, Test: 0.7610\n",
      "Epoch: 319,train_loss:0.0409063, Train: 1.0000, Val: 0.7120, Test: 0.7460\n",
      "Epoch: 320,train_loss:0.0453588, Train: 1.0000, Val: 0.7140, Test: 0.7440\n",
      "Epoch: 321,train_loss:0.0347493, Train: 1.0000, Val: 0.7080, Test: 0.7400\n",
      "Epoch: 322,train_loss:0.0471603, Train: 1.0000, Val: 0.7100, Test: 0.7440\n",
      "Epoch: 323,train_loss:0.0552839, Train: 1.0000, Val: 0.7060, Test: 0.7400\n",
      "Epoch: 324,train_loss:0.0653944, Train: 1.0000, Val: 0.7260, Test: 0.7450\n",
      "Epoch: 325,train_loss:0.0468024, Train: 1.0000, Val: 0.7320, Test: 0.7500\n",
      "Epoch: 326,train_loss:0.0614880, Train: 1.0000, Val: 0.7380, Test: 0.7640\n",
      "Epoch: 327,train_loss:0.0612774, Train: 1.0000, Val: 0.7360, Test: 0.7650\n",
      "Epoch: 328,train_loss:0.0557375, Train: 1.0000, Val: 0.7240, Test: 0.7510\n",
      "Epoch: 329,train_loss:0.0346384, Train: 1.0000, Val: 0.7100, Test: 0.7460\n",
      "Epoch: 330,train_loss:0.0526413, Train: 1.0000, Val: 0.7100, Test: 0.7460\n",
      "Epoch: 331,train_loss:0.0423062, Train: 1.0000, Val: 0.7240, Test: 0.7560\n",
      "Epoch: 332,train_loss:0.0322231, Train: 1.0000, Val: 0.7360, Test: 0.7710\n",
      "Epoch: 333,train_loss:0.0439310, Train: 1.0000, Val: 0.7420, Test: 0.7760\n",
      "Epoch: 334,train_loss:0.0380341, Train: 1.0000, Val: 0.7460, Test: 0.7790\n",
      "Epoch: 335,train_loss:0.0308111, Train: 1.0000, Val: 0.7440, Test: 0.7750\n",
      "Epoch: 336,train_loss:0.0392442, Train: 1.0000, Val: 0.7340, Test: 0.7690\n",
      "Epoch: 337,train_loss:0.0412283, Train: 1.0000, Val: 0.7260, Test: 0.7630\n",
      "Epoch: 338,train_loss:0.0337396, Train: 1.0000, Val: 0.7280, Test: 0.7630\n",
      "Epoch: 339,train_loss:0.0411182, Train: 1.0000, Val: 0.7320, Test: 0.7640\n",
      "Epoch: 340,train_loss:0.0305872, Train: 1.0000, Val: 0.7240, Test: 0.7630\n",
      "Epoch: 341,train_loss:0.0525988, Train: 1.0000, Val: 0.7400, Test: 0.7590\n",
      "Epoch: 342,train_loss:0.0386095, Train: 1.0000, Val: 0.7520, Test: 0.7630\n",
      "Epoch: 343,train_loss:0.0564123, Train: 1.0000, Val: 0.7460, Test: 0.7640\n",
      "Epoch: 344,train_loss:0.0372249, Train: 1.0000, Val: 0.7240, Test: 0.7530\n",
      "Epoch: 345,train_loss:0.0415404, Train: 1.0000, Val: 0.7100, Test: 0.7450\n",
      "Epoch: 346,train_loss:0.0354543, Train: 1.0000, Val: 0.7120, Test: 0.7440\n",
      "Epoch: 347,train_loss:0.0324684, Train: 1.0000, Val: 0.7240, Test: 0.7540\n",
      "Epoch: 348,train_loss:0.0397599, Train: 1.0000, Val: 0.7480, Test: 0.7580\n",
      "Epoch: 349,train_loss:0.0437482, Train: 1.0000, Val: 0.7280, Test: 0.7500\n",
      "Epoch: 350,train_loss:0.0829175, Train: 1.0000, Val: 0.7460, Test: 0.7670\n",
      "Epoch: 351,train_loss:0.0272392, Train: 1.0000, Val: 0.6960, Test: 0.7380\n",
      "Epoch: 352,train_loss:0.0472616, Train: 1.0000, Val: 0.6960, Test: 0.7440\n",
      "Epoch: 353,train_loss:0.0590696, Train: 1.0000, Val: 0.7300, Test: 0.7650\n",
      "Epoch: 354,train_loss:0.0625762, Train: 1.0000, Val: 0.7580, Test: 0.7830\n",
      "Epoch: 355,train_loss:0.0388518, Train: 1.0000, Val: 0.7760, Test: 0.7740\n",
      "Epoch: 356,train_loss:0.0537798, Train: 1.0000, Val: 0.7560, Test: 0.7640\n",
      "Epoch: 357,train_loss:0.0542085, Train: 1.0000, Val: 0.7520, Test: 0.7610\n",
      "Epoch: 358,train_loss:0.0374332, Train: 1.0000, Val: 0.7560, Test: 0.7650\n",
      "Epoch: 359,train_loss:0.0334399, Train: 1.0000, Val: 0.7520, Test: 0.7650\n",
      "Epoch: 360,train_loss:0.0397911, Train: 1.0000, Val: 0.7560, Test: 0.7730\n",
      "Epoch: 361,train_loss:0.0373282, Train: 1.0000, Val: 0.7500, Test: 0.7710\n",
      "Epoch: 362,train_loss:0.0324750, Train: 1.0000, Val: 0.7420, Test: 0.7730\n",
      "Epoch: 363,train_loss:0.0440386, Train: 1.0000, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 364,train_loss:0.0335861, Train: 1.0000, Val: 0.7520, Test: 0.7700\n",
      "Epoch: 365,train_loss:0.0528845, Train: 1.0000, Val: 0.7620, Test: 0.7750\n",
      "Epoch: 366,train_loss:0.0341369, Train: 1.0000, Val: 0.7580, Test: 0.7700\n",
      "Epoch: 367,train_loss:0.0464453, Train: 1.0000, Val: 0.7620, Test: 0.7790\n",
      "Epoch: 368,train_loss:0.0441921, Train: 1.0000, Val: 0.7520, Test: 0.7720\n",
      "Epoch: 369,train_loss:0.0479355, Train: 1.0000, Val: 0.7600, Test: 0.7810\n",
      "Epoch: 370,train_loss:0.0330699, Train: 1.0000, Val: 0.7600, Test: 0.7740\n",
      "Epoch: 371,train_loss:0.0330645, Train: 1.0000, Val: 0.7560, Test: 0.7720\n",
      "Epoch: 372,train_loss:0.0258120, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 373,train_loss:0.0277261, Train: 1.0000, Val: 0.7740, Test: 0.7800\n",
      "Epoch: 374,train_loss:0.0451640, Train: 1.0000, Val: 0.7600, Test: 0.7850\n",
      "Epoch: 375,train_loss:0.0550660, Train: 1.0000, Val: 0.7620, Test: 0.7830\n",
      "Epoch: 376,train_loss:0.0303166, Train: 1.0000, Val: 0.7620, Test: 0.7750\n",
      "Epoch: 377,train_loss:0.0358468, Train: 1.0000, Val: 0.7580, Test: 0.7670\n",
      "Epoch: 378,train_loss:0.0383875, Train: 1.0000, Val: 0.7540, Test: 0.7640\n",
      "Epoch: 379,train_loss:0.0322215, Train: 1.0000, Val: 0.7580, Test: 0.7720\n",
      "Epoch: 380,train_loss:0.0313826, Train: 1.0000, Val: 0.7720, Test: 0.7680\n",
      "Epoch: 381,train_loss:0.0424171, Train: 1.0000, Val: 0.7620, Test: 0.7720\n",
      "Epoch: 382,train_loss:0.0312519, Train: 1.0000, Val: 0.7500, Test: 0.7590\n",
      "Epoch: 383,train_loss:0.0416215, Train: 1.0000, Val: 0.7360, Test: 0.7420\n",
      "Epoch: 384,train_loss:0.0481419, Train: 1.0000, Val: 0.7380, Test: 0.7510\n",
      "Epoch: 385,train_loss:0.0560937, Train: 1.0000, Val: 0.7500, Test: 0.7480\n",
      "Epoch: 386,train_loss:0.0200355, Train: 1.0000, Val: 0.7600, Test: 0.7550\n",
      "Epoch: 387,train_loss:0.0443744, Train: 1.0000, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 388,train_loss:0.0395365, Train: 1.0000, Val: 0.7640, Test: 0.7780\n",
      "Epoch: 389,train_loss:0.0263618, Train: 1.0000, Val: 0.7660, Test: 0.7750\n",
      "Epoch: 390,train_loss:0.0384107, Train: 1.0000, Val: 0.7400, Test: 0.7700\n",
      "Epoch: 391,train_loss:0.0443584, Train: 1.0000, Val: 0.7480, Test: 0.7770\n",
      "Epoch: 392,train_loss:0.0354797, Train: 1.0000, Val: 0.7540, Test: 0.7820\n",
      "Epoch: 393,train_loss:0.0451451, Train: 1.0000, Val: 0.7640, Test: 0.7860\n",
      "Epoch: 394,train_loss:0.0631881, Train: 1.0000, Val: 0.7640, Test: 0.7940\n",
      "Epoch: 395,train_loss:0.0413173, Train: 1.0000, Val: 0.7780, Test: 0.7910\n",
      "Epoch: 396,train_loss:0.0505540, Train: 1.0000, Val: 0.7700, Test: 0.7920\n",
      "Epoch: 397,train_loss:0.0330196, Train: 1.0000, Val: 0.7680, Test: 0.7860\n",
      "Epoch: 398,train_loss:0.0450809, Train: 1.0000, Val: 0.7600, Test: 0.7750\n",
      "Epoch: 399,train_loss:0.0310090, Train: 1.0000, Val: 0.7260, Test: 0.7510\n",
      "Epoch: 400,train_loss:0.0685114, Train: 1.0000, Val: 0.7540, Test: 0.7640\n",
      "Epoch: 401,train_loss:0.0482148, Train: 1.0000, Val: 0.7600, Test: 0.7680\n",
      "Epoch: 402,train_loss:0.0214483, Train: 1.0000, Val: 0.7600, Test: 0.7670\n",
      "Epoch: 403,train_loss:0.0423861, Train: 1.0000, Val: 0.7480, Test: 0.7540\n",
      "Epoch: 404,train_loss:0.0587192, Train: 1.0000, Val: 0.7460, Test: 0.7690\n",
      "Epoch: 405,train_loss:0.0490152, Train: 1.0000, Val: 0.7360, Test: 0.7600\n",
      "Epoch: 406,train_loss:0.0383204, Train: 1.0000, Val: 0.7280, Test: 0.7520\n",
      "Epoch: 407,train_loss:0.0395288, Train: 1.0000, Val: 0.7280, Test: 0.7470\n",
      "Epoch: 408,train_loss:0.0386417, Train: 1.0000, Val: 0.7380, Test: 0.7450\n",
      "Epoch: 409,train_loss:0.0300408, Train: 1.0000, Val: 0.7460, Test: 0.7620\n",
      "Epoch: 410,train_loss:0.0429987, Train: 1.0000, Val: 0.7640, Test: 0.7720\n",
      "Epoch: 411,train_loss:0.0248329, Train: 1.0000, Val: 0.7700, Test: 0.7810\n",
      "Epoch: 412,train_loss:0.0311726, Train: 1.0000, Val: 0.7640, Test: 0.7850\n",
      "Epoch: 413,train_loss:0.0286954, Train: 1.0000, Val: 0.7660, Test: 0.7780\n",
      "Epoch: 414,train_loss:0.0441538, Train: 1.0000, Val: 0.7660, Test: 0.7730\n",
      "Epoch: 415,train_loss:0.0460715, Train: 1.0000, Val: 0.7600, Test: 0.7770\n",
      "Epoch: 416,train_loss:0.0473371, Train: 1.0000, Val: 0.7600, Test: 0.7670\n",
      "Epoch: 417,train_loss:0.0383304, Train: 1.0000, Val: 0.7540, Test: 0.7620\n",
      "Epoch: 418,train_loss:0.0302394, Train: 1.0000, Val: 0.7480, Test: 0.7690\n",
      "Epoch: 419,train_loss:0.0330962, Train: 1.0000, Val: 0.7440, Test: 0.7680\n",
      "Epoch: 420,train_loss:0.0571239, Train: 1.0000, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 421,train_loss:0.0345043, Train: 1.0000, Val: 0.7560, Test: 0.7640\n",
      "Epoch: 422,train_loss:0.0430259, Train: 1.0000, Val: 0.7520, Test: 0.7530\n",
      "Epoch: 423,train_loss:0.0625919, Train: 1.0000, Val: 0.7380, Test: 0.7580\n",
      "Epoch: 424,train_loss:0.0372543, Train: 1.0000, Val: 0.7360, Test: 0.7480\n",
      "Epoch: 425,train_loss:0.0444457, Train: 1.0000, Val: 0.7260, Test: 0.7340\n",
      "Epoch: 426,train_loss:0.0301585, Train: 1.0000, Val: 0.7160, Test: 0.7380\n",
      "Epoch: 427,train_loss:0.0245470, Train: 1.0000, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 428,train_loss:0.0437031, Train: 1.0000, Val: 0.7400, Test: 0.7610\n",
      "Epoch: 429,train_loss:0.0328433, Train: 1.0000, Val: 0.7480, Test: 0.7690\n",
      "Epoch: 430,train_loss:0.0322783, Train: 1.0000, Val: 0.7300, Test: 0.7420\n",
      "Epoch: 431,train_loss:0.0402163, Train: 1.0000, Val: 0.7320, Test: 0.7460\n",
      "Epoch: 432,train_loss:0.0319481, Train: 1.0000, Val: 0.7380, Test: 0.7530\n",
      "Epoch: 433,train_loss:0.0600344, Train: 1.0000, Val: 0.7440, Test: 0.7560\n",
      "Epoch: 434,train_loss:0.0318862, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 435,train_loss:0.0319378, Train: 1.0000, Val: 0.7380, Test: 0.7560\n",
      "Epoch: 436,train_loss:0.0333778, Train: 1.0000, Val: 0.7420, Test: 0.7530\n",
      "Epoch: 437,train_loss:0.0609862, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 438,train_loss:0.0254592, Train: 1.0000, Val: 0.7420, Test: 0.7640\n",
      "Epoch: 439,train_loss:0.0250667, Train: 1.0000, Val: 0.7420, Test: 0.7610\n",
      "Epoch: 440,train_loss:0.0237966, Train: 1.0000, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 441,train_loss:0.0346211, Train: 1.0000, Val: 0.7300, Test: 0.7580\n",
      "Epoch: 442,train_loss:0.0314449, Train: 1.0000, Val: 0.7460, Test: 0.7610\n",
      "Epoch: 443,train_loss:0.0356754, Train: 1.0000, Val: 0.7400, Test: 0.7570\n",
      "Epoch: 444,train_loss:0.0413553, Train: 1.0000, Val: 0.7320, Test: 0.7470\n",
      "Epoch: 445,train_loss:0.0464676, Train: 1.0000, Val: 0.7260, Test: 0.7400\n",
      "Epoch: 446,train_loss:0.0263423, Train: 1.0000, Val: 0.7360, Test: 0.7430\n",
      "Epoch: 447,train_loss:0.0444832, Train: 1.0000, Val: 0.7480, Test: 0.7620\n",
      "Epoch: 448,train_loss:0.0463382, Train: 1.0000, Val: 0.7500, Test: 0.7740\n",
      "Epoch: 449,train_loss:0.0608197, Train: 1.0000, Val: 0.7380, Test: 0.7630\n",
      "Epoch: 450,train_loss:0.0414630, Train: 1.0000, Val: 0.7400, Test: 0.7660\n",
      "Epoch: 451,train_loss:0.0457759, Train: 1.0000, Val: 0.7560, Test: 0.7640\n",
      "Epoch: 452,train_loss:0.0297344, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch: 453,train_loss:0.0310295, Train: 1.0000, Val: 0.7220, Test: 0.7260\n",
      "Epoch: 454,train_loss:0.0362378, Train: 1.0000, Val: 0.6980, Test: 0.7030\n",
      "Epoch: 455,train_loss:0.0432538, Train: 1.0000, Val: 0.7180, Test: 0.7110\n",
      "Epoch: 456,train_loss:0.0924871, Train: 1.0000, Val: 0.7240, Test: 0.7420\n",
      "Epoch: 457,train_loss:0.0329407, Train: 1.0000, Val: 0.7140, Test: 0.7110\n",
      "Epoch: 458,train_loss:0.1074249, Train: 1.0000, Val: 0.7240, Test: 0.7580\n",
      "Epoch: 459,train_loss:0.0444049, Train: 0.9929, Val: 0.6920, Test: 0.7260\n",
      "Epoch: 460,train_loss:0.0596611, Train: 0.9929, Val: 0.6920, Test: 0.7210\n",
      "Epoch: 461,train_loss:0.1011913, Train: 1.0000, Val: 0.7320, Test: 0.7430\n",
      "Epoch: 462,train_loss:0.0265583, Train: 1.0000, Val: 0.7240, Test: 0.7200\n",
      "Epoch: 463,train_loss:0.0609046, Train: 1.0000, Val: 0.6900, Test: 0.6960\n",
      "Epoch: 464,train_loss:0.1047840, Train: 1.0000, Val: 0.7120, Test: 0.7050\n",
      "Epoch: 465,train_loss:0.0565197, Train: 1.0000, Val: 0.7100, Test: 0.7200\n",
      "Epoch: 466,train_loss:0.0550505, Train: 1.0000, Val: 0.7200, Test: 0.7320\n",
      "Epoch: 467,train_loss:0.0605149, Train: 1.0000, Val: 0.7220, Test: 0.7570\n",
      "Epoch: 468,train_loss:0.0612241, Train: 1.0000, Val: 0.7280, Test: 0.7580\n",
      "Epoch: 469,train_loss:0.0704255, Train: 1.0000, Val: 0.7620, Test: 0.7590\n",
      "Epoch: 470,train_loss:0.0355813, Train: 1.0000, Val: 0.7320, Test: 0.7350\n",
      "Epoch: 471,train_loss:0.0390461, Train: 1.0000, Val: 0.6820, Test: 0.6860\n",
      "Epoch: 472,train_loss:0.0596987, Train: 1.0000, Val: 0.6920, Test: 0.6880\n",
      "Epoch: 473,train_loss:0.0895594, Train: 1.0000, Val: 0.7440, Test: 0.7430\n",
      "Epoch: 474,train_loss:0.0281870, Train: 1.0000, Val: 0.7340, Test: 0.7490\n",
      "Epoch: 475,train_loss:0.0570755, Train: 1.0000, Val: 0.7240, Test: 0.7430\n",
      "Epoch: 476,train_loss:0.0445492, Train: 1.0000, Val: 0.7420, Test: 0.7640\n",
      "Epoch: 477,train_loss:0.0634611, Train: 1.0000, Val: 0.7360, Test: 0.7810\n",
      "Epoch: 478,train_loss:0.0228783, Train: 1.0000, Val: 0.7540, Test: 0.7860\n",
      "Epoch: 479,train_loss:0.0735959, Train: 1.0000, Val: 0.7420, Test: 0.7680\n",
      "Epoch: 480,train_loss:0.0288076, Train: 1.0000, Val: 0.7220, Test: 0.7160\n",
      "Epoch: 481,train_loss:0.0380385, Train: 0.9929, Val: 0.6880, Test: 0.6700\n",
      "Epoch: 482,train_loss:0.0752240, Train: 1.0000, Val: 0.7020, Test: 0.7020\n",
      "Epoch: 483,train_loss:0.0520382, Train: 1.0000, Val: 0.7340, Test: 0.7390\n",
      "Epoch: 484,train_loss:0.0371758, Train: 1.0000, Val: 0.7600, Test: 0.7620\n",
      "Epoch: 485,train_loss:0.0243590, Train: 1.0000, Val: 0.7600, Test: 0.7690\n",
      "Epoch: 486,train_loss:0.0643836, Train: 1.0000, Val: 0.7520, Test: 0.7710\n",
      "Epoch: 487,train_loss:0.0543995, Train: 1.0000, Val: 0.7500, Test: 0.7740\n",
      "Epoch: 488,train_loss:0.0181377, Train: 1.0000, Val: 0.7540, Test: 0.7730\n",
      "Epoch: 489,train_loss:0.0242709, Train: 1.0000, Val: 0.7540, Test: 0.7630\n",
      "Epoch: 490,train_loss:0.0458446, Train: 1.0000, Val: 0.7480, Test: 0.7670\n",
      "Epoch: 491,train_loss:0.0194421, Train: 1.0000, Val: 0.7420, Test: 0.7530\n",
      "Epoch: 492,train_loss:0.0481211, Train: 1.0000, Val: 0.7320, Test: 0.7420\n",
      "Epoch: 493,train_loss:0.0377535, Train: 1.0000, Val: 0.7340, Test: 0.7330\n",
      "Epoch: 494,train_loss:0.0372848, Train: 1.0000, Val: 0.7380, Test: 0.7410\n",
      "Epoch: 495,train_loss:0.0562081, Train: 1.0000, Val: 0.7480, Test: 0.7620\n",
      "Epoch: 496,train_loss:0.0243716, Train: 1.0000, Val: 0.7540, Test: 0.7720\n",
      "Epoch: 497,train_loss:0.0225919, Train: 1.0000, Val: 0.7440, Test: 0.7690\n",
      "Epoch: 498,train_loss:0.0197124, Train: 1.0000, Val: 0.7260, Test: 0.7550\n",
      "Epoch: 499,train_loss:0.0271450, Train: 1.0000, Val: 0.7220, Test: 0.7510\n",
      "Epoch: 500,train_loss:0.0661751, Train: 1.0000, Val: 0.7580, Test: 0.7700\n",
      "Epoch: 501,train_loss:0.0289084, Train: 1.0000, Val: 0.7520, Test: 0.7670\n",
      "Epoch: 502,train_loss:0.0221840, Train: 1.0000, Val: 0.7400, Test: 0.7600\n",
      "Epoch: 503,train_loss:0.0360409, Train: 1.0000, Val: 0.7500, Test: 0.7540\n",
      "Epoch: 504,train_loss:0.0478988, Train: 1.0000, Val: 0.7520, Test: 0.7620\n",
      "Epoch: 505,train_loss:0.0339507, Train: 1.0000, Val: 0.7460, Test: 0.7610\n",
      "Epoch: 506,train_loss:0.0234414, Train: 1.0000, Val: 0.7300, Test: 0.7460\n",
      "Epoch: 507,train_loss:0.0308213, Train: 1.0000, Val: 0.7220, Test: 0.7390\n",
      "Epoch: 508,train_loss:0.0342482, Train: 1.0000, Val: 0.7160, Test: 0.7330\n",
      "Epoch: 509,train_loss:0.0404547, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch: 510,train_loss:0.0256307, Train: 1.0000, Val: 0.7320, Test: 0.7390\n",
      "Epoch: 511,train_loss:0.0222794, Train: 1.0000, Val: 0.7400, Test: 0.7450\n",
      "Epoch: 512,train_loss:0.0419369, Train: 1.0000, Val: 0.7360, Test: 0.7450\n",
      "Epoch: 513,train_loss:0.0226121, Train: 1.0000, Val: 0.7380, Test: 0.7520\n",
      "Epoch: 514,train_loss:0.0476618, Train: 1.0000, Val: 0.7540, Test: 0.7650\n",
      "Epoch: 515,train_loss:0.0330772, Train: 1.0000, Val: 0.7440, Test: 0.7630\n",
      "Epoch: 516,train_loss:0.0287823, Train: 1.0000, Val: 0.7440, Test: 0.7650\n",
      "Epoch: 517,train_loss:0.0227759, Train: 1.0000, Val: 0.7460, Test: 0.7610\n",
      "Epoch: 518,train_loss:0.0259551, Train: 1.0000, Val: 0.7380, Test: 0.7610\n",
      "Epoch: 519,train_loss:0.0307571, Train: 1.0000, Val: 0.7340, Test: 0.7480\n",
      "Epoch: 520,train_loss:0.0331330, Train: 1.0000, Val: 0.7320, Test: 0.7450\n",
      "Epoch: 521,train_loss:0.0327334, Train: 1.0000, Val: 0.7480, Test: 0.7610\n",
      "Epoch: 522,train_loss:0.0252868, Train: 1.0000, Val: 0.7540, Test: 0.7720\n",
      "Epoch: 523,train_loss:0.0218425, Train: 1.0000, Val: 0.7600, Test: 0.7670\n",
      "Epoch: 524,train_loss:0.0256025, Train: 1.0000, Val: 0.7600, Test: 0.7610\n",
      "Epoch: 525,train_loss:0.0355846, Train: 1.0000, Val: 0.7620, Test: 0.7580\n",
      "Epoch: 526,train_loss:0.0387438, Train: 1.0000, Val: 0.7520, Test: 0.7680\n",
      "Epoch: 527,train_loss:0.0194887, Train: 1.0000, Val: 0.7500, Test: 0.7610\n",
      "Epoch: 528,train_loss:0.0203470, Train: 1.0000, Val: 0.7320, Test: 0.7530\n",
      "Epoch: 529,train_loss:0.0203980, Train: 1.0000, Val: 0.7220, Test: 0.7470\n",
      "Epoch: 530,train_loss:0.0469499, Train: 1.0000, Val: 0.7400, Test: 0.7560\n",
      "Epoch: 531,train_loss:0.0248595, Train: 1.0000, Val: 0.7540, Test: 0.7680\n",
      "Epoch: 532,train_loss:0.0279936, Train: 1.0000, Val: 0.7540, Test: 0.7680\n",
      "Epoch: 533,train_loss:0.0267378, Train: 1.0000, Val: 0.7560, Test: 0.7710\n",
      "Epoch: 534,train_loss:0.0298973, Train: 1.0000, Val: 0.7540, Test: 0.7710\n",
      "Epoch: 535,train_loss:0.0217384, Train: 1.0000, Val: 0.7580, Test: 0.7690\n",
      "Epoch: 536,train_loss:0.0322564, Train: 1.0000, Val: 0.7540, Test: 0.7630\n",
      "Epoch: 537,train_loss:0.0205507, Train: 1.0000, Val: 0.7580, Test: 0.7560\n",
      "Epoch: 538,train_loss:0.0389452, Train: 1.0000, Val: 0.7500, Test: 0.7580\n",
      "Epoch: 539,train_loss:0.0300791, Train: 1.0000, Val: 0.7360, Test: 0.7490\n",
      "Epoch: 540,train_loss:0.0363792, Train: 1.0000, Val: 0.7460, Test: 0.7520\n",
      "Epoch: 541,train_loss:0.0423055, Train: 1.0000, Val: 0.7520, Test: 0.7580\n",
      "Epoch: 542,train_loss:0.0414022, Train: 1.0000, Val: 0.7500, Test: 0.7640\n",
      "Epoch: 543,train_loss:0.0313926, Train: 1.0000, Val: 0.7540, Test: 0.7620\n",
      "Epoch: 544,train_loss:0.0301030, Train: 1.0000, Val: 0.7400, Test: 0.7560\n",
      "Epoch: 545,train_loss:0.0286496, Train: 1.0000, Val: 0.7520, Test: 0.7580\n",
      "Epoch: 546,train_loss:0.0297174, Train: 1.0000, Val: 0.7420, Test: 0.7530\n",
      "Epoch: 547,train_loss:0.0408298, Train: 1.0000, Val: 0.7140, Test: 0.7210\n",
      "Epoch: 548,train_loss:0.0377710, Train: 1.0000, Val: 0.6940, Test: 0.7130\n",
      "Epoch: 549,train_loss:0.0260982, Train: 1.0000, Val: 0.7100, Test: 0.7260\n",
      "Epoch: 550,train_loss:0.0349089, Train: 1.0000, Val: 0.7240, Test: 0.7420\n",
      "Epoch: 551,train_loss:0.0327108, Train: 1.0000, Val: 0.7460, Test: 0.7650\n",
      "Epoch: 552,train_loss:0.0321369, Train: 1.0000, Val: 0.7500, Test: 0.7520\n",
      "Epoch: 553,train_loss:0.0329976, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
      "Epoch: 554,train_loss:0.0273020, Train: 1.0000, Val: 0.7520, Test: 0.7670\n",
      "Epoch: 555,train_loss:0.0327987, Train: 1.0000, Val: 0.7460, Test: 0.7720\n",
      "Epoch: 556,train_loss:0.0200511, Train: 1.0000, Val: 0.7200, Test: 0.7520\n",
      "Epoch: 557,train_loss:0.0256388, Train: 1.0000, Val: 0.7240, Test: 0.7590\n",
      "Epoch: 558,train_loss:0.0431868, Train: 1.0000, Val: 0.7380, Test: 0.7500\n",
      "Epoch: 559,train_loss:0.0258142, Train: 1.0000, Val: 0.7420, Test: 0.7510\n",
      "Epoch: 560,train_loss:0.0370457, Train: 1.0000, Val: 0.7420, Test: 0.7330\n",
      "Epoch: 561,train_loss:0.0315843, Train: 1.0000, Val: 0.7340, Test: 0.7270\n",
      "Epoch: 562,train_loss:0.0199875, Train: 1.0000, Val: 0.7320, Test: 0.7350\n",
      "Epoch: 563,train_loss:0.0324041, Train: 1.0000, Val: 0.7380, Test: 0.7430\n",
      "Epoch: 564,train_loss:0.0265468, Train: 1.0000, Val: 0.7440, Test: 0.7470\n",
      "Epoch: 565,train_loss:0.0236914, Train: 1.0000, Val: 0.7480, Test: 0.7580\n",
      "Epoch: 566,train_loss:0.0308850, Train: 1.0000, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 567,train_loss:0.0236453, Train: 1.0000, Val: 0.7540, Test: 0.7660\n",
      "Epoch: 568,train_loss:0.0270242, Train: 1.0000, Val: 0.7540, Test: 0.7640\n",
      "Epoch: 569,train_loss:0.0248810, Train: 1.0000, Val: 0.7540, Test: 0.7550\n",
      "Epoch: 570,train_loss:0.0331353, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch: 571,train_loss:0.0303576, Train: 1.0000, Val: 0.7360, Test: 0.7470\n",
      "Epoch: 572,train_loss:0.0199040, Train: 1.0000, Val: 0.7240, Test: 0.7380\n",
      "Epoch: 573,train_loss:0.0481462, Train: 1.0000, Val: 0.7340, Test: 0.7490\n",
      "Epoch: 574,train_loss:0.0227981, Train: 1.0000, Val: 0.7560, Test: 0.7590\n",
      "Epoch: 575,train_loss:0.0286758, Train: 1.0000, Val: 0.7520, Test: 0.7600\n",
      "Epoch: 576,train_loss:0.0555509, Train: 1.0000, Val: 0.7400, Test: 0.7420\n",
      "Epoch: 577,train_loss:0.0204829, Train: 1.0000, Val: 0.7240, Test: 0.7260\n",
      "Epoch: 578,train_loss:0.0517669, Train: 1.0000, Val: 0.7360, Test: 0.7450\n",
      "Epoch: 579,train_loss:0.0232254, Train: 1.0000, Val: 0.7360, Test: 0.7570\n",
      "Epoch: 580,train_loss:0.0289082, Train: 1.0000, Val: 0.7660, Test: 0.7660\n",
      "Epoch: 581,train_loss:0.0348609, Train: 1.0000, Val: 0.7460, Test: 0.7510\n",
      "Epoch: 582,train_loss:0.0497811, Train: 1.0000, Val: 0.7420, Test: 0.7570\n",
      "Epoch: 583,train_loss:0.0293093, Train: 1.0000, Val: 0.7220, Test: 0.7400\n",
      "Epoch: 584,train_loss:0.0325092, Train: 1.0000, Val: 0.7080, Test: 0.7120\n",
      "Epoch: 585,train_loss:0.0606482, Train: 1.0000, Val: 0.7200, Test: 0.7260\n",
      "Epoch: 586,train_loss:0.0389806, Train: 1.0000, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 587,train_loss:0.0248256, Train: 1.0000, Val: 0.7500, Test: 0.7640\n",
      "Epoch: 588,train_loss:0.0236912, Train: 1.0000, Val: 0.7520, Test: 0.7680\n",
      "Epoch: 589,train_loss:0.0320958, Train: 1.0000, Val: 0.7520, Test: 0.7700\n",
      "Epoch: 590,train_loss:0.0347548, Train: 1.0000, Val: 0.7520, Test: 0.7520\n",
      "Epoch: 591,train_loss:0.0220197, Train: 1.0000, Val: 0.7320, Test: 0.7510\n",
      "Epoch: 592,train_loss:0.0222991, Train: 1.0000, Val: 0.7240, Test: 0.7370\n",
      "Epoch: 593,train_loss:0.0266958, Train: 1.0000, Val: 0.7260, Test: 0.7400\n",
      "Epoch: 594,train_loss:0.0192691, Train: 1.0000, Val: 0.7240, Test: 0.7330\n",
      "Epoch: 595,train_loss:0.0288321, Train: 1.0000, Val: 0.7220, Test: 0.7480\n",
      "Epoch: 596,train_loss:0.0299573, Train: 1.0000, Val: 0.7240, Test: 0.7590\n",
      "Epoch: 597,train_loss:0.0312537, Train: 1.0000, Val: 0.7560, Test: 0.7690\n",
      "Epoch: 598,train_loss:0.0266453, Train: 1.0000, Val: 0.7420, Test: 0.7580\n",
      "Epoch: 599,train_loss:0.0277752, Train: 1.0000, Val: 0.7320, Test: 0.7560\n",
      "Epoch: 600,train_loss:0.0433276, Train: 1.0000, Val: 0.7540, Test: 0.7550\n",
      "Epoch: 601,train_loss:0.0339361, Train: 1.0000, Val: 0.7480, Test: 0.7610\n",
      "Epoch: 602,train_loss:0.0549437, Train: 1.0000, Val: 0.7240, Test: 0.7490\n",
      "Epoch: 603,train_loss:0.0172059, Train: 1.0000, Val: 0.7260, Test: 0.7460\n",
      "Epoch: 604,train_loss:0.0470225, Train: 1.0000, Val: 0.7160, Test: 0.7200\n",
      "Epoch: 605,train_loss:0.0329559, Train: 1.0000, Val: 0.7280, Test: 0.7400\n",
      "Epoch: 606,train_loss:0.0201867, Train: 1.0000, Val: 0.7260, Test: 0.7450\n",
      "Epoch: 607,train_loss:0.0257299, Train: 1.0000, Val: 0.7220, Test: 0.7410\n",
      "Epoch: 608,train_loss:0.0361677, Train: 1.0000, Val: 0.7340, Test: 0.7580\n",
      "Epoch: 609,train_loss:0.0361003, Train: 1.0000, Val: 0.7480, Test: 0.7730\n",
      "Epoch: 610,train_loss:0.0217368, Train: 1.0000, Val: 0.7520, Test: 0.7850\n",
      "Epoch: 611,train_loss:0.0399605, Train: 1.0000, Val: 0.7420, Test: 0.7730\n",
      "Epoch: 612,train_loss:0.0157876, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 613,train_loss:0.0188083, Train: 1.0000, Val: 0.7260, Test: 0.7590\n",
      "Epoch: 614,train_loss:0.0168609, Train: 1.0000, Val: 0.7220, Test: 0.7430\n",
      "Epoch: 615,train_loss:0.0141931, Train: 1.0000, Val: 0.7260, Test: 0.7360\n",
      "Epoch: 616,train_loss:0.0328469, Train: 1.0000, Val: 0.7260, Test: 0.7400\n",
      "Epoch: 617,train_loss:0.0381577, Train: 1.0000, Val: 0.7420, Test: 0.7450\n",
      "Epoch: 618,train_loss:0.0216021, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 619,train_loss:0.0442276, Train: 1.0000, Val: 0.7460, Test: 0.7720\n",
      "Epoch: 620,train_loss:0.0315602, Train: 1.0000, Val: 0.7380, Test: 0.7750\n",
      "Epoch: 621,train_loss:0.0280242, Train: 1.0000, Val: 0.7460, Test: 0.7770\n",
      "Epoch: 622,train_loss:0.0236354, Train: 1.0000, Val: 0.7480, Test: 0.7650\n",
      "Epoch: 623,train_loss:0.0233410, Train: 1.0000, Val: 0.7360, Test: 0.7590\n",
      "Epoch: 624,train_loss:0.0234662, Train: 1.0000, Val: 0.7260, Test: 0.7370\n",
      "Epoch: 625,train_loss:0.0232945, Train: 1.0000, Val: 0.7220, Test: 0.7400\n",
      "Epoch: 626,train_loss:0.0289322, Train: 1.0000, Val: 0.7300, Test: 0.7510\n",
      "Epoch: 627,train_loss:0.0243759, Train: 1.0000, Val: 0.7300, Test: 0.7540\n",
      "Epoch: 628,train_loss:0.0282934, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 629,train_loss:0.0537678, Train: 1.0000, Val: 0.7420, Test: 0.7660\n",
      "Epoch: 630,train_loss:0.0290205, Train: 1.0000, Val: 0.7420, Test: 0.7600\n",
      "Epoch: 631,train_loss:0.0186302, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch: 632,train_loss:0.0403558, Train: 1.0000, Val: 0.7260, Test: 0.7520\n",
      "Epoch: 633,train_loss:0.0156581, Train: 1.0000, Val: 0.7200, Test: 0.7370\n",
      "Epoch: 634,train_loss:0.0218269, Train: 1.0000, Val: 0.7260, Test: 0.7340\n",
      "Epoch: 635,train_loss:0.0365160, Train: 1.0000, Val: 0.7320, Test: 0.7470\n",
      "Epoch: 636,train_loss:0.0341118, Train: 1.0000, Val: 0.7400, Test: 0.7590\n",
      "Epoch: 637,train_loss:0.0268305, Train: 1.0000, Val: 0.7540, Test: 0.7630\n",
      "Epoch: 638,train_loss:0.0275913, Train: 1.0000, Val: 0.7460, Test: 0.7630\n",
      "Epoch: 639,train_loss:0.0209732, Train: 1.0000, Val: 0.7340, Test: 0.7490\n",
      "Epoch: 640,train_loss:0.0287806, Train: 1.0000, Val: 0.7300, Test: 0.7460\n",
      "Epoch: 641,train_loss:0.0433054, Train: 1.0000, Val: 0.7320, Test: 0.7600\n",
      "Epoch: 642,train_loss:0.0325715, Train: 1.0000, Val: 0.7160, Test: 0.7480\n",
      "Epoch: 643,train_loss:0.0626290, Train: 1.0000, Val: 0.7120, Test: 0.7370\n",
      "Epoch: 644,train_loss:0.0295384, Train: 1.0000, Val: 0.7140, Test: 0.7450\n",
      "Epoch: 645,train_loss:0.0267671, Train: 1.0000, Val: 0.7320, Test: 0.7610\n",
      "Epoch: 646,train_loss:0.0366358, Train: 1.0000, Val: 0.7380, Test: 0.7520\n",
      "Epoch: 647,train_loss:0.0271145, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 648,train_loss:0.0354456, Train: 1.0000, Val: 0.7420, Test: 0.7680\n",
      "Epoch: 649,train_loss:0.0237993, Train: 1.0000, Val: 0.7380, Test: 0.7730\n",
      "Epoch: 650,train_loss:0.0373893, Train: 1.0000, Val: 0.7340, Test: 0.7580\n",
      "Epoch: 651,train_loss:0.0241697, Train: 1.0000, Val: 0.7100, Test: 0.7380\n",
      "Epoch: 652,train_loss:0.0388288, Train: 1.0000, Val: 0.7200, Test: 0.7520\n",
      "Epoch: 653,train_loss:0.0364151, Train: 1.0000, Val: 0.7400, Test: 0.7530\n",
      "Epoch: 654,train_loss:0.0213885, Train: 1.0000, Val: 0.7440, Test: 0.7510\n",
      "Epoch: 655,train_loss:0.0407928, Train: 1.0000, Val: 0.7360, Test: 0.7520\n",
      "Epoch: 656,train_loss:0.0169861, Train: 1.0000, Val: 0.7440, Test: 0.7570\n",
      "Epoch: 657,train_loss:0.0360834, Train: 1.0000, Val: 0.7480, Test: 0.7620\n",
      "Epoch: 658,train_loss:0.0352058, Train: 1.0000, Val: 0.7320, Test: 0.7550\n",
      "Epoch: 659,train_loss:0.0304715, Train: 1.0000, Val: 0.7280, Test: 0.7410\n",
      "Epoch: 660,train_loss:0.0468683, Train: 1.0000, Val: 0.7320, Test: 0.7370\n",
      "Epoch: 661,train_loss:0.0587617, Train: 1.0000, Val: 0.7060, Test: 0.7220\n",
      "Epoch: 662,train_loss:0.0275569, Train: 1.0000, Val: 0.7000, Test: 0.7110\n",
      "Epoch: 663,train_loss:0.0237273, Train: 1.0000, Val: 0.6980, Test: 0.7060\n",
      "Epoch: 664,train_loss:0.0438456, Train: 1.0000, Val: 0.7180, Test: 0.7320\n",
      "Epoch: 665,train_loss:0.0321805, Train: 1.0000, Val: 0.7320, Test: 0.7480\n",
      "Epoch: 666,train_loss:0.0321336, Train: 1.0000, Val: 0.7460, Test: 0.7550\n",
      "Epoch: 667,train_loss:0.0234033, Train: 1.0000, Val: 0.7420, Test: 0.7520\n",
      "Epoch: 668,train_loss:0.0625198, Train: 1.0000, Val: 0.7400, Test: 0.7530\n",
      "Epoch: 669,train_loss:0.0184122, Train: 1.0000, Val: 0.7120, Test: 0.7400\n",
      "Epoch: 670,train_loss:0.0195307, Train: 1.0000, Val: 0.7000, Test: 0.7210\n",
      "Epoch: 671,train_loss:0.0479254, Train: 1.0000, Val: 0.7120, Test: 0.7350\n",
      "Epoch: 672,train_loss:0.0289925, Train: 1.0000, Val: 0.7300, Test: 0.7440\n",
      "Epoch: 673,train_loss:0.0615554, Train: 1.0000, Val: 0.7420, Test: 0.7580\n",
      "Epoch: 674,train_loss:0.0278093, Train: 1.0000, Val: 0.7360, Test: 0.7570\n",
      "Epoch: 675,train_loss:0.0349691, Train: 1.0000, Val: 0.7600, Test: 0.7710\n",
      "Epoch: 676,train_loss:0.0271152, Train: 1.0000, Val: 0.7540, Test: 0.7720\n",
      "Epoch: 677,train_loss:0.0472201, Train: 1.0000, Val: 0.7420, Test: 0.7410\n",
      "Epoch: 678,train_loss:0.0323595, Train: 1.0000, Val: 0.7200, Test: 0.7240\n",
      "Epoch: 679,train_loss:0.0361872, Train: 1.0000, Val: 0.7060, Test: 0.7210\n",
      "Epoch: 680,train_loss:0.0420698, Train: 1.0000, Val: 0.7160, Test: 0.7410\n",
      "Epoch: 681,train_loss:0.0185892, Train: 1.0000, Val: 0.7380, Test: 0.7630\n",
      "Epoch: 682,train_loss:0.0214594, Train: 1.0000, Val: 0.7460, Test: 0.7680\n",
      "Epoch: 683,train_loss:0.0289062, Train: 1.0000, Val: 0.7500, Test: 0.7630\n",
      "Epoch: 684,train_loss:0.0212753, Train: 1.0000, Val: 0.7500, Test: 0.7650\n",
      "Epoch: 685,train_loss:0.0337257, Train: 1.0000, Val: 0.7480, Test: 0.7710\n",
      "Epoch: 686,train_loss:0.0248993, Train: 1.0000, Val: 0.7340, Test: 0.7580\n",
      "Epoch: 687,train_loss:0.0322824, Train: 1.0000, Val: 0.7320, Test: 0.7440\n",
      "Epoch: 688,train_loss:0.0364036, Train: 1.0000, Val: 0.7460, Test: 0.7420\n",
      "Epoch: 689,train_loss:0.0549009, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 690,train_loss:0.0236862, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
      "Epoch: 691,train_loss:0.0547228, Train: 1.0000, Val: 0.7220, Test: 0.7270\n",
      "Epoch: 692,train_loss:0.0268006, Train: 1.0000, Val: 0.7440, Test: 0.7570\n",
      "Epoch: 693,train_loss:0.0263302, Train: 1.0000, Val: 0.7560, Test: 0.7620\n",
      "Epoch: 694,train_loss:0.0484780, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch: 695,train_loss:0.0173564, Train: 1.0000, Val: 0.7460, Test: 0.7440\n",
      "Epoch: 696,train_loss:0.0142360, Train: 1.0000, Val: 0.7300, Test: 0.7300\n",
      "Epoch: 697,train_loss:0.0179148, Train: 1.0000, Val: 0.7300, Test: 0.7310\n",
      "Epoch: 698,train_loss:0.0199599, Train: 1.0000, Val: 0.7300, Test: 0.7320\n",
      "Epoch: 699,train_loss:0.0173372, Train: 1.0000, Val: 0.7280, Test: 0.7370\n",
      "Epoch: 700,train_loss:0.0413868, Train: 1.0000, Val: 0.7280, Test: 0.7350\n",
      "Epoch: 701,train_loss:0.0267473, Train: 1.0000, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 702,train_loss:0.0241237, Train: 1.0000, Val: 0.7520, Test: 0.7510\n",
      "Epoch: 703,train_loss:0.0289267, Train: 1.0000, Val: 0.7560, Test: 0.7650\n",
      "Epoch: 704,train_loss:0.0237508, Train: 1.0000, Val: 0.7560, Test: 0.7720\n",
      "Epoch: 705,train_loss:0.0232134, Train: 1.0000, Val: 0.7540, Test: 0.7700\n",
      "Epoch: 706,train_loss:0.0237631, Train: 1.0000, Val: 0.7500, Test: 0.7550\n",
      "Epoch: 707,train_loss:0.0173482, Train: 1.0000, Val: 0.7440, Test: 0.7550\n",
      "Epoch: 708,train_loss:0.0193189, Train: 1.0000, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 709,train_loss:0.0290541, Train: 1.0000, Val: 0.7400, Test: 0.7570\n",
      "Epoch: 710,train_loss:0.0322969, Train: 1.0000, Val: 0.7480, Test: 0.7470\n",
      "Epoch: 711,train_loss:0.0227840, Train: 1.0000, Val: 0.7360, Test: 0.7480\n",
      "Epoch: 712,train_loss:0.0345761, Train: 1.0000, Val: 0.7420, Test: 0.7520\n",
      "Epoch: 713,train_loss:0.0183282, Train: 1.0000, Val: 0.7480, Test: 0.7620\n",
      "Epoch: 714,train_loss:0.0284856, Train: 1.0000, Val: 0.7500, Test: 0.7720\n",
      "Epoch: 715,train_loss:0.0162008, Train: 1.0000, Val: 0.7380, Test: 0.7550\n",
      "Epoch: 716,train_loss:0.0254342, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 717,train_loss:0.0464839, Train: 1.0000, Val: 0.7220, Test: 0.7440\n",
      "Epoch: 718,train_loss:0.0266756, Train: 1.0000, Val: 0.7080, Test: 0.7060\n",
      "Epoch: 719,train_loss:0.0556333, Train: 1.0000, Val: 0.6940, Test: 0.6950\n",
      "Epoch: 720,train_loss:0.0343997, Train: 1.0000, Val: 0.7080, Test: 0.7120\n",
      "Epoch: 721,train_loss:0.0381895, Train: 1.0000, Val: 0.7280, Test: 0.7300\n",
      "Epoch: 722,train_loss:0.0344966, Train: 1.0000, Val: 0.7260, Test: 0.7410\n",
      "Epoch: 723,train_loss:0.0263292, Train: 1.0000, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 724,train_loss:0.0288479, Train: 1.0000, Val: 0.7380, Test: 0.7580\n",
      "Epoch: 725,train_loss:0.0433842, Train: 1.0000, Val: 0.7440, Test: 0.7710\n",
      "Epoch: 726,train_loss:0.0299824, Train: 1.0000, Val: 0.7420, Test: 0.7520\n",
      "Epoch: 727,train_loss:0.0251988, Train: 1.0000, Val: 0.7280, Test: 0.7420\n",
      "Epoch: 728,train_loss:0.0250481, Train: 1.0000, Val: 0.7220, Test: 0.7350\n",
      "Epoch: 729,train_loss:0.0215931, Train: 1.0000, Val: 0.7200, Test: 0.7320\n",
      "Epoch: 730,train_loss:0.0221732, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch: 731,train_loss:0.0153941, Train: 1.0000, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 732,train_loss:0.0285021, Train: 1.0000, Val: 0.7320, Test: 0.7440\n",
      "Epoch: 733,train_loss:0.0194264, Train: 1.0000, Val: 0.7280, Test: 0.7380\n",
      "Epoch: 734,train_loss:0.0278932, Train: 1.0000, Val: 0.7400, Test: 0.7470\n",
      "Epoch: 735,train_loss:0.0172235, Train: 1.0000, Val: 0.7440, Test: 0.7510\n",
      "Epoch: 736,train_loss:0.0275700, Train: 1.0000, Val: 0.7280, Test: 0.7520\n",
      "Epoch: 737,train_loss:0.0196166, Train: 1.0000, Val: 0.7300, Test: 0.7460\n",
      "Epoch: 738,train_loss:0.0122256, Train: 1.0000, Val: 0.7180, Test: 0.7450\n",
      "Epoch: 739,train_loss:0.0168741, Train: 1.0000, Val: 0.7300, Test: 0.7530\n",
      "Epoch: 740,train_loss:0.0370135, Train: 1.0000, Val: 0.7400, Test: 0.7530\n",
      "Epoch: 741,train_loss:0.0383553, Train: 1.0000, Val: 0.7440, Test: 0.7610\n",
      "Epoch: 742,train_loss:0.0182469, Train: 1.0000, Val: 0.7460, Test: 0.7550\n",
      "Epoch: 743,train_loss:0.0273214, Train: 1.0000, Val: 0.7320, Test: 0.7490\n",
      "Epoch: 744,train_loss:0.0500079, Train: 1.0000, Val: 0.7160, Test: 0.7310\n",
      "Epoch: 745,train_loss:0.0328151, Train: 1.0000, Val: 0.7080, Test: 0.7190\n",
      "Epoch: 746,train_loss:0.0271350, Train: 1.0000, Val: 0.7140, Test: 0.7290\n",
      "Epoch: 747,train_loss:0.0463404, Train: 1.0000, Val: 0.7340, Test: 0.7500\n",
      "Epoch: 748,train_loss:0.0401550, Train: 1.0000, Val: 0.7360, Test: 0.7730\n",
      "Epoch: 749,train_loss:0.0261350, Train: 1.0000, Val: 0.7400, Test: 0.7720\n",
      "Epoch: 750,train_loss:0.0314421, Train: 1.0000, Val: 0.7360, Test: 0.7610\n",
      "Epoch: 751,train_loss:0.0298093, Train: 1.0000, Val: 0.7460, Test: 0.7690\n",
      "Epoch: 752,train_loss:0.0331215, Train: 1.0000, Val: 0.7280, Test: 0.7550\n",
      "Epoch: 753,train_loss:0.0388222, Train: 1.0000, Val: 0.7320, Test: 0.7580\n",
      "Epoch: 754,train_loss:0.0225655, Train: 1.0000, Val: 0.7320, Test: 0.7470\n",
      "Epoch: 755,train_loss:0.0312293, Train: 1.0000, Val: 0.7340, Test: 0.7480\n",
      "Epoch: 756,train_loss:0.0223918, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 757,train_loss:0.0466014, Train: 1.0000, Val: 0.7180, Test: 0.7600\n",
      "Epoch: 758,train_loss:0.0486803, Train: 1.0000, Val: 0.7400, Test: 0.7690\n",
      "Epoch: 759,train_loss:0.0184454, Train: 1.0000, Val: 0.7500, Test: 0.7800\n",
      "Epoch: 760,train_loss:0.0311726, Train: 1.0000, Val: 0.7340, Test: 0.7730\n",
      "Epoch: 761,train_loss:0.0119015, Train: 1.0000, Val: 0.7320, Test: 0.7610\n",
      "Epoch: 762,train_loss:0.0242152, Train: 1.0000, Val: 0.7360, Test: 0.7600\n",
      "Epoch: 763,train_loss:0.0311486, Train: 1.0000, Val: 0.7360, Test: 0.7410\n",
      "Epoch: 764,train_loss:0.0226534, Train: 1.0000, Val: 0.7200, Test: 0.7320\n",
      "Epoch: 765,train_loss:0.0292740, Train: 1.0000, Val: 0.7240, Test: 0.7330\n",
      "Epoch: 766,train_loss:0.0473731, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch: 767,train_loss:0.0183161, Train: 1.0000, Val: 0.7460, Test: 0.7650\n",
      "Epoch: 768,train_loss:0.0202361, Train: 1.0000, Val: 0.7400, Test: 0.7730\n",
      "Epoch: 769,train_loss:0.0168693, Train: 1.0000, Val: 0.7340, Test: 0.7700\n",
      "Epoch: 770,train_loss:0.0354482, Train: 1.0000, Val: 0.7360, Test: 0.7720\n",
      "Epoch: 771,train_loss:0.0273347, Train: 1.0000, Val: 0.7460, Test: 0.7750\n",
      "Epoch: 772,train_loss:0.0409272, Train: 1.0000, Val: 0.7380, Test: 0.7620\n",
      "Epoch: 773,train_loss:0.0303423, Train: 1.0000, Val: 0.7340, Test: 0.7520\n",
      "Epoch: 774,train_loss:0.0333781, Train: 1.0000, Val: 0.7260, Test: 0.7440\n",
      "Epoch: 775,train_loss:0.0192971, Train: 1.0000, Val: 0.7320, Test: 0.7470\n",
      "Epoch: 776,train_loss:0.0314813, Train: 1.0000, Val: 0.7360, Test: 0.7470\n",
      "Epoch: 777,train_loss:0.0203896, Train: 1.0000, Val: 0.7280, Test: 0.7450\n",
      "Epoch: 778,train_loss:0.0362057, Train: 1.0000, Val: 0.7260, Test: 0.7280\n",
      "Epoch: 779,train_loss:0.0541976, Train: 1.0000, Val: 0.7360, Test: 0.7500\n",
      "Epoch: 780,train_loss:0.0214764, Train: 1.0000, Val: 0.7060, Test: 0.7220\n",
      "Epoch: 781,train_loss:0.0392502, Train: 1.0000, Val: 0.7080, Test: 0.7250\n",
      "Epoch: 782,train_loss:0.0873677, Train: 1.0000, Val: 0.7440, Test: 0.7720\n",
      "Epoch: 783,train_loss:0.0280173, Train: 1.0000, Val: 0.6880, Test: 0.6820\n",
      "Epoch: 784,train_loss:0.0661460, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch: 785,train_loss:0.0412434, Train: 1.0000, Val: 0.7400, Test: 0.7530\n",
      "Epoch: 786,train_loss:0.0249657, Train: 1.0000, Val: 0.7020, Test: 0.7120\n",
      "Epoch: 787,train_loss:0.0646280, Train: 1.0000, Val: 0.7060, Test: 0.7170\n",
      "Epoch: 788,train_loss:0.0694540, Train: 1.0000, Val: 0.7400, Test: 0.7470\n",
      "Epoch: 789,train_loss:0.0320395, Train: 1.0000, Val: 0.7320, Test: 0.7320\n",
      "Epoch: 790,train_loss:0.0387825, Train: 1.0000, Val: 0.7080, Test: 0.6940\n",
      "Epoch: 791,train_loss:0.0652504, Train: 1.0000, Val: 0.6780, Test: 0.6840\n",
      "Epoch: 792,train_loss:0.0756037, Train: 1.0000, Val: 0.7260, Test: 0.7520\n",
      "Epoch: 793,train_loss:0.0156301, Train: 1.0000, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 794,train_loss:0.0295507, Train: 0.9929, Val: 0.7240, Test: 0.7170\n",
      "Epoch: 795,train_loss:0.0948259, Train: 1.0000, Val: 0.7160, Test: 0.7250\n",
      "Epoch: 796,train_loss:0.0464509, Train: 1.0000, Val: 0.7180, Test: 0.7390\n",
      "Epoch: 797,train_loss:0.0336005, Train: 1.0000, Val: 0.7100, Test: 0.7220\n",
      "Epoch: 798,train_loss:0.0223153, Train: 1.0000, Val: 0.6920, Test: 0.6940\n",
      "Epoch: 799,train_loss:0.0436724, Train: 1.0000, Val: 0.6880, Test: 0.6850\n",
      "Epoch: 800,train_loss:0.0722790, Train: 1.0000, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 801,train_loss:0.0343919, Train: 1.0000, Val: 0.7360, Test: 0.7400\n",
      "Epoch: 802,train_loss:0.0388177, Train: 1.0000, Val: 0.7360, Test: 0.7420\n",
      "Epoch: 803,train_loss:0.0204676, Train: 1.0000, Val: 0.7060, Test: 0.7150\n",
      "Epoch: 804,train_loss:0.0594818, Train: 1.0000, Val: 0.7160, Test: 0.7180\n",
      "Epoch: 805,train_loss:0.0808343, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
      "Epoch: 806,train_loss:0.0176298, Train: 1.0000, Val: 0.7240, Test: 0.7290\n",
      "Epoch: 807,train_loss:0.0363009, Train: 1.0000, Val: 0.6820, Test: 0.6900\n",
      "Epoch: 808,train_loss:0.0734782, Train: 1.0000, Val: 0.7180, Test: 0.7240\n",
      "Epoch: 809,train_loss:0.0556250, Train: 1.0000, Val: 0.7260, Test: 0.7500\n",
      "Epoch: 810,train_loss:0.0346608, Train: 1.0000, Val: 0.7140, Test: 0.7200\n",
      "Epoch: 811,train_loss:0.0310167, Train: 1.0000, Val: 0.7020, Test: 0.7140\n",
      "Epoch: 812,train_loss:0.0413395, Train: 1.0000, Val: 0.7080, Test: 0.7350\n",
      "Epoch: 813,train_loss:0.0428247, Train: 1.0000, Val: 0.7340, Test: 0.7680\n",
      "Epoch: 814,train_loss:0.0219379, Train: 1.0000, Val: 0.7440, Test: 0.7820\n",
      "Epoch: 815,train_loss:0.0280332, Train: 1.0000, Val: 0.7480, Test: 0.7760\n",
      "Epoch: 816,train_loss:0.0236777, Train: 1.0000, Val: 0.7420, Test: 0.7660\n",
      "Epoch: 817,train_loss:0.0148079, Train: 1.0000, Val: 0.7240, Test: 0.7530\n",
      "Epoch: 818,train_loss:0.0211493, Train: 1.0000, Val: 0.7200, Test: 0.7420\n",
      "Epoch: 819,train_loss:0.0216596, Train: 1.0000, Val: 0.7200, Test: 0.7340\n",
      "Epoch: 820,train_loss:0.0295799, Train: 1.0000, Val: 0.7100, Test: 0.7420\n",
      "Epoch: 821,train_loss:0.0293053, Train: 1.0000, Val: 0.7260, Test: 0.7480\n",
      "Epoch: 822,train_loss:0.0194317, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 823,train_loss:0.0124612, Train: 1.0000, Val: 0.7360, Test: 0.7550\n",
      "Epoch: 824,train_loss:0.0312726, Train: 1.0000, Val: 0.7480, Test: 0.7700\n",
      "Epoch: 825,train_loss:0.0377766, Train: 1.0000, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 826,train_loss:0.0229690, Train: 1.0000, Val: 0.7540, Test: 0.7750\n",
      "Epoch: 827,train_loss:0.0315905, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 828,train_loss:0.0376714, Train: 1.0000, Val: 0.7260, Test: 0.7470\n",
      "Epoch: 829,train_loss:0.0556335, Train: 1.0000, Val: 0.7200, Test: 0.7400\n",
      "Epoch: 830,train_loss:0.0274615, Train: 1.0000, Val: 0.6960, Test: 0.7240\n",
      "Epoch: 831,train_loss:0.0572538, Train: 1.0000, Val: 0.7100, Test: 0.7340\n",
      "Epoch: 832,train_loss:0.0272375, Train: 1.0000, Val: 0.7440, Test: 0.7530\n",
      "Epoch: 833,train_loss:0.0189588, Train: 1.0000, Val: 0.7560, Test: 0.7660\n",
      "Epoch: 834,train_loss:0.0371478, Train: 1.0000, Val: 0.7580, Test: 0.7810\n",
      "Epoch: 835,train_loss:0.0531338, Train: 1.0000, Val: 0.7580, Test: 0.7800\n",
      "Epoch: 836,train_loss:0.0293613, Train: 1.0000, Val: 0.7500, Test: 0.7680\n",
      "Epoch: 837,train_loss:0.0110756, Train: 1.0000, Val: 0.7300, Test: 0.7530\n",
      "Epoch: 838,train_loss:0.0176826, Train: 1.0000, Val: 0.7160, Test: 0.7390\n",
      "Epoch: 839,train_loss:0.0692251, Train: 1.0000, Val: 0.7280, Test: 0.7560\n",
      "Epoch: 840,train_loss:0.0408284, Train: 1.0000, Val: 0.7460, Test: 0.7760\n",
      "Epoch: 841,train_loss:0.0637386, Train: 1.0000, Val: 0.7500, Test: 0.7770\n",
      "Epoch: 842,train_loss:0.0292739, Train: 1.0000, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 843,train_loss:0.0470515, Train: 1.0000, Val: 0.7420, Test: 0.7660\n",
      "Epoch: 844,train_loss:0.0897949, Train: 1.0000, Val: 0.7420, Test: 0.7590\n",
      "Epoch: 845,train_loss:0.0294647, Train: 1.0000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 846,train_loss:0.0311734, Train: 1.0000, Val: 0.6660, Test: 0.6760\n",
      "Epoch: 847,train_loss:0.0863763, Train: 1.0000, Val: 0.6800, Test: 0.6870\n",
      "Epoch: 848,train_loss:0.0693845, Train: 1.0000, Val: 0.7160, Test: 0.7250\n",
      "Epoch: 849,train_loss:0.0156442, Train: 1.0000, Val: 0.6860, Test: 0.6940\n",
      "Epoch: 850,train_loss:0.0487177, Train: 1.0000, Val: 0.6800, Test: 0.6850\n",
      "Epoch: 851,train_loss:0.0833306, Train: 1.0000, Val: 0.7120, Test: 0.7400\n",
      "Epoch: 852,train_loss:0.0458565, Train: 1.0000, Val: 0.7440, Test: 0.7500\n",
      "Epoch: 853,train_loss:0.0412907, Train: 1.0000, Val: 0.7260, Test: 0.7360\n",
      "Epoch: 854,train_loss:0.0291719, Train: 1.0000, Val: 0.7120, Test: 0.7180\n",
      "Epoch: 855,train_loss:0.0631941, Train: 1.0000, Val: 0.7340, Test: 0.7430\n",
      "Epoch: 856,train_loss:0.0200911, Train: 1.0000, Val: 0.7180, Test: 0.7290\n",
      "Epoch: 857,train_loss:0.0101791, Train: 1.0000, Val: 0.6920, Test: 0.7150\n",
      "Epoch: 858,train_loss:0.0185412, Train: 1.0000, Val: 0.6760, Test: 0.7050\n",
      "Epoch: 859,train_loss:0.0656955, Train: 1.0000, Val: 0.7080, Test: 0.7290\n",
      "Epoch: 860,train_loss:0.0106829, Train: 1.0000, Val: 0.7360, Test: 0.7470\n",
      "Epoch: 861,train_loss:0.0195205, Train: 1.0000, Val: 0.7520, Test: 0.7600\n",
      "Epoch: 862,train_loss:0.0185206, Train: 1.0000, Val: 0.7440, Test: 0.7780\n",
      "Epoch: 863,train_loss:0.0247675, Train: 1.0000, Val: 0.7420, Test: 0.7760\n",
      "Epoch: 864,train_loss:0.0329492, Train: 1.0000, Val: 0.7440, Test: 0.7560\n",
      "Epoch: 865,train_loss:0.0530697, Train: 1.0000, Val: 0.7220, Test: 0.7360\n",
      "Epoch: 866,train_loss:0.0314347, Train: 1.0000, Val: 0.6920, Test: 0.7180\n",
      "Epoch: 867,train_loss:0.0207604, Train: 1.0000, Val: 0.6800, Test: 0.7040\n",
      "Epoch: 868,train_loss:0.0819853, Train: 1.0000, Val: 0.7080, Test: 0.7320\n",
      "Epoch: 869,train_loss:0.0222049, Train: 1.0000, Val: 0.7280, Test: 0.7230\n",
      "Epoch: 870,train_loss:0.0487162, Train: 1.0000, Val: 0.7160, Test: 0.7300\n",
      "Epoch: 871,train_loss:0.0364274, Train: 1.0000, Val: 0.7320, Test: 0.7500\n",
      "Epoch: 872,train_loss:0.0216301, Train: 1.0000, Val: 0.7500, Test: 0.7650\n",
      "Epoch: 873,train_loss:0.0145236, Train: 1.0000, Val: 0.7400, Test: 0.7660\n",
      "Epoch: 874,train_loss:0.0225017, Train: 1.0000, Val: 0.7480, Test: 0.7650\n",
      "Epoch: 875,train_loss:0.0299873, Train: 1.0000, Val: 0.7400, Test: 0.7500\n",
      "Epoch: 876,train_loss:0.0362897, Train: 1.0000, Val: 0.7220, Test: 0.7280\n",
      "Epoch: 877,train_loss:0.0574217, Train: 1.0000, Val: 0.6980, Test: 0.7190\n",
      "Epoch: 878,train_loss:0.0244962, Train: 1.0000, Val: 0.6980, Test: 0.7070\n",
      "Epoch: 879,train_loss:0.0314913, Train: 1.0000, Val: 0.6880, Test: 0.6630\n",
      "Epoch: 880,train_loss:0.0512819, Train: 1.0000, Val: 0.7180, Test: 0.7020\n",
      "Epoch: 881,train_loss:0.0621962, Train: 1.0000, Val: 0.7300, Test: 0.7510\n",
      "Epoch: 882,train_loss:0.0570614, Train: 1.0000, Val: 0.7420, Test: 0.7650\n",
      "Epoch: 883,train_loss:0.0576822, Train: 1.0000, Val: 0.7460, Test: 0.7720\n",
      "Epoch: 884,train_loss:0.0632586, Train: 1.0000, Val: 0.7220, Test: 0.7640\n",
      "Epoch: 885,train_loss:0.0333776, Train: 1.0000, Val: 0.7040, Test: 0.7390\n",
      "Epoch: 886,train_loss:0.0871239, Train: 1.0000, Val: 0.7200, Test: 0.7670\n",
      "Epoch: 887,train_loss:0.0368040, Train: 1.0000, Val: 0.7500, Test: 0.7720\n",
      "Epoch: 888,train_loss:0.0151009, Train: 1.0000, Val: 0.7360, Test: 0.7450\n",
      "Epoch: 889,train_loss:0.0307910, Train: 1.0000, Val: 0.7080, Test: 0.7140\n",
      "Epoch: 890,train_loss:0.0478142, Train: 1.0000, Val: 0.7140, Test: 0.7370\n",
      "Epoch: 891,train_loss:0.0316936, Train: 1.0000, Val: 0.7460, Test: 0.7650\n",
      "Epoch: 892,train_loss:0.0164044, Train: 1.0000, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 893,train_loss:0.0227350, Train: 1.0000, Val: 0.7180, Test: 0.7520\n",
      "Epoch: 894,train_loss:0.0278356, Train: 1.0000, Val: 0.7120, Test: 0.7510\n",
      "Epoch: 895,train_loss:0.0617889, Train: 1.0000, Val: 0.7420, Test: 0.7670\n",
      "Epoch: 896,train_loss:0.0349122, Train: 1.0000, Val: 0.7480, Test: 0.7820\n",
      "Epoch: 897,train_loss:0.0262154, Train: 1.0000, Val: 0.7280, Test: 0.7490\n",
      "Epoch: 898,train_loss:0.0359834, Train: 1.0000, Val: 0.7140, Test: 0.7300\n",
      "Epoch: 899,train_loss:0.0367087, Train: 1.0000, Val: 0.7280, Test: 0.7430\n",
      "Epoch: 900,train_loss:0.0262251, Train: 1.0000, Val: 0.7360, Test: 0.7530\n",
      "Epoch: 901,train_loss:0.0243479, Train: 1.0000, Val: 0.7220, Test: 0.7560\n",
      "Epoch: 902,train_loss:0.0195405, Train: 1.0000, Val: 0.7240, Test: 0.7340\n",
      "Epoch: 903,train_loss:0.0221023, Train: 1.0000, Val: 0.7080, Test: 0.7310\n",
      "Epoch: 904,train_loss:0.0341601, Train: 1.0000, Val: 0.7040, Test: 0.7480\n",
      "Epoch: 905,train_loss:0.0446554, Train: 1.0000, Val: 0.7240, Test: 0.7560\n",
      "Epoch: 906,train_loss:0.0471895, Train: 1.0000, Val: 0.7440, Test: 0.7630\n",
      "Epoch: 907,train_loss:0.0168699, Train: 1.0000, Val: 0.7320, Test: 0.7520\n",
      "Epoch: 908,train_loss:0.0456337, Train: 1.0000, Val: 0.7240, Test: 0.7360\n",
      "Epoch: 909,train_loss:0.0189498, Train: 1.0000, Val: 0.7200, Test: 0.7380\n",
      "Epoch: 910,train_loss:0.0311774, Train: 1.0000, Val: 0.7320, Test: 0.7500\n",
      "Epoch: 911,train_loss:0.0218762, Train: 1.0000, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 912,train_loss:0.0174787, Train: 1.0000, Val: 0.7360, Test: 0.7530\n",
      "Epoch: 913,train_loss:0.0379740, Train: 1.0000, Val: 0.7240, Test: 0.7550\n",
      "Epoch: 914,train_loss:0.0203140, Train: 1.0000, Val: 0.7160, Test: 0.7590\n",
      "Epoch: 915,train_loss:0.0584384, Train: 1.0000, Val: 0.7240, Test: 0.7590\n",
      "Epoch: 916,train_loss:0.0285019, Train: 1.0000, Val: 0.7280, Test: 0.7580\n",
      "Epoch: 917,train_loss:0.0179665, Train: 1.0000, Val: 0.7400, Test: 0.7620\n",
      "Epoch: 918,train_loss:0.0307752, Train: 1.0000, Val: 0.7440, Test: 0.7590\n",
      "Epoch: 919,train_loss:0.0306854, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 920,train_loss:0.0397070, Train: 1.0000, Val: 0.7380, Test: 0.7570\n",
      "Epoch: 921,train_loss:0.0404440, Train: 1.0000, Val: 0.7380, Test: 0.7700\n",
      "Epoch: 922,train_loss:0.0333843, Train: 1.0000, Val: 0.7320, Test: 0.7630\n",
      "Epoch: 923,train_loss:0.0156308, Train: 1.0000, Val: 0.7120, Test: 0.7420\n",
      "Epoch: 924,train_loss:0.0685325, Train: 1.0000, Val: 0.7240, Test: 0.7580\n",
      "Epoch: 925,train_loss:0.0303245, Train: 1.0000, Val: 0.7380, Test: 0.7630\n",
      "Epoch: 926,train_loss:0.0273601, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch: 927,train_loss:0.0230890, Train: 1.0000, Val: 0.7320, Test: 0.7420\n",
      "Epoch: 928,train_loss:0.0528131, Train: 1.0000, Val: 0.7460, Test: 0.7640\n",
      "Epoch: 929,train_loss:0.0238071, Train: 1.0000, Val: 0.7300, Test: 0.7610\n",
      "Epoch: 930,train_loss:0.0190043, Train: 1.0000, Val: 0.7380, Test: 0.7700\n",
      "Epoch: 931,train_loss:0.0170713, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 932,train_loss:0.0452155, Train: 1.0000, Val: 0.7280, Test: 0.7540\n",
      "Epoch: 933,train_loss:0.0370267, Train: 1.0000, Val: 0.7200, Test: 0.7510\n",
      "Epoch: 934,train_loss:0.0262350, Train: 1.0000, Val: 0.7140, Test: 0.7380\n",
      "Epoch: 935,train_loss:0.0198515, Train: 1.0000, Val: 0.7040, Test: 0.7210\n",
      "Epoch: 936,train_loss:0.0327689, Train: 1.0000, Val: 0.7120, Test: 0.7350\n",
      "Epoch: 937,train_loss:0.0471193, Train: 1.0000, Val: 0.7280, Test: 0.7510\n",
      "Epoch: 938,train_loss:0.0373376, Train: 1.0000, Val: 0.7340, Test: 0.7530\n",
      "Epoch: 939,train_loss:0.0288874, Train: 1.0000, Val: 0.7240, Test: 0.7490\n",
      "Epoch: 940,train_loss:0.0481812, Train: 1.0000, Val: 0.7260, Test: 0.7520\n",
      "Epoch: 941,train_loss:0.0313572, Train: 1.0000, Val: 0.7300, Test: 0.7640\n",
      "Epoch: 942,train_loss:0.0419030, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 943,train_loss:0.0229859, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
      "Epoch: 944,train_loss:0.0255550, Train: 1.0000, Val: 0.7100, Test: 0.7190\n",
      "Epoch: 945,train_loss:0.0308564, Train: 1.0000, Val: 0.7160, Test: 0.7170\n",
      "Epoch: 946,train_loss:0.0373314, Train: 1.0000, Val: 0.7140, Test: 0.7240\n",
      "Epoch: 947,train_loss:0.0138555, Train: 1.0000, Val: 0.7220, Test: 0.7160\n",
      "Epoch: 948,train_loss:0.0420327, Train: 1.0000, Val: 0.7180, Test: 0.7360\n",
      "Epoch: 949,train_loss:0.0246203, Train: 1.0000, Val: 0.7300, Test: 0.7390\n",
      "Epoch: 950,train_loss:0.0340380, Train: 1.0000, Val: 0.7300, Test: 0.7610\n",
      "Epoch: 951,train_loss:0.0098426, Train: 1.0000, Val: 0.7280, Test: 0.7710\n",
      "Epoch: 952,train_loss:0.0191236, Train: 1.0000, Val: 0.7300, Test: 0.7660\n",
      "Epoch: 953,train_loss:0.0256852, Train: 1.0000, Val: 0.7260, Test: 0.7590\n",
      "Epoch: 954,train_loss:0.1009588, Train: 1.0000, Val: 0.7420, Test: 0.7750\n",
      "Epoch: 955,train_loss:0.0224775, Train: 1.0000, Val: 0.7260, Test: 0.7550\n",
      "Epoch: 956,train_loss:0.0291359, Train: 1.0000, Val: 0.7240, Test: 0.7360\n",
      "Epoch: 957,train_loss:0.0295041, Train: 1.0000, Val: 0.7180, Test: 0.7300\n",
      "Epoch: 958,train_loss:0.0238033, Train: 1.0000, Val: 0.7120, Test: 0.7260\n",
      "Epoch: 959,train_loss:0.0252404, Train: 1.0000, Val: 0.7200, Test: 0.7230\n",
      "Epoch: 960,train_loss:0.0266263, Train: 1.0000, Val: 0.7220, Test: 0.7370\n",
      "Epoch: 961,train_loss:0.0135094, Train: 1.0000, Val: 0.7260, Test: 0.7490\n",
      "Epoch: 962,train_loss:0.0179677, Train: 1.0000, Val: 0.7340, Test: 0.7620\n",
      "Epoch: 963,train_loss:0.0205876, Train: 1.0000, Val: 0.7380, Test: 0.7670\n",
      "Epoch: 964,train_loss:0.0148788, Train: 1.0000, Val: 0.7260, Test: 0.7720\n",
      "Epoch: 965,train_loss:0.0236647, Train: 1.0000, Val: 0.7360, Test: 0.7720\n",
      "Epoch: 966,train_loss:0.0502058, Train: 1.0000, Val: 0.7280, Test: 0.7530\n",
      "Epoch: 967,train_loss:0.0277583, Train: 1.0000, Val: 0.7060, Test: 0.7370\n",
      "Epoch: 968,train_loss:0.0232668, Train: 1.0000, Val: 0.7000, Test: 0.7140\n",
      "Epoch: 969,train_loss:0.0291516, Train: 1.0000, Val: 0.7040, Test: 0.7220\n",
      "Epoch: 970,train_loss:0.0520128, Train: 1.0000, Val: 0.7120, Test: 0.7370\n",
      "Epoch: 971,train_loss:0.0282917, Train: 1.0000, Val: 0.7340, Test: 0.7480\n",
      "Epoch: 972,train_loss:0.0202927, Train: 1.0000, Val: 0.7300, Test: 0.7550\n",
      "Epoch: 973,train_loss:0.0221706, Train: 1.0000, Val: 0.7220, Test: 0.7620\n",
      "Epoch: 974,train_loss:0.0303852, Train: 1.0000, Val: 0.7300, Test: 0.7710\n",
      "Epoch: 975,train_loss:0.0156843, Train: 1.0000, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 976,train_loss:0.0434609, Train: 1.0000, Val: 0.7480, Test: 0.7680\n",
      "Epoch: 977,train_loss:0.0568952, Train: 1.0000, Val: 0.7300, Test: 0.7570\n",
      "Epoch: 978,train_loss:0.0328656, Train: 1.0000, Val: 0.7200, Test: 0.7440\n",
      "Epoch: 979,train_loss:0.0281978, Train: 1.0000, Val: 0.7280, Test: 0.7410\n",
      "Epoch: 980,train_loss:0.0389081, Train: 1.0000, Val: 0.7060, Test: 0.7180\n",
      "Epoch: 981,train_loss:0.0286782, Train: 1.0000, Val: 0.6960, Test: 0.7110\n",
      "Epoch: 982,train_loss:0.0402477, Train: 1.0000, Val: 0.7060, Test: 0.7420\n",
      "Epoch: 983,train_loss:0.0299700, Train: 1.0000, Val: 0.7280, Test: 0.7660\n",
      "Epoch: 984,train_loss:0.0204420, Train: 1.0000, Val: 0.7380, Test: 0.7660\n",
      "Epoch: 985,train_loss:0.0229511, Train: 1.0000, Val: 0.7320, Test: 0.7560\n",
      "Epoch: 986,train_loss:0.0363399, Train: 1.0000, Val: 0.7460, Test: 0.7670\n",
      "Epoch: 987,train_loss:0.0711368, Train: 1.0000, Val: 0.7360, Test: 0.7580\n",
      "Epoch: 988,train_loss:0.0218735, Train: 1.0000, Val: 0.7040, Test: 0.7250\n",
      "Epoch: 989,train_loss:0.0373748, Train: 1.0000, Val: 0.6820, Test: 0.6850\n",
      "Epoch: 990,train_loss:0.0512567, Train: 1.0000, Val: 0.6780, Test: 0.6820\n",
      "Epoch: 991,train_loss:0.0276529, Train: 1.0000, Val: 0.6920, Test: 0.6980\n",
      "Epoch: 992,train_loss:0.0244454, Train: 1.0000, Val: 0.7200, Test: 0.7200\n",
      "Epoch: 993,train_loss:0.0334882, Train: 1.0000, Val: 0.7320, Test: 0.7510\n",
      "Epoch: 994,train_loss:0.0159704, Train: 1.0000, Val: 0.7340, Test: 0.7700\n",
      "Epoch: 995,train_loss:0.0370055, Train: 1.0000, Val: 0.7400, Test: 0.7660\n",
      "Epoch: 996,train_loss:0.0528032, Train: 1.0000, Val: 0.7320, Test: 0.7510\n",
      "Epoch: 997,train_loss:0.0126969, Train: 1.0000, Val: 0.7100, Test: 0.7360\n",
      "Epoch: 998,train_loss:0.0480678, Train: 1.0000, Val: 0.7020, Test: 0.7090\n",
      "Epoch: 999,train_loss:0.0213147, Train: 1.0000, Val: 0.6660, Test: 0.6820\n",
      "Epoch: 1000,train_loss:0.0407571, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
      "CPU times: user 27.9 s, sys: 697 ms, total: 28.6 s\n",
      "Wall time: 26.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "446601"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 2 #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "        # self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_cora_mymodel,testacclist_cora_mymodel=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_cora_mymodel.append(loss)\n",
    "    testacclist_cora_mymodel.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# # %matplotlib inline\n",
    "\n",
    "# plt.plot(losslist)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000cora\n",
    "Epoch: 979,train_loss:0.1637752, Train: 1.0000, Val: 0.7000, Test: 0.7280\n",
    "Epoch: 980,train_loss:0.0791690, Train: 1.0000, Val: 0.7120, Test: 0.7390\n",
    "Epoch: 981,train_loss:0.0635783, Train: 1.0000, Val: 0.7240, Test: 0.7450\n",
    "Epoch: 982,train_loss:0.0318759, Train: 1.0000, Val: 0.7340, Test: 0.7460\n",
    "Epoch: 983,train_loss:0.0955521, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
    "Epoch: 984,train_loss:0.0810624, Train: 1.0000, Val: 0.7500, Test: 0.7670\n",
    "Epoch: 985,train_loss:0.0891033, Train: 1.0000, Val: 0.7640, Test: 0.7750\n",
    "Epoch: 986,train_loss:0.0739494, Train: 1.0000, Val: 0.7600, Test: 0.7830\n",
    "Epoch: 987,train_loss:0.0990233, Train: 1.0000, Val: 0.7560, Test: 0.7770\n",
    "Epoch: 988,train_loss:0.0988771, Train: 1.0000, Val: 0.7520, Test: 0.7710\n",
    "Epoch: 989,train_loss:0.0732601, Train: 1.0000, Val: 0.7520, Test: 0.7650\n",
    "Epoch: 990,train_loss:0.0788579, Train: 1.0000, Val: 0.7640, Test: 0.7680\n",
    "Epoch: 991,train_loss:0.0499137, Train: 1.0000, Val: 0.7620, Test: 0.7670\n",
    "Epoch: 992,train_loss:0.0951503, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
    "Epoch: 993,train_loss:0.1640868, Train: 1.0000, Val: 0.7500, Test: 0.7520\n",
    "Epoch: 994,train_loss:0.1025555, Train: 1.0000, Val: 0.7420, Test: 0.7400\n",
    "Epoch: 995,train_loss:0.0693817, Train: 1.0000, Val: 0.7480, Test: 0.7430\n",
    "Epoch: 996,train_loss:0.0883153, Train: 1.0000, Val: 0.7560, Test: 0.7580\n",
    "Epoch: 997,train_loss:0.1922379, Train: 1.0000, Val: 0.7540, Test: 0.7570\n",
    "Epoch: 998,train_loss:0.0917486, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
    "Epoch: 999,train_loss:0.1085564, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
    "Epoch: 1000,train_loss:0.0600472, Train: 1.0000, Val: 0.7560, Test: 0.7610\n",
    "[1]    5346 terminated  env PYTHONIOENCODING=UTF-8 PYTHONUNBUFFERED=1   --default --client --host    \n",
    "Wall time: 25.2 s\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sum([torch.numel(param) for param in model.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cora geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.9478176, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 002,train_loss:1.9472446, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 003,train_loss:1.9468615, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 004,train_loss:1.9465384, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 005,train_loss:1.9461062, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 006,train_loss:1.9459481, Train: 0.2786, Val: 0.1060, Test: 0.1370\n",
      "Epoch: 007,train_loss:1.9456667, Train: 0.1429, Val: 0.0720, Test: 0.0910\n",
      "Epoch: 008,train_loss:1.9454964, Train: 0.1429, Val: 0.0720, Test: 0.0910\n",
      "Epoch: 009,train_loss:1.9452314, Train: 0.1429, Val: 0.0720, Test: 0.0910\n",
      "Epoch: 010,train_loss:1.9455698, Train: 0.1429, Val: 0.0720, Test: 0.0910\n",
      "Epoch: 011,train_loss:1.9449145, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 012,train_loss:1.9442428, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 013,train_loss:1.9444253, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 014,train_loss:1.9438788, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 015,train_loss:1.9436198, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 016,train_loss:1.9439400, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 017,train_loss:1.9441221, Train: 0.1429, Val: 0.1560, Test: 0.1440\n",
      "Epoch: 018,train_loss:1.9428090, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 019,train_loss:1.9442579, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 020,train_loss:1.9415399, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 021,train_loss:1.9394270, Train: 0.1714, Val: 0.3160, Test: 0.3230\n",
      "Epoch: 022,train_loss:1.9357237, Train: 0.2643, Val: 0.3480, Test: 0.3510\n",
      "Epoch: 023,train_loss:1.9304152, Train: 0.3214, Val: 0.3440, Test: 0.3760\n",
      "Epoch: 024,train_loss:1.9152035, Train: 0.3857, Val: 0.3440, Test: 0.3790\n",
      "Epoch: 025,train_loss:1.8936731, Train: 0.3857, Val: 0.3460, Test: 0.3470\n",
      "Epoch: 026,train_loss:1.8415709, Train: 0.3286, Val: 0.3000, Test: 0.2850\n",
      "Epoch: 027,train_loss:1.7561104, Train: 0.3143, Val: 0.2600, Test: 0.2560\n",
      "Epoch: 028,train_loss:1.6638235, Train: 0.2929, Val: 0.2320, Test: 0.2200\n",
      "Epoch: 029,train_loss:1.5583212, Train: 0.2857, Val: 0.2240, Test: 0.2090\n",
      "Epoch: 030,train_loss:1.5126512, Train: 0.2929, Val: 0.2200, Test: 0.2010\n",
      "Epoch: 031,train_loss:1.6112542, Train: 0.2857, Val: 0.2080, Test: 0.2070\n",
      "Epoch: 032,train_loss:1.4144726, Train: 0.2929, Val: 0.2080, Test: 0.2040\n",
      "Epoch: 033,train_loss:1.4933838, Train: 0.2929, Val: 0.2180, Test: 0.2060\n",
      "Epoch: 034,train_loss:1.3796036, Train: 0.3643, Val: 0.2300, Test: 0.2170\n",
      "Epoch: 035,train_loss:1.4606123, Train: 0.3714, Val: 0.2580, Test: 0.2490\n",
      "Epoch: 036,train_loss:1.3669789, Train: 0.4429, Val: 0.2980, Test: 0.2940\n",
      "Epoch: 037,train_loss:1.3792754, Train: 0.4714, Val: 0.2840, Test: 0.2980\n",
      "Epoch: 038,train_loss:1.3833424, Train: 0.4643, Val: 0.3040, Test: 0.3190\n",
      "Epoch: 039,train_loss:1.3644793, Train: 0.4929, Val: 0.3320, Test: 0.3140\n",
      "Epoch: 040,train_loss:1.3375227, Train: 0.4571, Val: 0.2560, Test: 0.2660\n",
      "Epoch: 041,train_loss:1.3519827, Train: 0.4286, Val: 0.2600, Test: 0.2640\n",
      "Epoch: 042,train_loss:1.2584257, Train: 0.4143, Val: 0.2700, Test: 0.2840\n",
      "Epoch: 043,train_loss:1.3103553, Train: 0.3571, Val: 0.2580, Test: 0.2520\n",
      "Epoch: 044,train_loss:1.2520136, Train: 0.3857, Val: 0.2660, Test: 0.2590\n",
      "Epoch: 045,train_loss:1.2986174, Train: 0.4929, Val: 0.3100, Test: 0.3180\n",
      "Epoch: 046,train_loss:1.1509255, Train: 0.5429, Val: 0.2940, Test: 0.3020\n",
      "Epoch: 047,train_loss:1.2216896, Train: 0.4857, Val: 0.2600, Test: 0.2860\n",
      "Epoch: 048,train_loss:1.2444600, Train: 0.6857, Val: 0.3420, Test: 0.3760\n",
      "Epoch: 049,train_loss:1.1251125, Train: 0.7643, Val: 0.4240, Test: 0.4370\n",
      "Epoch: 050,train_loss:1.0785421, Train: 0.6929, Val: 0.4340, Test: 0.4350\n",
      "Epoch: 051,train_loss:1.1672940, Train: 0.7500, Val: 0.4300, Test: 0.4430\n",
      "Epoch: 052,train_loss:0.9980696, Train: 0.6786, Val: 0.3720, Test: 0.4030\n",
      "Epoch: 053,train_loss:0.9947740, Train: 0.6714, Val: 0.3960, Test: 0.4310\n",
      "Epoch: 054,train_loss:0.9717600, Train: 0.7143, Val: 0.5100, Test: 0.4970\n",
      "Epoch: 055,train_loss:0.9593053, Train: 0.7429, Val: 0.5080, Test: 0.4970\n",
      "Epoch: 056,train_loss:0.9511141, Train: 0.7643, Val: 0.4380, Test: 0.4770\n",
      "Epoch: 057,train_loss:0.8161311, Train: 0.6714, Val: 0.3760, Test: 0.4030\n",
      "Epoch: 058,train_loss:0.8200936, Train: 0.8214, Val: 0.4520, Test: 0.4540\n",
      "Epoch: 059,train_loss:0.8266284, Train: 0.6857, Val: 0.4580, Test: 0.4620\n",
      "Epoch: 060,train_loss:1.0348488, Train: 0.8429, Val: 0.5180, Test: 0.5210\n",
      "Epoch: 061,train_loss:0.8562998, Train: 0.8071, Val: 0.5000, Test: 0.5170\n",
      "Epoch: 062,train_loss:0.9357773, Train: 0.8286, Val: 0.5300, Test: 0.5490\n",
      "Epoch: 063,train_loss:0.9635912, Train: 0.8214, Val: 0.5540, Test: 0.5500\n",
      "Epoch: 064,train_loss:0.7264363, Train: 0.8286, Val: 0.5380, Test: 0.5390\n",
      "Epoch: 065,train_loss:0.7090856, Train: 0.7929, Val: 0.4420, Test: 0.4590\n",
      "Epoch: 066,train_loss:0.7810249, Train: 0.7857, Val: 0.4460, Test: 0.4590\n",
      "Epoch: 067,train_loss:0.8218473, Train: 0.8857, Val: 0.5680, Test: 0.5540\n",
      "Epoch: 068,train_loss:0.6146866, Train: 0.7786, Val: 0.5580, Test: 0.5430\n",
      "Epoch: 069,train_loss:0.8337080, Train: 0.8571, Val: 0.6000, Test: 0.5620\n",
      "Epoch: 070,train_loss:0.7354700, Train: 0.9071, Val: 0.5260, Test: 0.5380\n",
      "Epoch: 071,train_loss:0.6680972, Train: 0.8429, Val: 0.4500, Test: 0.4680\n",
      "Epoch: 072,train_loss:0.7518673, Train: 0.8429, Val: 0.4820, Test: 0.4990\n",
      "Epoch: 073,train_loss:0.6244599, Train: 0.8429, Val: 0.5840, Test: 0.5660\n",
      "Epoch: 074,train_loss:0.6426203, Train: 0.8571, Val: 0.6120, Test: 0.5850\n",
      "Epoch: 075,train_loss:0.6360909, Train: 0.9286, Val: 0.6340, Test: 0.6170\n",
      "Epoch: 076,train_loss:0.7279804, Train: 0.8643, Val: 0.6220, Test: 0.6030\n",
      "Epoch: 077,train_loss:0.5902946, Train: 0.8357, Val: 0.5280, Test: 0.5400\n",
      "Epoch: 078,train_loss:0.6656181, Train: 0.8429, Val: 0.5040, Test: 0.5100\n",
      "Epoch: 079,train_loss:0.5623792, Train: 0.8571, Val: 0.5420, Test: 0.5470\n",
      "Epoch: 080,train_loss:0.5342692, Train: 0.8571, Val: 0.5920, Test: 0.5910\n",
      "Epoch: 081,train_loss:0.5620788, Train: 0.8714, Val: 0.6160, Test: 0.6190\n",
      "Epoch: 082,train_loss:0.4749348, Train: 0.9286, Val: 0.6300, Test: 0.6200\n",
      "Epoch: 083,train_loss:0.5360154, Train: 0.9143, Val: 0.6220, Test: 0.6100\n",
      "Epoch: 084,train_loss:0.5571203, Train: 0.9286, Val: 0.5820, Test: 0.5740\n",
      "Epoch: 085,train_loss:0.5130011, Train: 0.9071, Val: 0.5440, Test: 0.5580\n",
      "Epoch: 086,train_loss:0.5202085, Train: 0.9143, Val: 0.6100, Test: 0.6040\n",
      "Epoch: 087,train_loss:0.3841190, Train: 0.9429, Val: 0.6260, Test: 0.6400\n",
      "Epoch: 088,train_loss:0.5087650, Train: 0.9429, Val: 0.6280, Test: 0.6390\n",
      "Epoch: 089,train_loss:0.5571134, Train: 0.9357, Val: 0.6180, Test: 0.6330\n",
      "Epoch: 090,train_loss:0.5381301, Train: 0.8929, Val: 0.6140, Test: 0.6300\n",
      "Epoch: 091,train_loss:0.5756862, Train: 0.8929, Val: 0.5980, Test: 0.6090\n",
      "Epoch: 092,train_loss:0.5293345, Train: 0.8929, Val: 0.6000, Test: 0.6060\n",
      "Epoch: 093,train_loss:0.3402381, Train: 0.8929, Val: 0.6080, Test: 0.6120\n",
      "Epoch: 094,train_loss:0.3745980, Train: 0.9071, Val: 0.6140, Test: 0.6260\n",
      "Epoch: 095,train_loss:0.3651237, Train: 0.9143, Val: 0.6120, Test: 0.6220\n",
      "Epoch: 096,train_loss:0.4015053, Train: 0.9214, Val: 0.6080, Test: 0.6120\n",
      "Epoch: 097,train_loss:0.3596680, Train: 0.9286, Val: 0.6140, Test: 0.5980\n",
      "Epoch: 098,train_loss:0.4081560, Train: 0.9286, Val: 0.6060, Test: 0.5870\n",
      "Epoch: 099,train_loss:0.5631620, Train: 0.9357, Val: 0.6280, Test: 0.6060\n",
      "Epoch: 100,train_loss:0.4261540, Train: 0.9357, Val: 0.6320, Test: 0.6190\n",
      "Epoch: 101,train_loss:0.4598900, Train: 0.9500, Val: 0.6260, Test: 0.6330\n",
      "Epoch: 102,train_loss:0.5995719, Train: 0.9500, Val: 0.6260, Test: 0.6300\n",
      "Epoch: 103,train_loss:0.4852290, Train: 0.9500, Val: 0.6420, Test: 0.6310\n",
      "Epoch: 104,train_loss:0.4332387, Train: 0.9643, Val: 0.6200, Test: 0.6040\n",
      "Epoch: 105,train_loss:0.4432591, Train: 0.9286, Val: 0.6140, Test: 0.6100\n",
      "Epoch: 106,train_loss:0.4669633, Train: 0.9000, Val: 0.6220, Test: 0.6320\n",
      "Epoch: 107,train_loss:0.3742488, Train: 0.8857, Val: 0.6240, Test: 0.6440\n",
      "Epoch: 108,train_loss:0.5088887, Train: 0.8929, Val: 0.6260, Test: 0.6450\n",
      "Epoch: 109,train_loss:0.5494719, Train: 0.9429, Val: 0.6240, Test: 0.6470\n",
      "Epoch: 110,train_loss:0.4153220, Train: 0.9429, Val: 0.6260, Test: 0.6290\n",
      "Epoch: 111,train_loss:0.4604148, Train: 0.9500, Val: 0.6280, Test: 0.6230\n",
      "Epoch: 112,train_loss:0.4782364, Train: 0.9643, Val: 0.6560, Test: 0.6410\n",
      "Epoch: 113,train_loss:0.4162696, Train: 0.9714, Val: 0.6560, Test: 0.6390\n",
      "Epoch: 114,train_loss:0.3910257, Train: 0.9714, Val: 0.6600, Test: 0.6440\n",
      "Epoch: 115,train_loss:0.4796859, Train: 0.9714, Val: 0.6480, Test: 0.6410\n",
      "Epoch: 116,train_loss:0.4579839, Train: 0.9714, Val: 0.6280, Test: 0.6390\n",
      "Epoch: 117,train_loss:0.3812236, Train: 0.9500, Val: 0.6060, Test: 0.6230\n",
      "Epoch: 118,train_loss:0.4622645, Train: 0.9143, Val: 0.6100, Test: 0.6290\n",
      "Epoch: 119,train_loss:0.3858159, Train: 0.9071, Val: 0.6380, Test: 0.6500\n",
      "Epoch: 120,train_loss:0.3643000, Train: 0.9357, Val: 0.6380, Test: 0.6650\n",
      "Epoch: 121,train_loss:0.4713314, Train: 0.9643, Val: 0.6560, Test: 0.6760\n",
      "Epoch: 122,train_loss:0.2931906, Train: 0.9714, Val: 0.6480, Test: 0.6520\n",
      "Epoch: 123,train_loss:0.3949460, Train: 0.9786, Val: 0.6300, Test: 0.6450\n",
      "Epoch: 124,train_loss:0.3847333, Train: 0.9786, Val: 0.6500, Test: 0.6570\n",
      "Epoch: 125,train_loss:0.4078062, Train: 0.9714, Val: 0.6560, Test: 0.6600\n",
      "Epoch: 126,train_loss:0.3977121, Train: 0.9714, Val: 0.6480, Test: 0.6620\n",
      "Epoch: 127,train_loss:0.3713237, Train: 0.9857, Val: 0.6360, Test: 0.6580\n",
      "Epoch: 128,train_loss:0.3484847, Train: 0.9786, Val: 0.6400, Test: 0.6420\n",
      "Epoch: 129,train_loss:0.4321621, Train: 0.9857, Val: 0.6440, Test: 0.6540\n",
      "Epoch: 130,train_loss:0.3103463, Train: 0.9786, Val: 0.6380, Test: 0.6620\n",
      "Epoch: 131,train_loss:0.3569054, Train: 0.9714, Val: 0.6320, Test: 0.6420\n",
      "Epoch: 132,train_loss:0.3355606, Train: 0.9786, Val: 0.6340, Test: 0.6460\n",
      "Epoch: 133,train_loss:0.3143694, Train: 0.9929, Val: 0.6420, Test: 0.6570\n",
      "Epoch: 134,train_loss:0.3272837, Train: 0.9857, Val: 0.6480, Test: 0.6350\n",
      "Epoch: 135,train_loss:0.3317899, Train: 0.9714, Val: 0.6320, Test: 0.6380\n",
      "Epoch: 136,train_loss:0.3381743, Train: 0.9929, Val: 0.6540, Test: 0.6630\n",
      "Epoch: 137,train_loss:0.2999102, Train: 1.0000, Val: 0.6480, Test: 0.6580\n",
      "Epoch: 138,train_loss:0.3192452, Train: 0.9857, Val: 0.6540, Test: 0.6670\n",
      "Epoch: 139,train_loss:0.2248316, Train: 0.9857, Val: 0.6680, Test: 0.6660\n",
      "Epoch: 140,train_loss:0.2936242, Train: 1.0000, Val: 0.6520, Test: 0.6740\n",
      "Epoch: 141,train_loss:0.3269386, Train: 1.0000, Val: 0.6520, Test: 0.6750\n",
      "Epoch: 142,train_loss:0.2640914, Train: 1.0000, Val: 0.6540, Test: 0.6700\n",
      "Epoch: 143,train_loss:0.3302375, Train: 0.9929, Val: 0.6540, Test: 0.6730\n",
      "Epoch: 144,train_loss:0.3305583, Train: 1.0000, Val: 0.6440, Test: 0.6640\n",
      "Epoch: 145,train_loss:0.2617685, Train: 1.0000, Val: 0.6640, Test: 0.6540\n",
      "Epoch: 146,train_loss:0.3075724, Train: 1.0000, Val: 0.6480, Test: 0.6510\n",
      "Epoch: 147,train_loss:0.2939169, Train: 0.9929, Val: 0.6360, Test: 0.6620\n",
      "Epoch: 148,train_loss:0.2974232, Train: 1.0000, Val: 0.6640, Test: 0.6730\n",
      "Epoch: 149,train_loss:0.3322361, Train: 0.9857, Val: 0.6320, Test: 0.6690\n",
      "Epoch: 150,train_loss:0.3371471, Train: 0.9786, Val: 0.6440, Test: 0.6580\n",
      "Epoch: 151,train_loss:0.3071421, Train: 0.9929, Val: 0.6580, Test: 0.6860\n",
      "Epoch: 152,train_loss:0.2514389, Train: 0.9929, Val: 0.6340, Test: 0.6610\n",
      "Epoch: 153,train_loss:0.2733749, Train: 0.9929, Val: 0.6400, Test: 0.6630\n",
      "Epoch: 154,train_loss:0.2455806, Train: 0.9929, Val: 0.6600, Test: 0.6740\n",
      "Epoch: 155,train_loss:0.2569949, Train: 1.0000, Val: 0.6700, Test: 0.7020\n",
      "Epoch: 156,train_loss:0.2970052, Train: 1.0000, Val: 0.6740, Test: 0.6990\n",
      "Epoch: 157,train_loss:0.1318088, Train: 0.9929, Val: 0.6660, Test: 0.6750\n",
      "Epoch: 158,train_loss:0.5272277, Train: 1.0000, Val: 0.6660, Test: 0.6850\n",
      "Epoch: 159,train_loss:0.2696779, Train: 0.9929, Val: 0.6700, Test: 0.6930\n",
      "Epoch: 160,train_loss:0.2129643, Train: 0.9929, Val: 0.6680, Test: 0.6790\n",
      "Epoch: 161,train_loss:0.1986252, Train: 0.9929, Val: 0.6740, Test: 0.6850\n",
      "Epoch: 162,train_loss:0.2721114, Train: 0.9929, Val: 0.6840, Test: 0.6830\n",
      "Epoch: 163,train_loss:0.2867439, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
      "Epoch: 164,train_loss:0.2126868, Train: 1.0000, Val: 0.6820, Test: 0.6790\n",
      "Epoch: 165,train_loss:0.2594183, Train: 0.9929, Val: 0.6700, Test: 0.6760\n",
      "Epoch: 166,train_loss:0.2339764, Train: 0.9929, Val: 0.6420, Test: 0.6610\n",
      "Epoch: 167,train_loss:0.2821548, Train: 0.9929, Val: 0.6460, Test: 0.6520\n",
      "Epoch: 168,train_loss:0.2288902, Train: 0.9929, Val: 0.6680, Test: 0.6790\n",
      "Epoch: 169,train_loss:0.2276036, Train: 0.9929, Val: 0.6700, Test: 0.6870\n",
      "Epoch: 170,train_loss:0.2517965, Train: 1.0000, Val: 0.6760, Test: 0.6950\n",
      "Epoch: 171,train_loss:0.2117959, Train: 1.0000, Val: 0.6660, Test: 0.6920\n",
      "Epoch: 172,train_loss:0.3395437, Train: 1.0000, Val: 0.6700, Test: 0.6900\n",
      "Epoch: 173,train_loss:0.2052571, Train: 1.0000, Val: 0.6660, Test: 0.6920\n",
      "Epoch: 174,train_loss:0.2646642, Train: 1.0000, Val: 0.6600, Test: 0.6810\n",
      "Epoch: 175,train_loss:0.2181758, Train: 1.0000, Val: 0.6540, Test: 0.6720\n",
      "Epoch: 176,train_loss:0.2608273, Train: 1.0000, Val: 0.6440, Test: 0.6650\n",
      "Epoch: 177,train_loss:0.3687246, Train: 1.0000, Val: 0.6520, Test: 0.6830\n",
      "Epoch: 178,train_loss:0.1810664, Train: 1.0000, Val: 0.6600, Test: 0.6830\n",
      "Epoch: 179,train_loss:0.2011605, Train: 0.9929, Val: 0.6620, Test: 0.6760\n",
      "Epoch: 180,train_loss:0.2755493, Train: 0.9929, Val: 0.6540, Test: 0.6690\n",
      "Epoch: 181,train_loss:0.2652326, Train: 1.0000, Val: 0.6480, Test: 0.6790\n",
      "Epoch: 182,train_loss:0.2060430, Train: 1.0000, Val: 0.6400, Test: 0.6650\n",
      "Epoch: 183,train_loss:0.2168530, Train: 1.0000, Val: 0.6420, Test: 0.6720\n",
      "Epoch: 184,train_loss:0.1997476, Train: 1.0000, Val: 0.6480, Test: 0.6700\n",
      "Epoch: 185,train_loss:0.1615639, Train: 1.0000, Val: 0.6520, Test: 0.6810\n",
      "Epoch: 186,train_loss:0.2153217, Train: 0.9929, Val: 0.6620, Test: 0.6870\n",
      "Epoch: 187,train_loss:0.2375396, Train: 0.9929, Val: 0.6680, Test: 0.6820\n",
      "Epoch: 188,train_loss:0.1945509, Train: 1.0000, Val: 0.6540, Test: 0.6830\n",
      "Epoch: 189,train_loss:0.2260464, Train: 1.0000, Val: 0.6580, Test: 0.6850\n",
      "Epoch: 190,train_loss:0.1187147, Train: 1.0000, Val: 0.6660, Test: 0.6800\n",
      "Epoch: 191,train_loss:0.1456063, Train: 1.0000, Val: 0.6620, Test: 0.6730\n",
      "Epoch: 192,train_loss:0.2150943, Train: 1.0000, Val: 0.6620, Test: 0.6630\n",
      "Epoch: 193,train_loss:0.2705094, Train: 1.0000, Val: 0.6580, Test: 0.6620\n",
      "Epoch: 194,train_loss:0.2503321, Train: 1.0000, Val: 0.6640, Test: 0.6720\n",
      "Epoch: 195,train_loss:0.1912055, Train: 1.0000, Val: 0.6620, Test: 0.6670\n",
      "Epoch: 196,train_loss:0.1691620, Train: 1.0000, Val: 0.6700, Test: 0.6770\n",
      "Epoch: 197,train_loss:0.2740309, Train: 1.0000, Val: 0.6600, Test: 0.6850\n",
      "Epoch: 198,train_loss:0.2733140, Train: 1.0000, Val: 0.6580, Test: 0.6780\n",
      "Epoch: 199,train_loss:0.1195257, Train: 0.9929, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 200,train_loss:0.1506391, Train: 0.9929, Val: 0.6700, Test: 0.6770\n",
      "Epoch: 201,train_loss:0.1809750, Train: 0.9929, Val: 0.6720, Test: 0.6760\n",
      "Epoch: 202,train_loss:0.2551468, Train: 1.0000, Val: 0.6680, Test: 0.6820\n",
      "Epoch: 203,train_loss:0.2831499, Train: 1.0000, Val: 0.6820, Test: 0.6840\n",
      "Epoch: 204,train_loss:0.1939989, Train: 1.0000, Val: 0.6660, Test: 0.6790\n",
      "Epoch: 205,train_loss:0.2405740, Train: 1.0000, Val: 0.6540, Test: 0.6650\n",
      "Epoch: 206,train_loss:0.2035109, Train: 1.0000, Val: 0.6500, Test: 0.6650\n",
      "Epoch: 207,train_loss:0.1799852, Train: 1.0000, Val: 0.6760, Test: 0.6690\n",
      "Epoch: 208,train_loss:0.2069031, Train: 1.0000, Val: 0.6840, Test: 0.6870\n",
      "Epoch: 209,train_loss:0.1326462, Train: 1.0000, Val: 0.6680, Test: 0.6770\n",
      "Epoch: 210,train_loss:0.1579319, Train: 1.0000, Val: 0.6480, Test: 0.6620\n",
      "Epoch: 211,train_loss:0.2535988, Train: 1.0000, Val: 0.6840, Test: 0.6810\n",
      "Epoch: 212,train_loss:0.2973655, Train: 1.0000, Val: 0.6920, Test: 0.6990\n",
      "Epoch: 213,train_loss:0.2332919, Train: 1.0000, Val: 0.6520, Test: 0.6690\n",
      "Epoch: 214,train_loss:0.2153528, Train: 1.0000, Val: 0.6500, Test: 0.6600\n",
      "Epoch: 215,train_loss:0.2136267, Train: 1.0000, Val: 0.6860, Test: 0.6770\n",
      "Epoch: 216,train_loss:0.2914935, Train: 1.0000, Val: 0.6860, Test: 0.6820\n",
      "Epoch: 217,train_loss:0.1745757, Train: 1.0000, Val: 0.6720, Test: 0.6640\n",
      "Epoch: 218,train_loss:0.2190004, Train: 1.0000, Val: 0.6600, Test: 0.6480\n",
      "Epoch: 219,train_loss:0.2063973, Train: 1.0000, Val: 0.6600, Test: 0.6550\n",
      "Epoch: 220,train_loss:0.2245461, Train: 1.0000, Val: 0.6620, Test: 0.6600\n",
      "Epoch: 221,train_loss:0.1693557, Train: 1.0000, Val: 0.6620, Test: 0.6520\n",
      "Epoch: 222,train_loss:0.2442479, Train: 1.0000, Val: 0.6780, Test: 0.6720\n",
      "Epoch: 223,train_loss:0.1623855, Train: 0.9929, Val: 0.6560, Test: 0.6630\n",
      "Epoch: 224,train_loss:0.2261211, Train: 0.9929, Val: 0.6600, Test: 0.6560\n",
      "Epoch: 225,train_loss:0.2943073, Train: 1.0000, Val: 0.6700, Test: 0.6720\n",
      "Epoch: 226,train_loss:0.2598490, Train: 1.0000, Val: 0.6600, Test: 0.6740\n",
      "Epoch: 227,train_loss:0.2924135, Train: 1.0000, Val: 0.6500, Test: 0.6580\n",
      "Epoch: 228,train_loss:0.2639502, Train: 1.0000, Val: 0.6620, Test: 0.6760\n",
      "Epoch: 229,train_loss:0.2105870, Train: 1.0000, Val: 0.6820, Test: 0.6930\n",
      "Epoch: 230,train_loss:0.2143494, Train: 1.0000, Val: 0.6800, Test: 0.6710\n",
      "Epoch: 231,train_loss:0.2674426, Train: 1.0000, Val: 0.6800, Test: 0.6750\n",
      "Epoch: 232,train_loss:0.1896017, Train: 1.0000, Val: 0.6700, Test: 0.6800\n",
      "Epoch: 233,train_loss:0.2057201, Train: 1.0000, Val: 0.6760, Test: 0.6690\n",
      "Epoch: 234,train_loss:0.2684292, Train: 1.0000, Val: 0.6500, Test: 0.6560\n",
      "Epoch: 235,train_loss:0.2490185, Train: 0.9857, Val: 0.6320, Test: 0.6500\n",
      "Epoch: 236,train_loss:0.2494597, Train: 1.0000, Val: 0.6600, Test: 0.6660\n",
      "Epoch: 237,train_loss:0.2545880, Train: 1.0000, Val: 0.6600, Test: 0.6660\n",
      "Epoch: 238,train_loss:0.1475955, Train: 1.0000, Val: 0.6460, Test: 0.6690\n",
      "Epoch: 239,train_loss:0.2357471, Train: 1.0000, Val: 0.6540, Test: 0.6670\n",
      "Epoch: 240,train_loss:0.2108673, Train: 1.0000, Val: 0.6600, Test: 0.6750\n",
      "Epoch: 241,train_loss:0.1963014, Train: 1.0000, Val: 0.6680, Test: 0.6800\n",
      "Epoch: 242,train_loss:0.2068652, Train: 1.0000, Val: 0.6640, Test: 0.6790\n",
      "Epoch: 243,train_loss:0.1473906, Train: 1.0000, Val: 0.6560, Test: 0.6640\n",
      "Epoch: 244,train_loss:0.2037193, Train: 1.0000, Val: 0.6600, Test: 0.6800\n",
      "Epoch: 245,train_loss:0.1310614, Train: 1.0000, Val: 0.6620, Test: 0.6910\n",
      "Epoch: 246,train_loss:0.1370117, Train: 1.0000, Val: 0.6600, Test: 0.6870\n",
      "Epoch: 247,train_loss:0.1917638, Train: 1.0000, Val: 0.6640, Test: 0.6880\n",
      "Epoch: 248,train_loss:0.2066795, Train: 1.0000, Val: 0.6600, Test: 0.6750\n",
      "Epoch: 249,train_loss:0.1798957, Train: 1.0000, Val: 0.6480, Test: 0.6560\n",
      "Epoch: 250,train_loss:0.2376739, Train: 1.0000, Val: 0.6360, Test: 0.6290\n",
      "Epoch: 251,train_loss:0.3514807, Train: 1.0000, Val: 0.6540, Test: 0.6590\n",
      "Epoch: 252,train_loss:0.2801194, Train: 1.0000, Val: 0.6760, Test: 0.6990\n",
      "Epoch: 253,train_loss:0.1236231, Train: 0.9857, Val: 0.6520, Test: 0.6520\n",
      "Epoch: 254,train_loss:0.3500663, Train: 0.9857, Val: 0.6640, Test: 0.6790\n",
      "Epoch: 255,train_loss:0.2145339, Train: 1.0000, Val: 0.6760, Test: 0.7060\n",
      "Epoch: 256,train_loss:0.3479059, Train: 1.0000, Val: 0.6600, Test: 0.6780\n",
      "Epoch: 257,train_loss:0.1797463, Train: 1.0000, Val: 0.6460, Test: 0.6450\n",
      "Epoch: 258,train_loss:0.2699011, Train: 1.0000, Val: 0.6620, Test: 0.6700\n",
      "Epoch: 259,train_loss:0.2671709, Train: 1.0000, Val: 0.6760, Test: 0.7070\n",
      "Epoch: 260,train_loss:0.1863097, Train: 1.0000, Val: 0.6520, Test: 0.6710\n",
      "Epoch: 261,train_loss:0.2063936, Train: 0.9929, Val: 0.6140, Test: 0.6300\n",
      "Epoch: 262,train_loss:0.3037683, Train: 1.0000, Val: 0.6400, Test: 0.6510\n",
      "Epoch: 263,train_loss:0.2130516, Train: 1.0000, Val: 0.6740, Test: 0.6930\n",
      "Epoch: 264,train_loss:0.1802962, Train: 1.0000, Val: 0.6720, Test: 0.6860\n",
      "Epoch: 265,train_loss:0.2449140, Train: 1.0000, Val: 0.6580, Test: 0.6680\n",
      "Epoch: 266,train_loss:0.2346721, Train: 1.0000, Val: 0.6520, Test: 0.6640\n",
      "Epoch: 267,train_loss:0.3243455, Train: 1.0000, Val: 0.6700, Test: 0.6860\n",
      "Epoch: 268,train_loss:0.1636118, Train: 1.0000, Val: 0.6780, Test: 0.6900\n",
      "Epoch: 269,train_loss:0.2600792, Train: 1.0000, Val: 0.6440, Test: 0.6850\n",
      "Epoch: 270,train_loss:0.1800746, Train: 1.0000, Val: 0.6360, Test: 0.6580\n",
      "Epoch: 271,train_loss:0.3036415, Train: 1.0000, Val: 0.6540, Test: 0.6740\n",
      "Epoch: 272,train_loss:0.2137727, Train: 1.0000, Val: 0.6620, Test: 0.6810\n",
      "Epoch: 273,train_loss:0.1408360, Train: 0.9929, Val: 0.6560, Test: 0.6720\n",
      "Epoch: 274,train_loss:0.2178231, Train: 0.9929, Val: 0.6580, Test: 0.6770\n",
      "Epoch: 275,train_loss:0.2309136, Train: 1.0000, Val: 0.6640, Test: 0.6750\n",
      "Epoch: 276,train_loss:0.2360183, Train: 1.0000, Val: 0.6820, Test: 0.6850\n",
      "Epoch: 277,train_loss:0.1621079, Train: 1.0000, Val: 0.6780, Test: 0.6880\n",
      "Epoch: 278,train_loss:0.2128002, Train: 1.0000, Val: 0.6760, Test: 0.6830\n",
      "Epoch: 279,train_loss:0.1831411, Train: 1.0000, Val: 0.6640, Test: 0.6750\n",
      "Epoch: 280,train_loss:0.1596593, Train: 1.0000, Val: 0.6620, Test: 0.6740\n",
      "Epoch: 281,train_loss:0.1487506, Train: 1.0000, Val: 0.6700, Test: 0.6740\n",
      "Epoch: 282,train_loss:0.2155524, Train: 1.0000, Val: 0.6860, Test: 0.6810\n",
      "Epoch: 283,train_loss:0.1241855, Train: 1.0000, Val: 0.6760, Test: 0.6770\n",
      "Epoch: 284,train_loss:0.1653223, Train: 1.0000, Val: 0.6620, Test: 0.6750\n",
      "Epoch: 285,train_loss:0.2271601, Train: 1.0000, Val: 0.6540, Test: 0.6630\n",
      "Epoch: 286,train_loss:0.2827524, Train: 1.0000, Val: 0.6540, Test: 0.6730\n",
      "Epoch: 287,train_loss:0.3242802, Train: 1.0000, Val: 0.6540, Test: 0.6710\n",
      "Epoch: 288,train_loss:0.3351881, Train: 1.0000, Val: 0.6540, Test: 0.6730\n",
      "Epoch: 289,train_loss:0.2472635, Train: 1.0000, Val: 0.6500, Test: 0.6710\n",
      "Epoch: 290,train_loss:0.2672896, Train: 1.0000, Val: 0.6700, Test: 0.6700\n",
      "Epoch: 291,train_loss:0.2279923, Train: 1.0000, Val: 0.6360, Test: 0.6650\n",
      "Epoch: 292,train_loss:0.3398536, Train: 0.9786, Val: 0.6380, Test: 0.6600\n",
      "Epoch: 293,train_loss:0.2754737, Train: 0.9929, Val: 0.6480, Test: 0.6610\n",
      "Epoch: 294,train_loss:0.2596948, Train: 1.0000, Val: 0.6820, Test: 0.6880\n",
      "Epoch: 295,train_loss:0.1582299, Train: 1.0000, Val: 0.6820, Test: 0.6940\n",
      "Epoch: 296,train_loss:0.2077643, Train: 1.0000, Val: 0.6720, Test: 0.6860\n",
      "Epoch: 297,train_loss:0.2726075, Train: 1.0000, Val: 0.6440, Test: 0.6630\n",
      "Epoch: 298,train_loss:0.2443232, Train: 1.0000, Val: 0.6460, Test: 0.6660\n",
      "Epoch: 299,train_loss:0.2504444, Train: 1.0000, Val: 0.6580, Test: 0.6790\n",
      "Epoch: 300,train_loss:0.2189455, Train: 1.0000, Val: 0.6660, Test: 0.6840\n",
      "Epoch: 301,train_loss:0.2426101, Train: 1.0000, Val: 0.6680, Test: 0.6930\n",
      "Epoch: 302,train_loss:0.1786366, Train: 1.0000, Val: 0.6760, Test: 0.6930\n",
      "Epoch: 303,train_loss:0.2268206, Train: 1.0000, Val: 0.6760, Test: 0.6900\n",
      "Epoch: 304,train_loss:0.2174105, Train: 1.0000, Val: 0.6820, Test: 0.6920\n",
      "Epoch: 305,train_loss:0.1634686, Train: 1.0000, Val: 0.6800, Test: 0.6960\n",
      "Epoch: 306,train_loss:0.1116937, Train: 1.0000, Val: 0.6840, Test: 0.6920\n",
      "Epoch: 307,train_loss:0.2027208, Train: 1.0000, Val: 0.6600, Test: 0.6910\n",
      "Epoch: 308,train_loss:0.2124914, Train: 1.0000, Val: 0.6540, Test: 0.6920\n",
      "Epoch: 309,train_loss:0.1631784, Train: 1.0000, Val: 0.6600, Test: 0.6880\n",
      "Epoch: 310,train_loss:0.2488060, Train: 1.0000, Val: 0.6700, Test: 0.6860\n",
      "Epoch: 311,train_loss:0.1234789, Train: 1.0000, Val: 0.6700, Test: 0.6740\n",
      "Epoch: 312,train_loss:0.2185605, Train: 1.0000, Val: 0.6760, Test: 0.6880\n",
      "Epoch: 313,train_loss:0.2865040, Train: 1.0000, Val: 0.6640, Test: 0.6990\n",
      "Epoch: 314,train_loss:0.2697005, Train: 1.0000, Val: 0.6600, Test: 0.6980\n",
      "Epoch: 315,train_loss:0.1920798, Train: 1.0000, Val: 0.6660, Test: 0.6950\n",
      "Epoch: 316,train_loss:0.2032295, Train: 1.0000, Val: 0.6640, Test: 0.6980\n",
      "Epoch: 317,train_loss:0.1170959, Train: 1.0000, Val: 0.6580, Test: 0.6790\n",
      "Epoch: 318,train_loss:0.3286298, Train: 1.0000, Val: 0.6460, Test: 0.6680\n",
      "Epoch: 319,train_loss:0.2822341, Train: 1.0000, Val: 0.6640, Test: 0.6940\n",
      "Epoch: 320,train_loss:0.2178891, Train: 1.0000, Val: 0.6760, Test: 0.7050\n",
      "Epoch: 321,train_loss:0.1580799, Train: 1.0000, Val: 0.6620, Test: 0.6990\n",
      "Epoch: 322,train_loss:0.1768317, Train: 0.9929, Val: 0.6520, Test: 0.6970\n",
      "Epoch: 323,train_loss:0.2388257, Train: 0.9929, Val: 0.6600, Test: 0.7040\n",
      "Epoch: 324,train_loss:0.1840464, Train: 1.0000, Val: 0.6640, Test: 0.7070\n",
      "Epoch: 325,train_loss:0.1926622, Train: 1.0000, Val: 0.6600, Test: 0.6890\n",
      "Epoch: 326,train_loss:0.2685751, Train: 1.0000, Val: 0.6600, Test: 0.6950\n",
      "Epoch: 327,train_loss:0.1576108, Train: 1.0000, Val: 0.6580, Test: 0.6880\n",
      "Epoch: 328,train_loss:0.1919384, Train: 1.0000, Val: 0.6720, Test: 0.6920\n",
      "Epoch: 329,train_loss:0.1532004, Train: 1.0000, Val: 0.6620, Test: 0.6870\n",
      "Epoch: 330,train_loss:0.2709534, Train: 1.0000, Val: 0.6700, Test: 0.6960\n",
      "Epoch: 331,train_loss:0.2284641, Train: 1.0000, Val: 0.6720, Test: 0.6990\n",
      "Epoch: 332,train_loss:0.1531176, Train: 1.0000, Val: 0.6640, Test: 0.7090\n",
      "Epoch: 333,train_loss:0.2691951, Train: 1.0000, Val: 0.6780, Test: 0.7110\n",
      "Epoch: 334,train_loss:0.1766439, Train: 1.0000, Val: 0.6880, Test: 0.7060\n",
      "Epoch: 335,train_loss:0.1341472, Train: 1.0000, Val: 0.6760, Test: 0.6850\n",
      "Epoch: 336,train_loss:0.2493262, Train: 1.0000, Val: 0.6760, Test: 0.6890\n",
      "Epoch: 337,train_loss:0.2337147, Train: 1.0000, Val: 0.6840, Test: 0.7120\n",
      "Epoch: 338,train_loss:0.2515763, Train: 1.0000, Val: 0.6780, Test: 0.7180\n",
      "Epoch: 339,train_loss:0.2909617, Train: 1.0000, Val: 0.6660, Test: 0.7100\n",
      "Epoch: 340,train_loss:0.2200966, Train: 1.0000, Val: 0.6700, Test: 0.6990\n",
      "Epoch: 341,train_loss:0.1617672, Train: 1.0000, Val: 0.6580, Test: 0.6900\n",
      "Epoch: 342,train_loss:0.1960793, Train: 1.0000, Val: 0.6500, Test: 0.6840\n",
      "Epoch: 343,train_loss:0.2142438, Train: 1.0000, Val: 0.6640, Test: 0.6920\n",
      "Epoch: 344,train_loss:0.2422184, Train: 1.0000, Val: 0.6680, Test: 0.6950\n",
      "Epoch: 345,train_loss:0.1578024, Train: 1.0000, Val: 0.6740, Test: 0.6950\n",
      "Epoch: 346,train_loss:0.1606413, Train: 0.9929, Val: 0.6680, Test: 0.6930\n",
      "Epoch: 347,train_loss:0.1674013, Train: 0.9929, Val: 0.6700, Test: 0.6970\n",
      "Epoch: 348,train_loss:0.2906111, Train: 1.0000, Val: 0.6780, Test: 0.7000\n",
      "Epoch: 349,train_loss:0.1304577, Train: 1.0000, Val: 0.6900, Test: 0.6980\n",
      "Epoch: 350,train_loss:0.2161195, Train: 1.0000, Val: 0.6900, Test: 0.7070\n",
      "Epoch: 351,train_loss:0.1845899, Train: 1.0000, Val: 0.6840, Test: 0.7050\n",
      "Epoch: 352,train_loss:0.2236357, Train: 1.0000, Val: 0.6880, Test: 0.6980\n",
      "Epoch: 353,train_loss:0.1964253, Train: 1.0000, Val: 0.6860, Test: 0.7010\n",
      "Epoch: 354,train_loss:0.2558612, Train: 1.0000, Val: 0.6880, Test: 0.7040\n",
      "Epoch: 355,train_loss:0.1729913, Train: 1.0000, Val: 0.6840, Test: 0.7030\n",
      "Epoch: 356,train_loss:0.2420034, Train: 1.0000, Val: 0.6800, Test: 0.6950\n",
      "Epoch: 357,train_loss:0.1555466, Train: 1.0000, Val: 0.6940, Test: 0.7060\n",
      "Epoch: 358,train_loss:0.1349908, Train: 1.0000, Val: 0.7020, Test: 0.7030\n",
      "Epoch: 359,train_loss:0.1601579, Train: 1.0000, Val: 0.6860, Test: 0.7010\n",
      "Epoch: 360,train_loss:0.1527677, Train: 1.0000, Val: 0.6740, Test: 0.6970\n",
      "Epoch: 361,train_loss:0.2861996, Train: 1.0000, Val: 0.6880, Test: 0.7050\n",
      "Epoch: 362,train_loss:0.3191195, Train: 1.0000, Val: 0.6860, Test: 0.6980\n",
      "Epoch: 363,train_loss:0.2294648, Train: 1.0000, Val: 0.6820, Test: 0.7030\n",
      "Epoch: 364,train_loss:0.1629157, Train: 1.0000, Val: 0.6800, Test: 0.7040\n",
      "Epoch: 365,train_loss:0.1602309, Train: 1.0000, Val: 0.6760, Test: 0.7070\n",
      "Epoch: 366,train_loss:0.2071523, Train: 1.0000, Val: 0.6780, Test: 0.7110\n",
      "Epoch: 367,train_loss:0.1841120, Train: 1.0000, Val: 0.6780, Test: 0.7010\n",
      "Epoch: 368,train_loss:0.2397648, Train: 1.0000, Val: 0.6780, Test: 0.7000\n",
      "Epoch: 369,train_loss:0.1609841, Train: 1.0000, Val: 0.6780, Test: 0.7040\n",
      "Epoch: 370,train_loss:0.1816007, Train: 1.0000, Val: 0.6820, Test: 0.7090\n",
      "Epoch: 371,train_loss:0.1712647, Train: 1.0000, Val: 0.6880, Test: 0.7030\n",
      "Epoch: 372,train_loss:0.1486605, Train: 1.0000, Val: 0.6920, Test: 0.6960\n",
      "Epoch: 373,train_loss:0.1689511, Train: 1.0000, Val: 0.6680, Test: 0.7060\n",
      "Epoch: 374,train_loss:0.2123895, Train: 1.0000, Val: 0.6700, Test: 0.7150\n",
      "Epoch: 375,train_loss:0.2087935, Train: 0.9929, Val: 0.6680, Test: 0.7000\n",
      "Epoch: 376,train_loss:0.2009027, Train: 1.0000, Val: 0.6780, Test: 0.7070\n",
      "Epoch: 377,train_loss:0.1820480, Train: 1.0000, Val: 0.6880, Test: 0.7210\n",
      "Epoch: 378,train_loss:0.1948574, Train: 1.0000, Val: 0.6960, Test: 0.7280\n",
      "Epoch: 379,train_loss:0.1737630, Train: 1.0000, Val: 0.6980, Test: 0.7220\n",
      "Epoch: 380,train_loss:0.1244494, Train: 1.0000, Val: 0.7060, Test: 0.7220\n",
      "Epoch: 381,train_loss:0.1162590, Train: 1.0000, Val: 0.7060, Test: 0.7220\n",
      "Epoch: 382,train_loss:0.2315204, Train: 1.0000, Val: 0.7120, Test: 0.7210\n",
      "Epoch: 383,train_loss:0.1261954, Train: 1.0000, Val: 0.6920, Test: 0.7210\n",
      "Epoch: 384,train_loss:0.1966683, Train: 1.0000, Val: 0.7020, Test: 0.7250\n",
      "Epoch: 385,train_loss:0.1377401, Train: 1.0000, Val: 0.6900, Test: 0.7190\n",
      "Epoch: 386,train_loss:0.1792473, Train: 1.0000, Val: 0.6900, Test: 0.7100\n",
      "Epoch: 387,train_loss:0.2581643, Train: 1.0000, Val: 0.7220, Test: 0.7240\n",
      "Epoch: 388,train_loss:0.1397491, Train: 1.0000, Val: 0.7280, Test: 0.7350\n",
      "Epoch: 389,train_loss:0.1911601, Train: 1.0000, Val: 0.7100, Test: 0.7360\n",
      "Epoch: 390,train_loss:0.1809777, Train: 1.0000, Val: 0.6960, Test: 0.7330\n",
      "Epoch: 391,train_loss:0.1608003, Train: 1.0000, Val: 0.6760, Test: 0.6980\n",
      "Epoch: 392,train_loss:0.1997091, Train: 1.0000, Val: 0.6940, Test: 0.7070\n",
      "Epoch: 393,train_loss:0.2056552, Train: 1.0000, Val: 0.7280, Test: 0.7310\n",
      "Epoch: 394,train_loss:0.1976254, Train: 1.0000, Val: 0.7220, Test: 0.7360\n",
      "Epoch: 395,train_loss:0.1261453, Train: 1.0000, Val: 0.6920, Test: 0.7140\n",
      "Epoch: 396,train_loss:0.1574894, Train: 1.0000, Val: 0.7100, Test: 0.7280\n",
      "Epoch: 397,train_loss:0.1697468, Train: 1.0000, Val: 0.7280, Test: 0.7340\n",
      "Epoch: 398,train_loss:0.1733062, Train: 1.0000, Val: 0.7220, Test: 0.7390\n",
      "Epoch: 399,train_loss:0.1236839, Train: 1.0000, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 400,train_loss:0.2332808, Train: 1.0000, Val: 0.7120, Test: 0.7120\n",
      "Epoch: 401,train_loss:0.2341605, Train: 1.0000, Val: 0.7120, Test: 0.7300\n",
      "Epoch: 402,train_loss:0.1300640, Train: 1.0000, Val: 0.7100, Test: 0.7310\n",
      "Epoch: 403,train_loss:0.1060141, Train: 1.0000, Val: 0.7000, Test: 0.7200\n",
      "Epoch: 404,train_loss:0.2154869, Train: 1.0000, Val: 0.7020, Test: 0.7140\n",
      "Epoch: 405,train_loss:0.1399968, Train: 1.0000, Val: 0.7040, Test: 0.7210\n",
      "Epoch: 406,train_loss:0.1340131, Train: 1.0000, Val: 0.7080, Test: 0.7280\n",
      "Epoch: 407,train_loss:0.1683271, Train: 1.0000, Val: 0.7100, Test: 0.7380\n",
      "Epoch: 408,train_loss:0.1682758, Train: 1.0000, Val: 0.7160, Test: 0.7250\n",
      "Epoch: 409,train_loss:0.1348050, Train: 1.0000, Val: 0.7040, Test: 0.7220\n",
      "Epoch: 410,train_loss:0.1421907, Train: 1.0000, Val: 0.6960, Test: 0.7120\n",
      "Epoch: 411,train_loss:0.1837848, Train: 1.0000, Val: 0.6920, Test: 0.7070\n",
      "Epoch: 412,train_loss:0.1837994, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch: 413,train_loss:0.0838675, Train: 1.0000, Val: 0.7000, Test: 0.7210\n",
      "Epoch: 414,train_loss:0.1317188, Train: 1.0000, Val: 0.7140, Test: 0.7270\n",
      "Epoch: 415,train_loss:0.1030302, Train: 1.0000, Val: 0.7280, Test: 0.7320\n",
      "Epoch: 416,train_loss:0.0962723, Train: 1.0000, Val: 0.7420, Test: 0.7390\n",
      "Epoch: 417,train_loss:0.1269625, Train: 1.0000, Val: 0.7440, Test: 0.7520\n",
      "Epoch: 418,train_loss:0.1328535, Train: 1.0000, Val: 0.7480, Test: 0.7540\n",
      "Epoch: 419,train_loss:0.0697443, Train: 1.0000, Val: 0.7480, Test: 0.7690\n",
      "Epoch: 420,train_loss:0.0671877, Train: 1.0000, Val: 0.7420, Test: 0.7720\n",
      "Epoch: 421,train_loss:0.0954659, Train: 1.0000, Val: 0.7440, Test: 0.7710\n",
      "Epoch: 422,train_loss:0.1139895, Train: 1.0000, Val: 0.7460, Test: 0.7610\n",
      "Epoch: 423,train_loss:0.1205193, Train: 1.0000, Val: 0.7360, Test: 0.7440\n",
      "Epoch: 424,train_loss:0.1131104, Train: 1.0000, Val: 0.7240, Test: 0.7400\n",
      "Epoch: 425,train_loss:0.1338239, Train: 1.0000, Val: 0.7240, Test: 0.7490\n",
      "Epoch: 426,train_loss:0.1161348, Train: 1.0000, Val: 0.7380, Test: 0.7530\n",
      "Epoch: 427,train_loss:0.1018101, Train: 1.0000, Val: 0.7440, Test: 0.7640\n",
      "Epoch: 428,train_loss:0.1010393, Train: 1.0000, Val: 0.7520, Test: 0.7680\n",
      "Epoch: 429,train_loss:0.0890809, Train: 1.0000, Val: 0.7480, Test: 0.7670\n",
      "Epoch: 430,train_loss:0.1263304, Train: 1.0000, Val: 0.7520, Test: 0.7670\n",
      "Epoch: 431,train_loss:0.1003877, Train: 1.0000, Val: 0.7440, Test: 0.7730\n",
      "Epoch: 432,train_loss:0.0798309, Train: 1.0000, Val: 0.7380, Test: 0.7690\n",
      "Epoch: 433,train_loss:0.1366105, Train: 1.0000, Val: 0.7340, Test: 0.7670\n",
      "Epoch: 434,train_loss:0.1024575, Train: 1.0000, Val: 0.7300, Test: 0.7670\n",
      "Epoch: 435,train_loss:0.1113512, Train: 1.0000, Val: 0.7120, Test: 0.7620\n",
      "Epoch: 436,train_loss:0.1089165, Train: 1.0000, Val: 0.7140, Test: 0.7600\n",
      "Epoch: 437,train_loss:0.0636727, Train: 1.0000, Val: 0.7160, Test: 0.7580\n",
      "Epoch: 438,train_loss:0.1500464, Train: 1.0000, Val: 0.7240, Test: 0.7670\n",
      "Epoch: 439,train_loss:0.0885334, Train: 1.0000, Val: 0.7300, Test: 0.7670\n",
      "Epoch: 440,train_loss:0.0667324, Train: 1.0000, Val: 0.7360, Test: 0.7650\n",
      "Epoch: 441,train_loss:0.1214152, Train: 1.0000, Val: 0.7320, Test: 0.7570\n",
      "Epoch: 442,train_loss:0.0780236, Train: 1.0000, Val: 0.7380, Test: 0.7620\n",
      "Epoch: 443,train_loss:0.0678749, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 444,train_loss:0.1400018, Train: 1.0000, Val: 0.7360, Test: 0.7660\n",
      "Epoch: 445,train_loss:0.0756225, Train: 1.0000, Val: 0.7320, Test: 0.7700\n",
      "Epoch: 446,train_loss:0.0612749, Train: 1.0000, Val: 0.7300, Test: 0.7820\n",
      "Epoch: 447,train_loss:0.1121721, Train: 1.0000, Val: 0.7280, Test: 0.7790\n",
      "Epoch: 448,train_loss:0.1395300, Train: 1.0000, Val: 0.7220, Test: 0.7680\n",
      "Epoch: 449,train_loss:0.0629297, Train: 1.0000, Val: 0.7300, Test: 0.7660\n",
      "Epoch: 450,train_loss:0.0499404, Train: 1.0000, Val: 0.7300, Test: 0.7630\n",
      "Epoch: 451,train_loss:0.0554688, Train: 1.0000, Val: 0.7320, Test: 0.7580\n",
      "Epoch: 452,train_loss:0.1153105, Train: 1.0000, Val: 0.7320, Test: 0.7530\n",
      "Epoch: 453,train_loss:0.0857045, Train: 1.0000, Val: 0.7260, Test: 0.7550\n",
      "Epoch: 454,train_loss:0.1647869, Train: 1.0000, Val: 0.7260, Test: 0.7490\n",
      "Epoch: 455,train_loss:0.1102026, Train: 1.0000, Val: 0.7260, Test: 0.7500\n",
      "Epoch: 456,train_loss:0.0877435, Train: 1.0000, Val: 0.7200, Test: 0.7460\n",
      "Epoch: 457,train_loss:0.1733386, Train: 1.0000, Val: 0.7300, Test: 0.7440\n",
      "Epoch: 458,train_loss:0.1211099, Train: 1.0000, Val: 0.7360, Test: 0.7550\n",
      "Epoch: 459,train_loss:0.1248383, Train: 1.0000, Val: 0.7300, Test: 0.7670\n",
      "Epoch: 460,train_loss:0.0888863, Train: 1.0000, Val: 0.7240, Test: 0.7710\n",
      "Epoch: 461,train_loss:0.1158846, Train: 1.0000, Val: 0.7220, Test: 0.7650\n",
      "Epoch: 462,train_loss:0.0950981, Train: 1.0000, Val: 0.7200, Test: 0.7700\n",
      "Epoch: 463,train_loss:0.0832860, Train: 1.0000, Val: 0.7220, Test: 0.7670\n",
      "Epoch: 464,train_loss:0.0895121, Train: 1.0000, Val: 0.7200, Test: 0.7710\n",
      "Epoch: 465,train_loss:0.1423869, Train: 1.0000, Val: 0.7320, Test: 0.7830\n",
      "Epoch: 466,train_loss:0.0633121, Train: 1.0000, Val: 0.7460, Test: 0.7850\n",
      "Epoch: 467,train_loss:0.0692989, Train: 1.0000, Val: 0.7540, Test: 0.7770\n",
      "Epoch: 468,train_loss:0.0579594, Train: 1.0000, Val: 0.7460, Test: 0.7770\n",
      "Epoch: 469,train_loss:0.0366428, Train: 1.0000, Val: 0.7440, Test: 0.7700\n",
      "Epoch: 470,train_loss:0.0997920, Train: 1.0000, Val: 0.7460, Test: 0.7570\n",
      "Epoch: 471,train_loss:0.1626941, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 472,train_loss:0.1262572, Train: 1.0000, Val: 0.7320, Test: 0.7640\n",
      "Epoch: 473,train_loss:0.1284556, Train: 1.0000, Val: 0.7340, Test: 0.7640\n",
      "Epoch: 474,train_loss:0.0373622, Train: 1.0000, Val: 0.7260, Test: 0.7610\n",
      "Epoch: 475,train_loss:0.0563395, Train: 1.0000, Val: 0.7240, Test: 0.7580\n",
      "Epoch: 476,train_loss:0.1142407, Train: 1.0000, Val: 0.7140, Test: 0.7590\n",
      "Epoch: 477,train_loss:0.0999363, Train: 1.0000, Val: 0.7100, Test: 0.7550\n",
      "Epoch: 478,train_loss:0.0688241, Train: 1.0000, Val: 0.7100, Test: 0.7520\n",
      "Epoch: 479,train_loss:0.0894099, Train: 1.0000, Val: 0.7100, Test: 0.7560\n",
      "Epoch: 480,train_loss:0.1019315, Train: 1.0000, Val: 0.7200, Test: 0.7680\n",
      "Epoch: 481,train_loss:0.1425959, Train: 1.0000, Val: 0.7240, Test: 0.7750\n",
      "Epoch: 482,train_loss:0.1035465, Train: 1.0000, Val: 0.7240, Test: 0.7630\n",
      "Epoch: 483,train_loss:0.1053302, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 484,train_loss:0.0396537, Train: 1.0000, Val: 0.7300, Test: 0.7290\n",
      "Epoch: 485,train_loss:0.1782838, Train: 1.0000, Val: 0.7240, Test: 0.7330\n",
      "Epoch: 486,train_loss:0.0498384, Train: 1.0000, Val: 0.7240, Test: 0.7500\n",
      "Epoch: 487,train_loss:0.1435946, Train: 1.0000, Val: 0.7240, Test: 0.7670\n",
      "Epoch: 488,train_loss:0.0842885, Train: 1.0000, Val: 0.7260, Test: 0.7700\n",
      "Epoch: 489,train_loss:0.0872212, Train: 1.0000, Val: 0.7380, Test: 0.7710\n",
      "Epoch: 490,train_loss:0.0907994, Train: 1.0000, Val: 0.7340, Test: 0.7670\n",
      "Epoch: 491,train_loss:0.0987484, Train: 1.0000, Val: 0.7280, Test: 0.7630\n",
      "Epoch: 492,train_loss:0.1435722, Train: 1.0000, Val: 0.7260, Test: 0.7600\n",
      "Epoch: 493,train_loss:0.1105930, Train: 1.0000, Val: 0.7220, Test: 0.7620\n",
      "Epoch: 494,train_loss:0.1056702, Train: 1.0000, Val: 0.7020, Test: 0.7420\n",
      "Epoch: 495,train_loss:0.0758343, Train: 1.0000, Val: 0.6900, Test: 0.7340\n",
      "Epoch: 496,train_loss:0.0784798, Train: 1.0000, Val: 0.6940, Test: 0.7180\n",
      "Epoch: 497,train_loss:0.1220369, Train: 1.0000, Val: 0.6960, Test: 0.7250\n",
      "Epoch: 498,train_loss:0.0501957, Train: 1.0000, Val: 0.7120, Test: 0.7350\n",
      "Epoch: 499,train_loss:0.0848425, Train: 1.0000, Val: 0.7300, Test: 0.7480\n",
      "Epoch: 500,train_loss:0.0719574, Train: 1.0000, Val: 0.7320, Test: 0.7640\n",
      "Epoch: 501,train_loss:0.0449314, Train: 1.0000, Val: 0.7380, Test: 0.7640\n",
      "Epoch: 502,train_loss:0.1270159, Train: 1.0000, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 503,train_loss:0.0625528, Train: 1.0000, Val: 0.7300, Test: 0.7720\n",
      "Epoch: 504,train_loss:0.0432667, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 505,train_loss:0.0799406, Train: 1.0000, Val: 0.7280, Test: 0.7690\n",
      "Epoch: 506,train_loss:0.0909803, Train: 1.0000, Val: 0.7240, Test: 0.7730\n",
      "Epoch: 507,train_loss:0.0591455, Train: 1.0000, Val: 0.7200, Test: 0.7700\n",
      "Epoch: 508,train_loss:0.0765696, Train: 1.0000, Val: 0.7160, Test: 0.7690\n",
      "Epoch: 509,train_loss:0.1194674, Train: 1.0000, Val: 0.7140, Test: 0.7620\n",
      "Epoch: 510,train_loss:0.0626366, Train: 1.0000, Val: 0.7120, Test: 0.7610\n",
      "Epoch: 511,train_loss:0.1083642, Train: 1.0000, Val: 0.7200, Test: 0.7700\n",
      "Epoch: 512,train_loss:0.1211651, Train: 1.0000, Val: 0.7300, Test: 0.7720\n",
      "Epoch: 513,train_loss:0.0590102, Train: 1.0000, Val: 0.7320, Test: 0.7750\n",
      "Epoch: 514,train_loss:0.0818831, Train: 1.0000, Val: 0.7400, Test: 0.7750\n",
      "Epoch: 515,train_loss:0.1399631, Train: 1.0000, Val: 0.7380, Test: 0.7730\n",
      "Epoch: 516,train_loss:0.1779639, Train: 1.0000, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 517,train_loss:0.0786428, Train: 1.0000, Val: 0.7340, Test: 0.7730\n",
      "Epoch: 518,train_loss:0.0460945, Train: 1.0000, Val: 0.7320, Test: 0.7710\n",
      "Epoch: 519,train_loss:0.0927012, Train: 1.0000, Val: 0.7360, Test: 0.7660\n",
      "Epoch: 520,train_loss:0.0896839, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 521,train_loss:0.0985726, Train: 1.0000, Val: 0.7400, Test: 0.7620\n",
      "Epoch: 522,train_loss:0.0679865, Train: 1.0000, Val: 0.7360, Test: 0.7650\n",
      "Epoch: 523,train_loss:0.0781357, Train: 1.0000, Val: 0.7260, Test: 0.7660\n",
      "Epoch: 524,train_loss:0.0618274, Train: 1.0000, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 525,train_loss:0.0650302, Train: 1.0000, Val: 0.7300, Test: 0.7710\n",
      "Epoch: 526,train_loss:0.0587452, Train: 1.0000, Val: 0.7340, Test: 0.7700\n",
      "Epoch: 527,train_loss:0.0719605, Train: 1.0000, Val: 0.7400, Test: 0.7710\n",
      "Epoch: 528,train_loss:0.0769430, Train: 1.0000, Val: 0.7400, Test: 0.7710\n",
      "Epoch: 529,train_loss:0.0580076, Train: 1.0000, Val: 0.7340, Test: 0.7700\n",
      "Epoch: 530,train_loss:0.1053408, Train: 1.0000, Val: 0.7160, Test: 0.7730\n",
      "Epoch: 531,train_loss:0.0972542, Train: 1.0000, Val: 0.7100, Test: 0.7680\n",
      "Epoch: 532,train_loss:0.0619108, Train: 1.0000, Val: 0.7160, Test: 0.7670\n",
      "Epoch: 533,train_loss:0.0346201, Train: 1.0000, Val: 0.7180, Test: 0.7640\n",
      "Epoch: 534,train_loss:0.0355367, Train: 1.0000, Val: 0.7180, Test: 0.7660\n",
      "Epoch: 535,train_loss:0.0785000, Train: 1.0000, Val: 0.7240, Test: 0.7650\n",
      "Epoch: 536,train_loss:0.0572578, Train: 1.0000, Val: 0.7260, Test: 0.7650\n",
      "Epoch: 537,train_loss:0.0468729, Train: 1.0000, Val: 0.7320, Test: 0.7680\n",
      "Epoch: 538,train_loss:0.1100736, Train: 1.0000, Val: 0.7340, Test: 0.7680\n",
      "Epoch: 539,train_loss:0.0481123, Train: 1.0000, Val: 0.7320, Test: 0.7670\n",
      "Epoch: 540,train_loss:0.1085134, Train: 1.0000, Val: 0.7160, Test: 0.7690\n",
      "Epoch: 541,train_loss:0.0608198, Train: 1.0000, Val: 0.7220, Test: 0.7540\n",
      "Epoch: 542,train_loss:0.0805826, Train: 1.0000, Val: 0.7160, Test: 0.7520\n",
      "Epoch: 543,train_loss:0.0758977, Train: 1.0000, Val: 0.7200, Test: 0.7530\n",
      "Epoch: 544,train_loss:0.0673365, Train: 1.0000, Val: 0.7260, Test: 0.7560\n",
      "Epoch: 545,train_loss:0.1258855, Train: 1.0000, Val: 0.7120, Test: 0.7630\n",
      "Epoch: 546,train_loss:0.0914663, Train: 1.0000, Val: 0.7400, Test: 0.7710\n",
      "Epoch: 547,train_loss:0.1082386, Train: 1.0000, Val: 0.7420, Test: 0.7690\n",
      "Epoch: 548,train_loss:0.0877565, Train: 1.0000, Val: 0.7320, Test: 0.7610\n",
      "Epoch: 549,train_loss:0.1037152, Train: 1.0000, Val: 0.7360, Test: 0.7590\n",
      "Epoch: 550,train_loss:0.0819862, Train: 1.0000, Val: 0.7420, Test: 0.7680\n",
      "Epoch: 551,train_loss:0.1088913, Train: 1.0000, Val: 0.7400, Test: 0.7710\n",
      "Epoch: 552,train_loss:0.0808739, Train: 1.0000, Val: 0.7400, Test: 0.7680\n",
      "Epoch: 553,train_loss:0.0637685, Train: 1.0000, Val: 0.7380, Test: 0.7640\n",
      "Epoch: 554,train_loss:0.0556116, Train: 1.0000, Val: 0.7320, Test: 0.7540\n",
      "Epoch: 555,train_loss:0.1002667, Train: 1.0000, Val: 0.7360, Test: 0.7590\n",
      "Epoch: 556,train_loss:0.0578892, Train: 1.0000, Val: 0.7340, Test: 0.7590\n",
      "Epoch: 557,train_loss:0.0709421, Train: 1.0000, Val: 0.7340, Test: 0.7620\n",
      "Epoch: 558,train_loss:0.0816291, Train: 1.0000, Val: 0.7400, Test: 0.7710\n",
      "Epoch: 559,train_loss:0.0605288, Train: 1.0000, Val: 0.7440, Test: 0.7780\n",
      "Epoch: 560,train_loss:0.0717127, Train: 1.0000, Val: 0.7360, Test: 0.7790\n",
      "Epoch: 561,train_loss:0.0711529, Train: 1.0000, Val: 0.7340, Test: 0.7730\n",
      "Epoch: 562,train_loss:0.0700362, Train: 1.0000, Val: 0.7340, Test: 0.7730\n",
      "Epoch: 563,train_loss:0.1493644, Train: 1.0000, Val: 0.7360, Test: 0.7760\n",
      "Epoch: 564,train_loss:0.0682971, Train: 1.0000, Val: 0.7300, Test: 0.7700\n",
      "Epoch: 565,train_loss:0.0904424, Train: 1.0000, Val: 0.7340, Test: 0.7590\n",
      "Epoch: 566,train_loss:0.1272872, Train: 1.0000, Val: 0.7360, Test: 0.7490\n",
      "Epoch: 567,train_loss:0.0884136, Train: 1.0000, Val: 0.7360, Test: 0.7510\n",
      "Epoch: 568,train_loss:0.1053635, Train: 1.0000, Val: 0.7220, Test: 0.7430\n",
      "Epoch: 569,train_loss:0.0984535, Train: 1.0000, Val: 0.7180, Test: 0.7490\n",
      "Epoch: 570,train_loss:0.0988330, Train: 1.0000, Val: 0.7320, Test: 0.7540\n",
      "Epoch: 571,train_loss:0.0988459, Train: 1.0000, Val: 0.7380, Test: 0.7640\n",
      "Epoch: 572,train_loss:0.0909958, Train: 1.0000, Val: 0.7320, Test: 0.7710\n",
      "Epoch: 573,train_loss:0.1269529, Train: 1.0000, Val: 0.7340, Test: 0.7640\n",
      "Epoch: 574,train_loss:0.1691685, Train: 1.0000, Val: 0.7340, Test: 0.7640\n",
      "Epoch: 575,train_loss:0.0829944, Train: 0.9929, Val: 0.7200, Test: 0.7630\n",
      "Epoch: 576,train_loss:0.1129678, Train: 1.0000, Val: 0.7180, Test: 0.7540\n",
      "Epoch: 577,train_loss:0.0725746, Train: 1.0000, Val: 0.7180, Test: 0.7560\n",
      "Epoch: 578,train_loss:0.1390034, Train: 1.0000, Val: 0.7120, Test: 0.7640\n",
      "Epoch: 579,train_loss:0.0954589, Train: 1.0000, Val: 0.7260, Test: 0.7750\n",
      "Epoch: 580,train_loss:0.0668827, Train: 1.0000, Val: 0.7280, Test: 0.7800\n",
      "Epoch: 581,train_loss:0.0607173, Train: 1.0000, Val: 0.7200, Test: 0.7720\n",
      "Epoch: 582,train_loss:0.0888470, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 583,train_loss:0.1338555, Train: 1.0000, Val: 0.7360, Test: 0.7680\n",
      "Epoch: 584,train_loss:0.0619328, Train: 1.0000, Val: 0.7280, Test: 0.7550\n",
      "Epoch: 585,train_loss:0.1348725, Train: 1.0000, Val: 0.7400, Test: 0.7580\n",
      "Epoch: 586,train_loss:0.0840660, Train: 1.0000, Val: 0.7280, Test: 0.7700\n",
      "Epoch: 587,train_loss:0.1138532, Train: 1.0000, Val: 0.7240, Test: 0.7680\n",
      "Epoch: 588,train_loss:0.0868465, Train: 1.0000, Val: 0.7180, Test: 0.7510\n",
      "Epoch: 589,train_loss:0.1208088, Train: 1.0000, Val: 0.7080, Test: 0.7470\n",
      "Epoch: 590,train_loss:0.0686235, Train: 1.0000, Val: 0.7040, Test: 0.7510\n",
      "Epoch: 591,train_loss:0.0514723, Train: 1.0000, Val: 0.7180, Test: 0.7610\n",
      "Epoch: 592,train_loss:0.1630567, Train: 1.0000, Val: 0.7320, Test: 0.7560\n",
      "Epoch: 593,train_loss:0.0611835, Train: 1.0000, Val: 0.7440, Test: 0.7580\n",
      "Epoch: 594,train_loss:0.1209524, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
      "Epoch: 595,train_loss:0.1366892, Train: 1.0000, Val: 0.7500, Test: 0.7660\n",
      "Epoch: 596,train_loss:0.0653040, Train: 1.0000, Val: 0.7460, Test: 0.7630\n",
      "Epoch: 597,train_loss:0.1905225, Train: 1.0000, Val: 0.7240, Test: 0.7520\n",
      "Epoch: 598,train_loss:0.1326428, Train: 1.0000, Val: 0.7100, Test: 0.7300\n",
      "Epoch: 599,train_loss:0.1292084, Train: 1.0000, Val: 0.7120, Test: 0.7420\n",
      "Epoch: 600,train_loss:0.1137263, Train: 1.0000, Val: 0.7260, Test: 0.7590\n",
      "Epoch: 601,train_loss:0.1248126, Train: 1.0000, Val: 0.7220, Test: 0.7530\n",
      "Epoch: 602,train_loss:0.1118817, Train: 1.0000, Val: 0.7100, Test: 0.7440\n",
      "Epoch: 603,train_loss:0.1896685, Train: 1.0000, Val: 0.7380, Test: 0.7360\n",
      "Epoch: 604,train_loss:0.1220652, Train: 1.0000, Val: 0.7380, Test: 0.7480\n",
      "Epoch: 605,train_loss:0.0792152, Train: 1.0000, Val: 0.7420, Test: 0.7520\n",
      "Epoch: 606,train_loss:0.1115686, Train: 1.0000, Val: 0.7340, Test: 0.7580\n",
      "Epoch: 607,train_loss:0.0804619, Train: 1.0000, Val: 0.7420, Test: 0.7640\n",
      "Epoch: 608,train_loss:0.1212368, Train: 1.0000, Val: 0.7360, Test: 0.7630\n",
      "Epoch: 609,train_loss:0.0961122, Train: 1.0000, Val: 0.7320, Test: 0.7580\n",
      "Epoch: 610,train_loss:0.1356722, Train: 1.0000, Val: 0.7300, Test: 0.7540\n",
      "Epoch: 611,train_loss:0.1505619, Train: 1.0000, Val: 0.7160, Test: 0.7540\n",
      "Epoch: 612,train_loss:0.1004080, Train: 1.0000, Val: 0.7240, Test: 0.7650\n",
      "Epoch: 613,train_loss:0.1830357, Train: 1.0000, Val: 0.7400, Test: 0.7550\n",
      "Epoch: 614,train_loss:0.1231127, Train: 1.0000, Val: 0.7440, Test: 0.7640\n",
      "Epoch: 615,train_loss:0.1116841, Train: 1.0000, Val: 0.7320, Test: 0.7400\n",
      "Epoch: 616,train_loss:0.0791550, Train: 1.0000, Val: 0.7300, Test: 0.7280\n",
      "Epoch: 617,train_loss:0.1324866, Train: 1.0000, Val: 0.7340, Test: 0.7450\n",
      "Epoch: 618,train_loss:0.0898069, Train: 1.0000, Val: 0.7500, Test: 0.7620\n",
      "Epoch: 619,train_loss:0.0981879, Train: 1.0000, Val: 0.7460, Test: 0.7660\n",
      "Epoch: 620,train_loss:0.1049642, Train: 1.0000, Val: 0.7500, Test: 0.7610\n",
      "Epoch: 621,train_loss:0.1312525, Train: 1.0000, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 622,train_loss:0.1708261, Train: 1.0000, Val: 0.7340, Test: 0.7580\n",
      "Epoch: 623,train_loss:0.1128096, Train: 1.0000, Val: 0.7400, Test: 0.7650\n",
      "Epoch: 624,train_loss:0.0931758, Train: 1.0000, Val: 0.7400, Test: 0.7590\n",
      "Epoch: 625,train_loss:0.0658106, Train: 1.0000, Val: 0.7320, Test: 0.7540\n",
      "Epoch: 626,train_loss:0.0439348, Train: 1.0000, Val: 0.7280, Test: 0.7560\n",
      "Epoch: 627,train_loss:0.1625894, Train: 1.0000, Val: 0.7340, Test: 0.7600\n",
      "Epoch: 628,train_loss:0.1299654, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 629,train_loss:0.1629466, Train: 1.0000, Val: 0.7280, Test: 0.7650\n",
      "Epoch: 630,train_loss:0.0813546, Train: 1.0000, Val: 0.7300, Test: 0.7640\n",
      "Epoch: 631,train_loss:0.1274769, Train: 1.0000, Val: 0.7340, Test: 0.7600\n",
      "Epoch: 632,train_loss:0.0940511, Train: 1.0000, Val: 0.7320, Test: 0.7590\n",
      "Epoch: 633,train_loss:0.0544362, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 634,train_loss:0.0767640, Train: 1.0000, Val: 0.7400, Test: 0.7600\n",
      "Epoch: 635,train_loss:0.0837743, Train: 1.0000, Val: 0.7300, Test: 0.7650\n",
      "Epoch: 636,train_loss:0.1145088, Train: 1.0000, Val: 0.7280, Test: 0.7640\n",
      "Epoch: 637,train_loss:0.1162109, Train: 1.0000, Val: 0.7220, Test: 0.7650\n",
      "Epoch: 638,train_loss:0.0366493, Train: 1.0000, Val: 0.7200, Test: 0.7660\n",
      "Epoch: 639,train_loss:0.0736467, Train: 1.0000, Val: 0.7180, Test: 0.7650\n",
      "Epoch: 640,train_loss:0.0515069, Train: 1.0000, Val: 0.7260, Test: 0.7540\n",
      "Epoch: 641,train_loss:0.0953774, Train: 1.0000, Val: 0.7240, Test: 0.7460\n",
      "Epoch: 642,train_loss:0.1152667, Train: 1.0000, Val: 0.7220, Test: 0.7430\n",
      "Epoch: 643,train_loss:0.0549821, Train: 1.0000, Val: 0.7340, Test: 0.7400\n",
      "Epoch: 644,train_loss:0.1435136, Train: 1.0000, Val: 0.7380, Test: 0.7620\n",
      "Epoch: 645,train_loss:0.0782822, Train: 1.0000, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 646,train_loss:0.1133796, Train: 1.0000, Val: 0.7460, Test: 0.7710\n",
      "Epoch: 647,train_loss:0.0797019, Train: 1.0000, Val: 0.7440, Test: 0.7640\n",
      "Epoch: 648,train_loss:0.1031834, Train: 1.0000, Val: 0.7420, Test: 0.7630\n",
      "Epoch: 649,train_loss:0.0513399, Train: 1.0000, Val: 0.7400, Test: 0.7730\n",
      "Epoch: 650,train_loss:0.1680318, Train: 1.0000, Val: 0.7560, Test: 0.7840\n",
      "Epoch: 651,train_loss:0.0538950, Train: 1.0000, Val: 0.7400, Test: 0.7870\n",
      "Epoch: 652,train_loss:0.0483980, Train: 1.0000, Val: 0.7340, Test: 0.7800\n",
      "Epoch: 653,train_loss:0.0870510, Train: 1.0000, Val: 0.7280, Test: 0.7730\n",
      "Epoch: 654,train_loss:0.0835674, Train: 1.0000, Val: 0.7240, Test: 0.7720\n",
      "Epoch: 655,train_loss:0.0887556, Train: 1.0000, Val: 0.7300, Test: 0.7810\n",
      "Epoch: 656,train_loss:0.0358252, Train: 1.0000, Val: 0.7300, Test: 0.7780\n",
      "Epoch: 657,train_loss:0.0943664, Train: 1.0000, Val: 0.7220, Test: 0.7780\n",
      "Epoch: 658,train_loss:0.0984477, Train: 1.0000, Val: 0.7260, Test: 0.7760\n",
      "Epoch: 659,train_loss:0.1268216, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 660,train_loss:0.1049986, Train: 1.0000, Val: 0.7260, Test: 0.7740\n",
      "Epoch: 661,train_loss:0.0754601, Train: 1.0000, Val: 0.7340, Test: 0.7760\n",
      "Epoch: 662,train_loss:0.0874741, Train: 1.0000, Val: 0.7340, Test: 0.7750\n",
      "Epoch: 663,train_loss:0.0939093, Train: 1.0000, Val: 0.7300, Test: 0.7680\n",
      "Epoch: 664,train_loss:0.1217249, Train: 1.0000, Val: 0.7340, Test: 0.7690\n",
      "Epoch: 665,train_loss:0.0587569, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 666,train_loss:0.1278082, Train: 1.0000, Val: 0.7440, Test: 0.7760\n",
      "Epoch: 667,train_loss:0.0632247, Train: 1.0000, Val: 0.7400, Test: 0.7750\n",
      "Epoch: 668,train_loss:0.0462903, Train: 1.0000, Val: 0.7380, Test: 0.7710\n",
      "Epoch: 669,train_loss:0.0878093, Train: 1.0000, Val: 0.7360, Test: 0.7680\n",
      "Epoch: 670,train_loss:0.1363454, Train: 1.0000, Val: 0.7240, Test: 0.7650\n",
      "Epoch: 671,train_loss:0.1291443, Train: 1.0000, Val: 0.7260, Test: 0.7640\n",
      "Epoch: 672,train_loss:0.0866308, Train: 1.0000, Val: 0.7220, Test: 0.7610\n",
      "Epoch: 673,train_loss:0.0548567, Train: 1.0000, Val: 0.7180, Test: 0.7510\n",
      "Epoch: 674,train_loss:0.0687648, Train: 1.0000, Val: 0.7180, Test: 0.7440\n",
      "Epoch: 675,train_loss:0.0762939, Train: 1.0000, Val: 0.7220, Test: 0.7520\n",
      "Epoch: 676,train_loss:0.0711815, Train: 1.0000, Val: 0.7240, Test: 0.7560\n",
      "Epoch: 677,train_loss:0.0418008, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 678,train_loss:0.0445256, Train: 1.0000, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 679,train_loss:0.0390660, Train: 1.0000, Val: 0.7480, Test: 0.7730\n",
      "Epoch: 680,train_loss:0.1170451, Train: 1.0000, Val: 0.7460, Test: 0.7770\n",
      "Epoch: 681,train_loss:0.1098110, Train: 1.0000, Val: 0.7260, Test: 0.7740\n",
      "Epoch: 682,train_loss:0.0970445, Train: 1.0000, Val: 0.7200, Test: 0.7580\n",
      "Epoch: 683,train_loss:0.0589654, Train: 1.0000, Val: 0.7200, Test: 0.7480\n",
      "Epoch: 684,train_loss:0.0904639, Train: 1.0000, Val: 0.7260, Test: 0.7470\n",
      "Epoch: 685,train_loss:0.1012161, Train: 1.0000, Val: 0.7240, Test: 0.7570\n",
      "Epoch: 686,train_loss:0.0468208, Train: 1.0000, Val: 0.7400, Test: 0.7650\n",
      "Epoch: 687,train_loss:0.0799007, Train: 1.0000, Val: 0.7500, Test: 0.7690\n",
      "Epoch: 688,train_loss:0.0497142, Train: 1.0000, Val: 0.7460, Test: 0.7690\n",
      "Epoch: 689,train_loss:0.0740774, Train: 1.0000, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 690,train_loss:0.1036007, Train: 1.0000, Val: 0.7460, Test: 0.7680\n",
      "Epoch: 691,train_loss:0.0670429, Train: 1.0000, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 692,train_loss:0.0812471, Train: 1.0000, Val: 0.7340, Test: 0.7730\n",
      "Epoch: 693,train_loss:0.0378101, Train: 1.0000, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 694,train_loss:0.0769146, Train: 1.0000, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 695,train_loss:0.0791362, Train: 1.0000, Val: 0.7300, Test: 0.7700\n",
      "Epoch: 696,train_loss:0.0584300, Train: 1.0000, Val: 0.7280, Test: 0.7650\n",
      "Epoch: 697,train_loss:0.1005277, Train: 1.0000, Val: 0.7220, Test: 0.7630\n",
      "Epoch: 698,train_loss:0.0965380, Train: 1.0000, Val: 0.7200, Test: 0.7630\n",
      "Epoch: 699,train_loss:0.0508485, Train: 1.0000, Val: 0.7200, Test: 0.7620\n",
      "Epoch: 700,train_loss:0.0733675, Train: 1.0000, Val: 0.7200, Test: 0.7620\n",
      "Epoch: 701,train_loss:0.0515083, Train: 1.0000, Val: 0.7280, Test: 0.7610\n",
      "Epoch: 702,train_loss:0.0439408, Train: 1.0000, Val: 0.7320, Test: 0.7630\n",
      "Epoch: 703,train_loss:0.0713256, Train: 1.0000, Val: 0.7260, Test: 0.7610\n",
      "Epoch: 704,train_loss:0.0574999, Train: 1.0000, Val: 0.7240, Test: 0.7590\n",
      "Epoch: 705,train_loss:0.1480541, Train: 1.0000, Val: 0.7240, Test: 0.7670\n",
      "Epoch: 706,train_loss:0.0958904, Train: 1.0000, Val: 0.7300, Test: 0.7560\n",
      "Epoch: 707,train_loss:0.1052041, Train: 1.0000, Val: 0.7320, Test: 0.7530\n",
      "Epoch: 708,train_loss:0.0631682, Train: 1.0000, Val: 0.7280, Test: 0.7490\n",
      "Epoch: 709,train_loss:0.1523616, Train: 1.0000, Val: 0.7280, Test: 0.7590\n",
      "Epoch: 710,train_loss:0.0571393, Train: 1.0000, Val: 0.7220, Test: 0.7670\n",
      "Epoch: 711,train_loss:0.0435133, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 712,train_loss:0.0683770, Train: 1.0000, Val: 0.7260, Test: 0.7600\n",
      "Epoch: 713,train_loss:0.0615273, Train: 1.0000, Val: 0.7360, Test: 0.7580\n",
      "Epoch: 714,train_loss:0.0600604, Train: 1.0000, Val: 0.7360, Test: 0.7580\n",
      "Epoch: 715,train_loss:0.0932250, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 716,train_loss:0.0830461, Train: 1.0000, Val: 0.7400, Test: 0.7700\n",
      "Epoch: 717,train_loss:0.0655544, Train: 1.0000, Val: 0.7280, Test: 0.7730\n",
      "Epoch: 718,train_loss:0.0625864, Train: 1.0000, Val: 0.7220, Test: 0.7700\n",
      "Epoch: 719,train_loss:0.0956530, Train: 1.0000, Val: 0.7260, Test: 0.7710\n",
      "Epoch: 720,train_loss:0.0778549, Train: 1.0000, Val: 0.7260, Test: 0.7680\n",
      "Epoch: 721,train_loss:0.1136336, Train: 1.0000, Val: 0.7180, Test: 0.7650\n",
      "Epoch: 722,train_loss:0.0645815, Train: 1.0000, Val: 0.7120, Test: 0.7600\n",
      "Epoch: 723,train_loss:0.1401796, Train: 1.0000, Val: 0.7140, Test: 0.7650\n",
      "Epoch: 724,train_loss:0.0615854, Train: 1.0000, Val: 0.7160, Test: 0.7660\n",
      "Epoch: 725,train_loss:0.0773926, Train: 1.0000, Val: 0.7360, Test: 0.7670\n",
      "Epoch: 726,train_loss:0.0719533, Train: 1.0000, Val: 0.7340, Test: 0.7750\n",
      "Epoch: 727,train_loss:0.0952827, Train: 1.0000, Val: 0.7320, Test: 0.7780\n",
      "Epoch: 728,train_loss:0.0642238, Train: 1.0000, Val: 0.7320, Test: 0.7720\n",
      "Epoch: 729,train_loss:0.0576308, Train: 1.0000, Val: 0.7160, Test: 0.7680\n",
      "Epoch: 730,train_loss:0.1397282, Train: 1.0000, Val: 0.7200, Test: 0.7610\n",
      "Epoch: 731,train_loss:0.1040096, Train: 1.0000, Val: 0.7180, Test: 0.7600\n",
      "Epoch: 732,train_loss:0.0381908, Train: 1.0000, Val: 0.7160, Test: 0.7610\n",
      "Epoch: 733,train_loss:0.1521885, Train: 1.0000, Val: 0.7100, Test: 0.7560\n",
      "Epoch: 734,train_loss:0.1019162, Train: 1.0000, Val: 0.7120, Test: 0.7450\n",
      "Epoch: 735,train_loss:0.1180440, Train: 1.0000, Val: 0.6980, Test: 0.7350\n",
      "Epoch: 736,train_loss:0.1389799, Train: 1.0000, Val: 0.6980, Test: 0.7250\n",
      "Epoch: 737,train_loss:0.0999472, Train: 1.0000, Val: 0.7160, Test: 0.7340\n",
      "Epoch: 738,train_loss:0.0956468, Train: 1.0000, Val: 0.7080, Test: 0.7470\n",
      "Epoch: 739,train_loss:0.0292565, Train: 1.0000, Val: 0.7000, Test: 0.7420\n",
      "Epoch: 740,train_loss:0.0681972, Train: 1.0000, Val: 0.6960, Test: 0.7430\n",
      "Epoch: 741,train_loss:0.0496940, Train: 1.0000, Val: 0.7080, Test: 0.7450\n",
      "Epoch: 742,train_loss:0.1359184, Train: 1.0000, Val: 0.7120, Test: 0.7560\n",
      "Epoch: 743,train_loss:0.0689633, Train: 1.0000, Val: 0.7200, Test: 0.7550\n",
      "Epoch: 744,train_loss:0.0767082, Train: 1.0000, Val: 0.7200, Test: 0.7500\n",
      "Epoch: 745,train_loss:0.1155756, Train: 1.0000, Val: 0.7300, Test: 0.7640\n",
      "Epoch: 746,train_loss:0.0627125, Train: 1.0000, Val: 0.7280, Test: 0.7600\n",
      "Epoch: 747,train_loss:0.1340250, Train: 1.0000, Val: 0.7360, Test: 0.7660\n",
      "Epoch: 748,train_loss:0.1132238, Train: 1.0000, Val: 0.7340, Test: 0.7630\n",
      "Epoch: 749,train_loss:0.0993670, Train: 1.0000, Val: 0.7320, Test: 0.7550\n",
      "Epoch: 750,train_loss:0.1086069, Train: 1.0000, Val: 0.7220, Test: 0.7520\n",
      "Epoch: 751,train_loss:0.1037364, Train: 1.0000, Val: 0.7260, Test: 0.7530\n",
      "Epoch: 752,train_loss:0.0777412, Train: 1.0000, Val: 0.7260, Test: 0.7540\n",
      "Epoch: 753,train_loss:0.0614254, Train: 1.0000, Val: 0.7200, Test: 0.7570\n",
      "Epoch: 754,train_loss:0.0697861, Train: 1.0000, Val: 0.7180, Test: 0.7570\n",
      "Epoch: 755,train_loss:0.0715902, Train: 1.0000, Val: 0.7260, Test: 0.7640\n",
      "Epoch: 756,train_loss:0.0995398, Train: 1.0000, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 757,train_loss:0.1076768, Train: 1.0000, Val: 0.7400, Test: 0.7600\n",
      "Epoch: 758,train_loss:0.1244313, Train: 1.0000, Val: 0.7320, Test: 0.7610\n",
      "Epoch: 759,train_loss:0.1364651, Train: 1.0000, Val: 0.7260, Test: 0.7540\n",
      "Epoch: 760,train_loss:0.1124313, Train: 1.0000, Val: 0.7120, Test: 0.7270\n",
      "Epoch: 761,train_loss:0.1798522, Train: 1.0000, Val: 0.7060, Test: 0.7170\n",
      "Epoch: 762,train_loss:0.1031778, Train: 1.0000, Val: 0.7020, Test: 0.7170\n",
      "Epoch: 763,train_loss:0.0729490, Train: 1.0000, Val: 0.7060, Test: 0.7230\n",
      "Epoch: 764,train_loss:0.0783033, Train: 1.0000, Val: 0.6980, Test: 0.7340\n",
      "Epoch: 765,train_loss:0.0614340, Train: 1.0000, Val: 0.7180, Test: 0.7480\n",
      "Epoch: 766,train_loss:0.0837805, Train: 1.0000, Val: 0.7240, Test: 0.7590\n",
      "Epoch: 767,train_loss:0.0835633, Train: 1.0000, Val: 0.7440, Test: 0.7630\n",
      "Epoch: 768,train_loss:0.0942928, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 769,train_loss:0.0503933, Train: 1.0000, Val: 0.7200, Test: 0.7520\n",
      "Epoch: 770,train_loss:0.1311897, Train: 1.0000, Val: 0.7180, Test: 0.7490\n",
      "Epoch: 771,train_loss:0.0890659, Train: 1.0000, Val: 0.7160, Test: 0.7340\n",
      "Epoch: 772,train_loss:0.1194944, Train: 1.0000, Val: 0.7140, Test: 0.7400\n",
      "Epoch: 773,train_loss:0.0622155, Train: 1.0000, Val: 0.7200, Test: 0.7490\n",
      "Epoch: 774,train_loss:0.0740294, Train: 1.0000, Val: 0.7120, Test: 0.7610\n",
      "Epoch: 775,train_loss:0.0598604, Train: 1.0000, Val: 0.7140, Test: 0.7630\n",
      "Epoch: 776,train_loss:0.0717190, Train: 1.0000, Val: 0.7180, Test: 0.7630\n",
      "Epoch: 777,train_loss:0.1257557, Train: 1.0000, Val: 0.7160, Test: 0.7480\n",
      "Epoch: 778,train_loss:0.0558581, Train: 1.0000, Val: 0.7100, Test: 0.7250\n",
      "Epoch: 779,train_loss:0.1219776, Train: 1.0000, Val: 0.7120, Test: 0.7180\n",
      "Epoch: 780,train_loss:0.1285033, Train: 1.0000, Val: 0.7240, Test: 0.7480\n",
      "Epoch: 781,train_loss:0.0513560, Train: 1.0000, Val: 0.7240, Test: 0.7530\n",
      "Epoch: 782,train_loss:0.1131858, Train: 1.0000, Val: 0.7400, Test: 0.7570\n",
      "Epoch: 783,train_loss:0.0780538, Train: 0.9929, Val: 0.7300, Test: 0.7530\n",
      "Epoch: 784,train_loss:0.1099065, Train: 0.9929, Val: 0.7260, Test: 0.7510\n",
      "Epoch: 785,train_loss:0.1205235, Train: 1.0000, Val: 0.7240, Test: 0.7600\n",
      "Epoch: 786,train_loss:0.1049161, Train: 1.0000, Val: 0.7280, Test: 0.7570\n",
      "Epoch: 787,train_loss:0.0513840, Train: 1.0000, Val: 0.7240, Test: 0.7470\n",
      "Epoch: 788,train_loss:0.0516527, Train: 1.0000, Val: 0.7120, Test: 0.7360\n",
      "Epoch: 789,train_loss:0.1012409, Train: 1.0000, Val: 0.7140, Test: 0.7370\n",
      "Epoch: 790,train_loss:0.0389516, Train: 1.0000, Val: 0.7260, Test: 0.7450\n",
      "Epoch: 791,train_loss:0.2260139, Train: 1.0000, Val: 0.7200, Test: 0.7580\n",
      "Epoch: 792,train_loss:0.0724776, Train: 1.0000, Val: 0.7280, Test: 0.7680\n",
      "Epoch: 793,train_loss:0.1298091, Train: 1.0000, Val: 0.7160, Test: 0.7590\n",
      "Epoch: 794,train_loss:0.1169676, Train: 1.0000, Val: 0.6960, Test: 0.7540\n",
      "Epoch: 795,train_loss:0.1937395, Train: 1.0000, Val: 0.7100, Test: 0.7700\n",
      "Epoch: 796,train_loss:0.1400928, Train: 1.0000, Val: 0.7260, Test: 0.7690\n",
      "Epoch: 797,train_loss:0.1017291, Train: 1.0000, Val: 0.7220, Test: 0.7590\n",
      "Epoch: 798,train_loss:0.0988642, Train: 1.0000, Val: 0.7280, Test: 0.7430\n",
      "Epoch: 799,train_loss:0.0945644, Train: 1.0000, Val: 0.7120, Test: 0.7210\n",
      "Epoch: 800,train_loss:0.1039800, Train: 1.0000, Val: 0.7040, Test: 0.7200\n",
      "Epoch: 801,train_loss:0.0388871, Train: 1.0000, Val: 0.7140, Test: 0.7280\n",
      "Epoch: 802,train_loss:0.0861184, Train: 1.0000, Val: 0.7060, Test: 0.7280\n",
      "Epoch: 803,train_loss:0.0739919, Train: 1.0000, Val: 0.7020, Test: 0.7270\n",
      "Epoch: 804,train_loss:0.0659828, Train: 1.0000, Val: 0.7000, Test: 0.7270\n",
      "Epoch: 805,train_loss:0.1143305, Train: 1.0000, Val: 0.7020, Test: 0.7300\n",
      "Epoch: 806,train_loss:0.0971474, Train: 1.0000, Val: 0.7160, Test: 0.7550\n",
      "Epoch: 807,train_loss:0.0989563, Train: 1.0000, Val: 0.7200, Test: 0.7630\n",
      "Epoch: 808,train_loss:0.1127755, Train: 1.0000, Val: 0.7220, Test: 0.7600\n",
      "Epoch: 809,train_loss:0.1056867, Train: 1.0000, Val: 0.7220, Test: 0.7580\n",
      "Epoch: 810,train_loss:0.1615032, Train: 1.0000, Val: 0.7280, Test: 0.7590\n",
      "Epoch: 811,train_loss:0.0492666, Train: 1.0000, Val: 0.7140, Test: 0.7730\n",
      "Epoch: 812,train_loss:0.1044293, Train: 1.0000, Val: 0.7180, Test: 0.7610\n",
      "Epoch: 813,train_loss:0.0660279, Train: 1.0000, Val: 0.7040, Test: 0.7370\n",
      "Epoch: 814,train_loss:0.1326507, Train: 0.9929, Val: 0.6940, Test: 0.7260\n",
      "Epoch: 815,train_loss:0.0734446, Train: 1.0000, Val: 0.7040, Test: 0.7270\n",
      "Epoch: 816,train_loss:0.1074102, Train: 1.0000, Val: 0.7220, Test: 0.7550\n",
      "Epoch: 817,train_loss:0.1368784, Train: 1.0000, Val: 0.7260, Test: 0.7650\n",
      "Epoch: 818,train_loss:0.0881353, Train: 1.0000, Val: 0.7300, Test: 0.7590\n",
      "Epoch: 819,train_loss:0.0980685, Train: 1.0000, Val: 0.7340, Test: 0.7650\n",
      "Epoch: 820,train_loss:0.1496302, Train: 1.0000, Val: 0.7240, Test: 0.7670\n",
      "Epoch: 821,train_loss:0.1045382, Train: 1.0000, Val: 0.7220, Test: 0.7650\n",
      "Epoch: 822,train_loss:0.1003426, Train: 1.0000, Val: 0.7220, Test: 0.7540\n",
      "Epoch: 823,train_loss:0.0555570, Train: 1.0000, Val: 0.7100, Test: 0.7320\n",
      "Epoch: 824,train_loss:0.0384206, Train: 1.0000, Val: 0.7140, Test: 0.7270\n",
      "Epoch: 825,train_loss:0.1084753, Train: 1.0000, Val: 0.7100, Test: 0.7210\n",
      "Epoch: 826,train_loss:0.1128203, Train: 1.0000, Val: 0.7120, Test: 0.7300\n",
      "Epoch: 827,train_loss:0.0704265, Train: 1.0000, Val: 0.7300, Test: 0.7450\n",
      "Epoch: 828,train_loss:0.0811059, Train: 1.0000, Val: 0.7200, Test: 0.7630\n",
      "Epoch: 829,train_loss:0.0430237, Train: 1.0000, Val: 0.7220, Test: 0.7610\n",
      "Epoch: 830,train_loss:0.0865324, Train: 1.0000, Val: 0.7220, Test: 0.7560\n",
      "Epoch: 831,train_loss:0.0829372, Train: 1.0000, Val: 0.7220, Test: 0.7580\n",
      "Epoch: 832,train_loss:0.1311407, Train: 1.0000, Val: 0.7200, Test: 0.7570\n",
      "Epoch: 833,train_loss:0.1277869, Train: 1.0000, Val: 0.7200, Test: 0.7600\n",
      "Epoch: 834,train_loss:0.0444403, Train: 1.0000, Val: 0.7180, Test: 0.7600\n",
      "Epoch: 835,train_loss:0.0762313, Train: 1.0000, Val: 0.7180, Test: 0.7590\n",
      "Epoch: 836,train_loss:0.0905176, Train: 1.0000, Val: 0.7240, Test: 0.7550\n",
      "Epoch: 837,train_loss:0.0925872, Train: 1.0000, Val: 0.7320, Test: 0.7510\n",
      "Epoch: 838,train_loss:0.0887544, Train: 1.0000, Val: 0.7280, Test: 0.7390\n",
      "Epoch: 839,train_loss:0.1502135, Train: 1.0000, Val: 0.7240, Test: 0.7420\n",
      "Epoch: 840,train_loss:0.0941516, Train: 1.0000, Val: 0.7300, Test: 0.7580\n",
      "Epoch: 841,train_loss:0.0439671, Train: 1.0000, Val: 0.7300, Test: 0.7650\n",
      "Epoch: 842,train_loss:0.0630649, Train: 1.0000, Val: 0.7340, Test: 0.7640\n",
      "Epoch: 843,train_loss:0.0639284, Train: 1.0000, Val: 0.7320, Test: 0.7610\n",
      "Epoch: 844,train_loss:0.0635662, Train: 1.0000, Val: 0.7320, Test: 0.7580\n",
      "Epoch: 845,train_loss:0.0858505, Train: 1.0000, Val: 0.7320, Test: 0.7620\n",
      "Epoch: 846,train_loss:0.0492016, Train: 1.0000, Val: 0.7240, Test: 0.7570\n",
      "Epoch: 847,train_loss:0.0464954, Train: 1.0000, Val: 0.7280, Test: 0.7660\n",
      "Epoch: 848,train_loss:0.1030027, Train: 1.0000, Val: 0.7200, Test: 0.7520\n",
      "Epoch: 849,train_loss:0.0605645, Train: 0.9929, Val: 0.7020, Test: 0.7130\n",
      "Epoch: 850,train_loss:0.1095695, Train: 0.9929, Val: 0.6800, Test: 0.7070\n",
      "Epoch: 851,train_loss:0.1014935, Train: 1.0000, Val: 0.6940, Test: 0.7220\n",
      "Epoch: 852,train_loss:0.0477830, Train: 1.0000, Val: 0.7040, Test: 0.7500\n",
      "Epoch: 853,train_loss:0.0618136, Train: 1.0000, Val: 0.7100, Test: 0.7610\n",
      "Epoch: 854,train_loss:0.1069973, Train: 1.0000, Val: 0.7180, Test: 0.7660\n",
      "Epoch: 855,train_loss:0.1195465, Train: 1.0000, Val: 0.7300, Test: 0.7570\n",
      "Epoch: 856,train_loss:0.0976057, Train: 1.0000, Val: 0.7300, Test: 0.7490\n",
      "Epoch: 857,train_loss:0.0242486, Train: 1.0000, Val: 0.7120, Test: 0.7300\n",
      "Epoch: 858,train_loss:0.1883686, Train: 1.0000, Val: 0.7160, Test: 0.7570\n",
      "Epoch: 859,train_loss:0.0481976, Train: 1.0000, Val: 0.7200, Test: 0.7520\n",
      "Epoch: 860,train_loss:0.0740207, Train: 1.0000, Val: 0.7080, Test: 0.7380\n",
      "Epoch: 861,train_loss:0.0656451, Train: 1.0000, Val: 0.7100, Test: 0.7430\n",
      "Epoch: 862,train_loss:0.1479976, Train: 1.0000, Val: 0.7260, Test: 0.7520\n",
      "Epoch: 863,train_loss:0.0508515, Train: 1.0000, Val: 0.7300, Test: 0.7640\n",
      "Epoch: 864,train_loss:0.0851798, Train: 1.0000, Val: 0.7280, Test: 0.7700\n",
      "Epoch: 865,train_loss:0.1435176, Train: 1.0000, Val: 0.7320, Test: 0.7430\n",
      "Epoch: 866,train_loss:0.0950846, Train: 1.0000, Val: 0.6920, Test: 0.7170\n",
      "Epoch: 867,train_loss:0.0781687, Train: 1.0000, Val: 0.6680, Test: 0.6860\n",
      "Epoch: 868,train_loss:0.1218936, Train: 1.0000, Val: 0.6680, Test: 0.6900\n",
      "Epoch: 869,train_loss:0.0954255, Train: 1.0000, Val: 0.7000, Test: 0.7320\n",
      "Epoch: 870,train_loss:0.0786609, Train: 1.0000, Val: 0.7240, Test: 0.7480\n",
      "Epoch: 871,train_loss:0.0380696, Train: 1.0000, Val: 0.7200, Test: 0.7560\n",
      "Epoch: 872,train_loss:0.1052701, Train: 1.0000, Val: 0.7100, Test: 0.7640\n",
      "Epoch: 873,train_loss:0.1040702, Train: 1.0000, Val: 0.7200, Test: 0.7720\n",
      "Epoch: 874,train_loss:0.1067930, Train: 1.0000, Val: 0.7140, Test: 0.7730\n",
      "Epoch: 875,train_loss:0.0568255, Train: 1.0000, Val: 0.7180, Test: 0.7720\n",
      "Epoch: 876,train_loss:0.0903022, Train: 1.0000, Val: 0.7180, Test: 0.7610\n",
      "Epoch: 877,train_loss:0.0711684, Train: 1.0000, Val: 0.7240, Test: 0.7580\n",
      "Epoch: 878,train_loss:0.1134383, Train: 1.0000, Val: 0.7260, Test: 0.7700\n",
      "Epoch: 879,train_loss:0.0504773, Train: 1.0000, Val: 0.7420, Test: 0.7730\n",
      "Epoch: 880,train_loss:0.0859216, Train: 1.0000, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 881,train_loss:0.0588046, Train: 1.0000, Val: 0.7420, Test: 0.7760\n",
      "Epoch: 882,train_loss:0.1344758, Train: 1.0000, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 883,train_loss:0.0560069, Train: 1.0000, Val: 0.7260, Test: 0.7710\n",
      "Epoch: 884,train_loss:0.1118422, Train: 1.0000, Val: 0.7280, Test: 0.7720\n",
      "Epoch: 885,train_loss:0.1021247, Train: 1.0000, Val: 0.7240, Test: 0.7750\n",
      "Epoch: 886,train_loss:0.0978124, Train: 1.0000, Val: 0.7340, Test: 0.7790\n",
      "Epoch: 887,train_loss:0.0512865, Train: 1.0000, Val: 0.7200, Test: 0.7700\n",
      "Epoch: 888,train_loss:0.0322246, Train: 1.0000, Val: 0.7080, Test: 0.7660\n",
      "Epoch: 889,train_loss:0.0476533, Train: 1.0000, Val: 0.7060, Test: 0.7630\n",
      "Epoch: 890,train_loss:0.0774516, Train: 1.0000, Val: 0.7020, Test: 0.7590\n",
      "Epoch: 891,train_loss:0.0903310, Train: 1.0000, Val: 0.7120, Test: 0.7570\n",
      "Epoch: 892,train_loss:0.1484879, Train: 1.0000, Val: 0.7240, Test: 0.7540\n",
      "Epoch: 893,train_loss:0.1034859, Train: 1.0000, Val: 0.7240, Test: 0.7560\n",
      "Epoch: 894,train_loss:0.1264187, Train: 1.0000, Val: 0.7240, Test: 0.7640\n",
      "Epoch: 895,train_loss:0.0886376, Train: 1.0000, Val: 0.7220, Test: 0.7700\n",
      "Epoch: 896,train_loss:0.0808028, Train: 1.0000, Val: 0.7240, Test: 0.7640\n",
      "Epoch: 897,train_loss:0.0789400, Train: 1.0000, Val: 0.7220, Test: 0.7630\n",
      "Epoch: 898,train_loss:0.1328616, Train: 1.0000, Val: 0.7240, Test: 0.7650\n",
      "Epoch: 899,train_loss:0.0945847, Train: 1.0000, Val: 0.7300, Test: 0.7730\n",
      "Epoch: 900,train_loss:0.0915769, Train: 1.0000, Val: 0.7360, Test: 0.7680\n",
      "Epoch: 901,train_loss:0.0768838, Train: 1.0000, Val: 0.7400, Test: 0.7650\n",
      "Epoch: 902,train_loss:0.0656134, Train: 1.0000, Val: 0.7400, Test: 0.7640\n",
      "Epoch: 903,train_loss:0.1124821, Train: 1.0000, Val: 0.7360, Test: 0.7630\n",
      "Epoch: 904,train_loss:0.0900998, Train: 1.0000, Val: 0.7300, Test: 0.7750\n",
      "Epoch: 905,train_loss:0.0969060, Train: 1.0000, Val: 0.7200, Test: 0.7760\n",
      "Epoch: 906,train_loss:0.1255611, Train: 1.0000, Val: 0.7160, Test: 0.7610\n",
      "Epoch: 907,train_loss:0.0741764, Train: 1.0000, Val: 0.7140, Test: 0.7590\n",
      "Epoch: 908,train_loss:0.0673177, Train: 1.0000, Val: 0.7160, Test: 0.7690\n",
      "Epoch: 909,train_loss:0.1232767, Train: 1.0000, Val: 0.7240, Test: 0.7670\n",
      "Epoch: 910,train_loss:0.0392859, Train: 1.0000, Val: 0.7200, Test: 0.7660\n",
      "Epoch: 911,train_loss:0.0566962, Train: 1.0000, Val: 0.7280, Test: 0.7660\n",
      "Epoch: 912,train_loss:0.1506889, Train: 1.0000, Val: 0.7320, Test: 0.7710\n",
      "Epoch: 913,train_loss:0.0465585, Train: 1.0000, Val: 0.7260, Test: 0.7460\n",
      "Epoch: 914,train_loss:0.0504534, Train: 1.0000, Val: 0.6980, Test: 0.7190\n",
      "Epoch: 915,train_loss:0.1350130, Train: 1.0000, Val: 0.7080, Test: 0.7240\n",
      "Epoch: 916,train_loss:0.0779411, Train: 1.0000, Val: 0.7220, Test: 0.7400\n",
      "Epoch: 917,train_loss:0.0859600, Train: 1.0000, Val: 0.7320, Test: 0.7680\n",
      "Epoch: 918,train_loss:0.0843277, Train: 1.0000, Val: 0.7340, Test: 0.7770\n",
      "Epoch: 919,train_loss:0.0658596, Train: 1.0000, Val: 0.7160, Test: 0.7630\n",
      "Epoch: 920,train_loss:0.1356944, Train: 1.0000, Val: 0.7200, Test: 0.7620\n",
      "Epoch: 921,train_loss:0.1318017, Train: 1.0000, Val: 0.7380, Test: 0.7640\n",
      "Epoch: 922,train_loss:0.0756974, Train: 1.0000, Val: 0.7440, Test: 0.7760\n",
      "Epoch: 923,train_loss:0.0725361, Train: 1.0000, Val: 0.7400, Test: 0.7580\n",
      "Epoch: 924,train_loss:0.1219471, Train: 1.0000, Val: 0.7200, Test: 0.7320\n",
      "Epoch: 925,train_loss:0.1045175, Train: 1.0000, Val: 0.7220, Test: 0.7310\n",
      "Epoch: 926,train_loss:0.1518488, Train: 1.0000, Val: 0.7400, Test: 0.7550\n",
      "Epoch: 927,train_loss:0.0905361, Train: 1.0000, Val: 0.7320, Test: 0.7680\n",
      "Epoch: 928,train_loss:0.0622324, Train: 1.0000, Val: 0.7340, Test: 0.7660\n",
      "Epoch: 929,train_loss:0.1261290, Train: 1.0000, Val: 0.7320, Test: 0.7730\n",
      "Epoch: 930,train_loss:0.1184635, Train: 1.0000, Val: 0.7260, Test: 0.7740\n",
      "Epoch: 931,train_loss:0.1919938, Train: 1.0000, Val: 0.7340, Test: 0.7720\n",
      "Epoch: 932,train_loss:0.1131432, Train: 1.0000, Val: 0.7300, Test: 0.7700\n",
      "Epoch: 933,train_loss:0.0691150, Train: 1.0000, Val: 0.7340, Test: 0.7560\n",
      "Epoch: 934,train_loss:0.0897620, Train: 1.0000, Val: 0.7160, Test: 0.7360\n",
      "Epoch: 935,train_loss:0.1679668, Train: 1.0000, Val: 0.7160, Test: 0.7310\n",
      "Epoch: 936,train_loss:0.0719965, Train: 1.0000, Val: 0.7360, Test: 0.7570\n",
      "Epoch: 937,train_loss:0.0425766, Train: 1.0000, Val: 0.7440, Test: 0.7610\n",
      "Epoch: 938,train_loss:0.0827632, Train: 1.0000, Val: 0.7500, Test: 0.7600\n",
      "Epoch: 939,train_loss:0.1458872, Train: 1.0000, Val: 0.7300, Test: 0.7590\n",
      "Epoch: 940,train_loss:0.0694497, Train: 1.0000, Val: 0.7060, Test: 0.7420\n",
      "Epoch: 941,train_loss:0.1007289, Train: 1.0000, Val: 0.6780, Test: 0.7090\n",
      "Epoch: 942,train_loss:0.1684875, Train: 1.0000, Val: 0.7080, Test: 0.7270\n",
      "Epoch: 943,train_loss:0.1530972, Train: 1.0000, Val: 0.7480, Test: 0.7730\n",
      "Epoch: 944,train_loss:0.0643787, Train: 1.0000, Val: 0.7180, Test: 0.7480\n",
      "Epoch: 945,train_loss:0.1120685, Train: 1.0000, Val: 0.6940, Test: 0.7140\n",
      "Epoch: 946,train_loss:0.0828323, Train: 1.0000, Val: 0.7060, Test: 0.7120\n",
      "Epoch: 947,train_loss:0.1247391, Train: 1.0000, Val: 0.7440, Test: 0.7510\n",
      "Epoch: 948,train_loss:0.1157402, Train: 1.0000, Val: 0.7320, Test: 0.7710\n",
      "Epoch: 949,train_loss:0.0973437, Train: 1.0000, Val: 0.7280, Test: 0.7740\n",
      "Epoch: 950,train_loss:0.0499586, Train: 1.0000, Val: 0.7120, Test: 0.7520\n",
      "Epoch: 951,train_loss:0.0788229, Train: 1.0000, Val: 0.7080, Test: 0.7390\n",
      "Epoch: 952,train_loss:0.0965008, Train: 1.0000, Val: 0.7080, Test: 0.7400\n",
      "Epoch: 953,train_loss:0.1323462, Train: 1.0000, Val: 0.7220, Test: 0.7560\n",
      "Epoch: 954,train_loss:0.1372430, Train: 1.0000, Val: 0.7120, Test: 0.7590\n",
      "Epoch: 955,train_loss:0.1152555, Train: 1.0000, Val: 0.7140, Test: 0.7470\n",
      "Epoch: 956,train_loss:0.1003472, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch: 957,train_loss:0.1377255, Train: 1.0000, Val: 0.7320, Test: 0.7310\n",
      "Epoch: 958,train_loss:0.1168808, Train: 1.0000, Val: 0.7360, Test: 0.7410\n",
      "Epoch: 959,train_loss:0.0867490, Train: 1.0000, Val: 0.7340, Test: 0.7560\n",
      "Epoch: 960,train_loss:0.0829481, Train: 1.0000, Val: 0.7220, Test: 0.7470\n",
      "Epoch: 961,train_loss:0.0733386, Train: 0.9929, Val: 0.7160, Test: 0.7490\n",
      "Epoch: 962,train_loss:0.1434522, Train: 0.9929, Val: 0.7220, Test: 0.7590\n",
      "Epoch: 963,train_loss:0.0940403, Train: 1.0000, Val: 0.7300, Test: 0.7600\n",
      "Epoch: 964,train_loss:0.0443673, Train: 1.0000, Val: 0.7280, Test: 0.7650\n",
      "Epoch: 965,train_loss:0.0698230, Train: 1.0000, Val: 0.7280, Test: 0.7560\n",
      "Epoch: 966,train_loss:0.1117505, Train: 1.0000, Val: 0.7340, Test: 0.7560\n",
      "Epoch: 967,train_loss:0.1115326, Train: 1.0000, Val: 0.7340, Test: 0.7520\n",
      "Epoch: 968,train_loss:0.1168532, Train: 1.0000, Val: 0.7120, Test: 0.7540\n",
      "Epoch: 969,train_loss:0.0608624, Train: 1.0000, Val: 0.6960, Test: 0.7450\n",
      "Epoch: 970,train_loss:0.1114717, Train: 1.0000, Val: 0.6960, Test: 0.7420\n",
      "Epoch: 971,train_loss:0.1001887, Train: 1.0000, Val: 0.6980, Test: 0.7330\n",
      "Epoch: 972,train_loss:0.0559270, Train: 1.0000, Val: 0.7120, Test: 0.7350\n",
      "Epoch: 973,train_loss:0.1110366, Train: 1.0000, Val: 0.7220, Test: 0.7490\n",
      "Epoch: 974,train_loss:0.1040085, Train: 1.0000, Val: 0.7280, Test: 0.7480\n",
      "Epoch: 975,train_loss:0.1048815, Train: 1.0000, Val: 0.7360, Test: 0.7460\n",
      "Epoch: 976,train_loss:0.1061927, Train: 1.0000, Val: 0.7340, Test: 0.7350\n",
      "Epoch: 977,train_loss:0.0974583, Train: 1.0000, Val: 0.7320, Test: 0.7390\n",
      "Epoch: 978,train_loss:0.1731212, Train: 1.0000, Val: 0.7340, Test: 0.7570\n",
      "Epoch: 979,train_loss:0.0682962, Train: 1.0000, Val: 0.7140, Test: 0.7370\n",
      "Epoch: 980,train_loss:0.1860349, Train: 0.9929, Val: 0.7120, Test: 0.7100\n",
      "Epoch: 981,train_loss:0.1033121, Train: 0.9929, Val: 0.7040, Test: 0.7130\n",
      "Epoch: 982,train_loss:0.0869215, Train: 1.0000, Val: 0.7100, Test: 0.7290\n",
      "Epoch: 983,train_loss:0.1062539, Train: 1.0000, Val: 0.7300, Test: 0.7570\n",
      "Epoch: 984,train_loss:0.1083497, Train: 1.0000, Val: 0.7320, Test: 0.7700\n",
      "Epoch: 985,train_loss:0.0939852, Train: 1.0000, Val: 0.7480, Test: 0.7760\n",
      "Epoch: 986,train_loss:0.1193413, Train: 1.0000, Val: 0.7340, Test: 0.7510\n",
      "Epoch: 987,train_loss:0.0947795, Train: 1.0000, Val: 0.6960, Test: 0.7290\n",
      "Epoch: 988,train_loss:0.1821070, Train: 1.0000, Val: 0.6940, Test: 0.7200\n",
      "Epoch: 989,train_loss:0.1486087, Train: 1.0000, Val: 0.7160, Test: 0.7410\n",
      "Epoch: 990,train_loss:0.0938104, Train: 1.0000, Val: 0.7360, Test: 0.7700\n",
      "Epoch: 991,train_loss:0.0928288, Train: 1.0000, Val: 0.7500, Test: 0.7880\n",
      "Epoch: 992,train_loss:0.1102132, Train: 1.0000, Val: 0.7340, Test: 0.7710\n",
      "Epoch: 993,train_loss:0.0515609, Train: 1.0000, Val: 0.7400, Test: 0.7650\n",
      "Epoch: 994,train_loss:0.0682958, Train: 1.0000, Val: 0.7380, Test: 0.7610\n",
      "Epoch: 995,train_loss:0.1471240, Train: 1.0000, Val: 0.7280, Test: 0.7520\n",
      "Epoch: 996,train_loss:0.0950600, Train: 1.0000, Val: 0.7100, Test: 0.7440\n",
      "Epoch: 997,train_loss:0.0668201, Train: 1.0000, Val: 0.7020, Test: 0.7340\n",
      "Epoch: 998,train_loss:0.0534088, Train: 1.0000, Val: 0.7040, Test: 0.7360\n",
      "Epoch: 999,train_loss:0.1268853, Train: 1.0000, Val: 0.7060, Test: 0.7410\n",
      "Epoch: 1000,train_loss:0.0656396, Train: 1.0000, Val: 0.7220, Test: 0.7460\n",
      "CPU times: user 32.7 s, sys: 5.38 s, total: 38.1 s\n",
      "Wall time: 32.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "480135"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 2  #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        # self.gatconv = AGNNConv(requires_grad=True)\n",
    "        self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_cora_geniepath,testacclist_cora_geniepath=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_cora_geniepath.append(loss)\n",
    "    testacclist_cora_geniepath.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# %matplotlib inline\n",
    "# f, ax = plt.subplots(1,2)\n",
    "\n",
    "# ax[0][0].plot(losslist_cora_geniepath,label=\"losslist_cora_geniepath\")\n",
    "# ax[0][1].plot(testacclist_cora_geniepath,label=\"testacclist_cora_geniepath\")\n",
    "# plt.legend(loc=0, ncol=1) \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000cora\n",
    "Epoch: 984,train_loss:0.0691620, Train: 1.0000, Val: 0.7180, Test: 0.7240\n",
    "Epoch: 985,train_loss:0.0437133, Train: 1.0000, Val: 0.7220, Test: 0.7410\n",
    "Epoch: 986,train_loss:0.0614745, Train: 1.0000, Val: 0.7200, Test: 0.7450\n",
    "Epoch: 987,train_loss:0.1168561, Train: 1.0000, Val: 0.7120, Test: 0.7460\n",
    "Epoch: 988,train_loss:0.1891420, Train: 1.0000, Val: 0.7140, Test: 0.7410\n",
    "Epoch: 989,train_loss:0.1568069, Train: 1.0000, Val: 0.7240, Test: 0.7360\n",
    "Epoch: 990,train_loss:0.0887830, Train: 1.0000, Val: 0.7300, Test: 0.7310\n",
    "Epoch: 991,train_loss:0.1009816, Train: 1.0000, Val: 0.7220, Test: 0.7140\n",
    "Epoch: 992,train_loss:0.0463423, Train: 1.0000, Val: 0.7120, Test: 0.7070\n",
    "Epoch: 993,train_loss:0.1423967, Train: 1.0000, Val: 0.7060, Test: 0.7330\n",
    "Epoch: 994,train_loss:0.0659238, Train: 1.0000, Val: 0.7200, Test: 0.7550\n",
    "Epoch: 995,train_loss:0.1288171, Train: 1.0000, Val: 0.7280, Test: 0.7590\n",
    "Epoch: 996,train_loss:0.0688528, Train: 1.0000, Val: 0.7300, Test: 0.7590\n",
    "Epoch: 997,train_loss:0.0710301, Train: 1.0000, Val: 0.7300, Test: 0.7590\n",
    "Epoch: 998,train_loss:0.1300182, Train: 1.0000, Val: 0.7300, Test: 0.7500\n",
    "Epoch: 999,train_loss:0.1003017, Train: 1.0000, Val: 0.7260, Test: 0.7510\n",
    "Epoch: 1000,train_loss:0.0455814, Train: 1.0000, Val: 0.7220, Test: 0.7460\n",
    "CPU times: user 28.3 s, sys: 631 ms, total: 29 s\n",
    "Wall time: 25.9 s\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sum([torch.numel(param) for param in model.parameters()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "f, ax = plt.subplots(1,2, figsize=(10,5), squeeze=False)\n",
    "\n",
    "ax[0][0].plot(losslist_cora_mymodel,label=\"losslist_cora_mymodel\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_cora_mymodel,label=\"testacclist_cora_mymodel\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "ax[0][0].plot(losslist_cora_geniepath,label=\"losslist_cora_geniepath\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_cora_geniepath,label=\"testacclist_cora_geniepath\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "plt.savefig(\"gen_sgencora\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pubmed sgeniepath agnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.0988692, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 002,train_loss:1.0986657, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 003,train_loss:1.0986352, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 004,train_loss:1.0984639, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 005,train_loss:1.0985398, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 006,train_loss:1.0985246, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 007,train_loss:1.0983005, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 008,train_loss:1.0983397, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 009,train_loss:1.0982456, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 010,train_loss:1.0982630, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 011,train_loss:1.0982293, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 012,train_loss:1.0981506, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 013,train_loss:1.0979565, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 014,train_loss:1.0978974, Train: 0.3333, Val: 0.4160, Test: 0.4070\n",
      "Epoch: 015,train_loss:1.0975486, Train: 0.3667, Val: 0.4180, Test: 0.4100\n",
      "Epoch: 016,train_loss:1.0975840, Train: 0.6000, Val: 0.5220, Test: 0.5060\n",
      "Epoch: 017,train_loss:1.0961559, Train: 0.6500, Val: 0.5700, Test: 0.5280\n",
      "Epoch: 018,train_loss:1.0957664, Train: 0.6500, Val: 0.5640, Test: 0.5250\n",
      "Epoch: 019,train_loss:1.0936654, Train: 0.7500, Val: 0.5760, Test: 0.5630\n",
      "Epoch: 020,train_loss:1.0895982, Train: 0.7167, Val: 0.5820, Test: 0.5620\n",
      "Epoch: 021,train_loss:1.0827619, Train: 0.7000, Val: 0.5800, Test: 0.5510\n",
      "Epoch: 022,train_loss:1.0685455, Train: 0.7000, Val: 0.5680, Test: 0.5370\n",
      "Epoch: 023,train_loss:1.0363079, Train: 0.6667, Val: 0.5700, Test: 0.5310\n",
      "Epoch: 024,train_loss:0.9897912, Train: 0.6500, Val: 0.5640, Test: 0.5320\n",
      "Epoch: 025,train_loss:0.9097925, Train: 0.6500, Val: 0.5660, Test: 0.5290\n",
      "Epoch: 026,train_loss:0.8166584, Train: 0.6500, Val: 0.5640, Test: 0.5280\n",
      "Epoch: 027,train_loss:0.6875674, Train: 0.6667, Val: 0.5640, Test: 0.5300\n",
      "Epoch: 028,train_loss:0.5981091, Train: 0.6500, Val: 0.5700, Test: 0.5320\n",
      "Epoch: 029,train_loss:0.5780401, Train: 0.6500, Val: 0.5700, Test: 0.5310\n",
      "Epoch: 030,train_loss:0.5452901, Train: 0.6500, Val: 0.5520, Test: 0.5270\n",
      "Epoch: 031,train_loss:0.5786164, Train: 0.6500, Val: 0.5540, Test: 0.5270\n",
      "Epoch: 032,train_loss:0.4911449, Train: 0.6667, Val: 0.5540, Test: 0.5280\n",
      "Epoch: 033,train_loss:0.4889760, Train: 0.6500, Val: 0.5620, Test: 0.5250\n",
      "Epoch: 034,train_loss:0.4975000, Train: 0.7333, Val: 0.5320, Test: 0.5200\n",
      "Epoch: 035,train_loss:0.4649895, Train: 0.6667, Val: 0.5340, Test: 0.5360\n",
      "Epoch: 036,train_loss:0.4644854, Train: 0.6667, Val: 0.5360, Test: 0.5380\n",
      "Epoch: 037,train_loss:0.5099833, Train: 0.7000, Val: 0.5440, Test: 0.5350\n",
      "Epoch: 038,train_loss:0.4736413, Train: 0.7167, Val: 0.5280, Test: 0.5010\n",
      "Epoch: 039,train_loss:0.5743688, Train: 0.6667, Val: 0.5220, Test: 0.4940\n",
      "Epoch: 040,train_loss:0.4631359, Train: 0.7000, Val: 0.5360, Test: 0.4890\n",
      "Epoch: 041,train_loss:0.4541360, Train: 0.7833, Val: 0.5600, Test: 0.5130\n",
      "Epoch: 042,train_loss:0.5441570, Train: 0.7333, Val: 0.5440, Test: 0.5370\n",
      "Epoch: 043,train_loss:0.4986230, Train: 0.7333, Val: 0.5560, Test: 0.5450\n",
      "Epoch: 044,train_loss:0.4527003, Train: 0.8167, Val: 0.5980, Test: 0.5480\n",
      "Epoch: 045,train_loss:0.4412193, Train: 0.8167, Val: 0.5620, Test: 0.5420\n",
      "Epoch: 046,train_loss:0.4541210, Train: 0.7833, Val: 0.5560, Test: 0.5440\n",
      "Epoch: 047,train_loss:0.4553947, Train: 0.8000, Val: 0.5500, Test: 0.5370\n",
      "Epoch: 048,train_loss:0.4529797, Train: 0.8000, Val: 0.5540, Test: 0.5410\n",
      "Epoch: 049,train_loss:0.3823627, Train: 0.7500, Val: 0.5600, Test: 0.5550\n",
      "Epoch: 050,train_loss:0.4308324, Train: 0.8833, Val: 0.5840, Test: 0.5680\n",
      "Epoch: 051,train_loss:0.3646098, Train: 0.9500, Val: 0.6280, Test: 0.6030\n",
      "Epoch: 052,train_loss:0.3808327, Train: 0.9500, Val: 0.6460, Test: 0.6160\n",
      "Epoch: 053,train_loss:0.3481114, Train: 0.9333, Val: 0.6380, Test: 0.6130\n",
      "Epoch: 054,train_loss:0.3150722, Train: 0.9167, Val: 0.6300, Test: 0.6100\n",
      "Epoch: 055,train_loss:0.3137670, Train: 0.9667, Val: 0.6520, Test: 0.6340\n",
      "Epoch: 056,train_loss:0.3150209, Train: 0.9833, Val: 0.6680, Test: 0.6450\n",
      "Epoch: 057,train_loss:0.3446987, Train: 0.9833, Val: 0.6700, Test: 0.6390\n",
      "Epoch: 058,train_loss:0.2839981, Train: 0.9833, Val: 0.6720, Test: 0.6420\n",
      "Epoch: 059,train_loss:0.2662837, Train: 0.9833, Val: 0.6720, Test: 0.6340\n",
      "Epoch: 060,train_loss:0.2494464, Train: 0.9833, Val: 0.6640, Test: 0.6360\n",
      "Epoch: 061,train_loss:0.1889245, Train: 0.9833, Val: 0.6660, Test: 0.6380\n",
      "Epoch: 062,train_loss:0.2783465, Train: 0.9833, Val: 0.6780, Test: 0.6410\n",
      "Epoch: 063,train_loss:0.1420181, Train: 0.9833, Val: 0.6800, Test: 0.6490\n",
      "Epoch: 064,train_loss:0.1345181, Train: 0.9833, Val: 0.6860, Test: 0.6530\n",
      "Epoch: 065,train_loss:0.1415165, Train: 0.9833, Val: 0.6800, Test: 0.6640\n",
      "Epoch: 066,train_loss:0.1233658, Train: 0.9833, Val: 0.6820, Test: 0.6600\n",
      "Epoch: 067,train_loss:0.1572457, Train: 0.9833, Val: 0.6700, Test: 0.6520\n",
      "Epoch: 068,train_loss:0.0821141, Train: 0.9833, Val: 0.6560, Test: 0.6450\n",
      "Epoch: 069,train_loss:0.1408000, Train: 0.9833, Val: 0.6840, Test: 0.6640\n",
      "Epoch: 070,train_loss:0.0927403, Train: 0.9833, Val: 0.6920, Test: 0.6730\n",
      "Epoch: 071,train_loss:0.1116609, Train: 0.9833, Val: 0.6880, Test: 0.6770\n",
      "Epoch: 072,train_loss:0.1137012, Train: 0.9833, Val: 0.6860, Test: 0.6700\n",
      "Epoch: 073,train_loss:0.1209584, Train: 1.0000, Val: 0.6660, Test: 0.6500\n",
      "Epoch: 074,train_loss:0.1738862, Train: 1.0000, Val: 0.6720, Test: 0.6490\n",
      "Epoch: 075,train_loss:0.1310064, Train: 1.0000, Val: 0.6920, Test: 0.6640\n",
      "Epoch: 076,train_loss:0.0769150, Train: 0.9667, Val: 0.6920, Test: 0.6730\n",
      "Epoch: 077,train_loss:0.0869031, Train: 0.9667, Val: 0.6700, Test: 0.6650\n",
      "Epoch: 078,train_loss:0.1277974, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
      "Epoch: 079,train_loss:0.0937338, Train: 1.0000, Val: 0.6720, Test: 0.6590\n",
      "Epoch: 080,train_loss:0.0881476, Train: 1.0000, Val: 0.6700, Test: 0.6510\n",
      "Epoch: 081,train_loss:0.0543182, Train: 1.0000, Val: 0.6920, Test: 0.6710\n",
      "Epoch: 082,train_loss:0.1024458, Train: 1.0000, Val: 0.6940, Test: 0.6850\n",
      "Epoch: 083,train_loss:0.0654370, Train: 1.0000, Val: 0.6940, Test: 0.6860\n",
      "Epoch: 084,train_loss:0.0680340, Train: 1.0000, Val: 0.6880, Test: 0.6880\n",
      "Epoch: 085,train_loss:0.0820231, Train: 1.0000, Val: 0.6940, Test: 0.6820\n",
      "Epoch: 086,train_loss:0.0494338, Train: 1.0000, Val: 0.6940, Test: 0.6810\n",
      "Epoch: 087,train_loss:0.0396313, Train: 1.0000, Val: 0.6940, Test: 0.6850\n",
      "Epoch: 088,train_loss:0.0526350, Train: 1.0000, Val: 0.6900, Test: 0.6830\n",
      "Epoch: 089,train_loss:0.0738368, Train: 1.0000, Val: 0.6880, Test: 0.6750\n",
      "Epoch: 090,train_loss:0.1074613, Train: 1.0000, Val: 0.6960, Test: 0.6720\n",
      "Epoch: 091,train_loss:0.1407764, Train: 1.0000, Val: 0.6820, Test: 0.6620\n",
      "Epoch: 092,train_loss:0.1193719, Train: 1.0000, Val: 0.6860, Test: 0.6700\n",
      "Epoch: 093,train_loss:0.0582003, Train: 1.0000, Val: 0.6920, Test: 0.6710\n",
      "Epoch: 094,train_loss:0.0378833, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
      "Epoch: 095,train_loss:0.0410324, Train: 1.0000, Val: 0.7020, Test: 0.6970\n",
      "Epoch: 096,train_loss:0.0917301, Train: 1.0000, Val: 0.7000, Test: 0.6980\n",
      "Epoch: 097,train_loss:0.0394838, Train: 1.0000, Val: 0.7020, Test: 0.6980\n",
      "Epoch: 098,train_loss:0.1197705, Train: 1.0000, Val: 0.6940, Test: 0.6960\n",
      "Epoch: 099,train_loss:0.0552111, Train: 1.0000, Val: 0.7040, Test: 0.6930\n",
      "Epoch: 100,train_loss:0.0657222, Train: 1.0000, Val: 0.7000, Test: 0.6890\n",
      "Epoch: 101,train_loss:0.0820077, Train: 1.0000, Val: 0.7060, Test: 0.6870\n",
      "Epoch: 102,train_loss:0.0405703, Train: 1.0000, Val: 0.7080, Test: 0.6880\n",
      "Epoch: 103,train_loss:0.1137518, Train: 1.0000, Val: 0.7120, Test: 0.6890\n",
      "Epoch: 104,train_loss:0.0236532, Train: 1.0000, Val: 0.7100, Test: 0.6890\n",
      "Epoch: 105,train_loss:0.0304434, Train: 1.0000, Val: 0.7060, Test: 0.6910\n",
      "Epoch: 106,train_loss:0.0242563, Train: 1.0000, Val: 0.7020, Test: 0.6920\n",
      "Epoch: 107,train_loss:0.0515733, Train: 1.0000, Val: 0.7080, Test: 0.6950\n",
      "Epoch: 108,train_loss:0.0261588, Train: 1.0000, Val: 0.7040, Test: 0.6930\n",
      "Epoch: 109,train_loss:0.1148055, Train: 1.0000, Val: 0.7100, Test: 0.7000\n",
      "Epoch: 110,train_loss:0.0333961, Train: 1.0000, Val: 0.7100, Test: 0.6990\n",
      "Epoch: 111,train_loss:0.0783744, Train: 1.0000, Val: 0.7060, Test: 0.6910\n",
      "Epoch: 112,train_loss:0.0289862, Train: 1.0000, Val: 0.6880, Test: 0.6790\n",
      "Epoch: 113,train_loss:0.0162462, Train: 1.0000, Val: 0.6800, Test: 0.6650\n",
      "Epoch: 114,train_loss:0.0372542, Train: 1.0000, Val: 0.7000, Test: 0.6770\n",
      "Epoch: 115,train_loss:0.0260780, Train: 1.0000, Val: 0.7020, Test: 0.6910\n",
      "Epoch: 116,train_loss:0.0184579, Train: 1.0000, Val: 0.7040, Test: 0.7010\n",
      "Epoch: 117,train_loss:0.0440967, Train: 1.0000, Val: 0.7060, Test: 0.6990\n",
      "Epoch: 118,train_loss:0.0125995, Train: 1.0000, Val: 0.7080, Test: 0.6990\n",
      "Epoch: 119,train_loss:0.0706811, Train: 1.0000, Val: 0.7100, Test: 0.7080\n",
      "Epoch: 120,train_loss:0.0440517, Train: 1.0000, Val: 0.7020, Test: 0.7010\n",
      "Epoch: 121,train_loss:0.0365153, Train: 1.0000, Val: 0.7060, Test: 0.7010\n",
      "Epoch: 122,train_loss:0.0426492, Train: 1.0000, Val: 0.7040, Test: 0.6950\n",
      "Epoch: 123,train_loss:0.0338272, Train: 1.0000, Val: 0.7060, Test: 0.6940\n",
      "Epoch: 124,train_loss:0.0202712, Train: 1.0000, Val: 0.7060, Test: 0.6910\n",
      "Epoch: 125,train_loss:0.0295675, Train: 1.0000, Val: 0.7200, Test: 0.7000\n",
      "Epoch: 126,train_loss:0.0690662, Train: 1.0000, Val: 0.7140, Test: 0.7020\n",
      "Epoch: 127,train_loss:0.0481842, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
      "Epoch: 128,train_loss:0.0189233, Train: 1.0000, Val: 0.6840, Test: 0.6720\n",
      "Epoch: 129,train_loss:0.0421377, Train: 1.0000, Val: 0.6880, Test: 0.6740\n",
      "Epoch: 130,train_loss:0.0316935, Train: 1.0000, Val: 0.7000, Test: 0.6970\n",
      "Epoch: 131,train_loss:0.0208670, Train: 1.0000, Val: 0.7120, Test: 0.7120\n",
      "Epoch: 132,train_loss:0.1223759, Train: 1.0000, Val: 0.7160, Test: 0.7070\n",
      "Epoch: 133,train_loss:0.0394170, Train: 1.0000, Val: 0.7180, Test: 0.7070\n",
      "Epoch: 134,train_loss:0.0268929, Train: 1.0000, Val: 0.7120, Test: 0.7050\n",
      "Epoch: 135,train_loss:0.0343913, Train: 1.0000, Val: 0.7220, Test: 0.7090\n",
      "Epoch: 136,train_loss:0.0143918, Train: 1.0000, Val: 0.7180, Test: 0.7070\n",
      "Epoch: 137,train_loss:0.0293760, Train: 1.0000, Val: 0.7100, Test: 0.7080\n",
      "Epoch: 138,train_loss:0.0200539, Train: 1.0000, Val: 0.7120, Test: 0.7020\n",
      "Epoch: 139,train_loss:0.0144953, Train: 1.0000, Val: 0.7100, Test: 0.6950\n",
      "Epoch: 140,train_loss:0.0116175, Train: 1.0000, Val: 0.7140, Test: 0.6960\n",
      "Epoch: 141,train_loss:0.0139581, Train: 1.0000, Val: 0.7120, Test: 0.7010\n",
      "Epoch: 142,train_loss:0.0178174, Train: 1.0000, Val: 0.7040, Test: 0.7120\n",
      "Epoch: 143,train_loss:0.0167711, Train: 1.0000, Val: 0.7140, Test: 0.7210\n",
      "Epoch: 144,train_loss:0.0104998, Train: 1.0000, Val: 0.7180, Test: 0.7230\n",
      "Epoch: 145,train_loss:0.0106947, Train: 1.0000, Val: 0.7240, Test: 0.7280\n",
      "Epoch: 146,train_loss:0.0380858, Train: 1.0000, Val: 0.7340, Test: 0.7250\n",
      "Epoch: 147,train_loss:0.0269530, Train: 1.0000, Val: 0.7240, Test: 0.7250\n",
      "Epoch: 148,train_loss:0.0117372, Train: 1.0000, Val: 0.7260, Test: 0.7250\n",
      "Epoch: 149,train_loss:0.0311050, Train: 1.0000, Val: 0.7140, Test: 0.7150\n",
      "Epoch: 150,train_loss:0.0062749, Train: 1.0000, Val: 0.7260, Test: 0.7000\n",
      "Epoch: 151,train_loss:0.0386753, Train: 1.0000, Val: 0.7260, Test: 0.7000\n",
      "Epoch: 152,train_loss:0.0152935, Train: 1.0000, Val: 0.7220, Test: 0.7010\n",
      "Epoch: 153,train_loss:0.0128136, Train: 1.0000, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 154,train_loss:0.0443100, Train: 1.0000, Val: 0.7220, Test: 0.7280\n",
      "Epoch: 155,train_loss:0.0086805, Train: 1.0000, Val: 0.7300, Test: 0.7240\n",
      "Epoch: 156,train_loss:0.0489264, Train: 1.0000, Val: 0.7260, Test: 0.7250\n",
      "Epoch: 157,train_loss:0.0132359, Train: 1.0000, Val: 0.7200, Test: 0.7310\n",
      "Epoch: 158,train_loss:0.0099075, Train: 1.0000, Val: 0.7260, Test: 0.7320\n",
      "Epoch: 159,train_loss:0.0137461, Train: 1.0000, Val: 0.7240, Test: 0.7230\n",
      "Epoch: 160,train_loss:0.0203877, Train: 1.0000, Val: 0.7300, Test: 0.7100\n",
      "Epoch: 161,train_loss:0.0122686, Train: 1.0000, Val: 0.7340, Test: 0.7100\n",
      "Epoch: 162,train_loss:0.0105756, Train: 1.0000, Val: 0.7340, Test: 0.7090\n",
      "Epoch: 163,train_loss:0.0067143, Train: 1.0000, Val: 0.7360, Test: 0.7100\n",
      "Epoch: 164,train_loss:0.0237518, Train: 1.0000, Val: 0.7320, Test: 0.7190\n",
      "Epoch: 165,train_loss:0.0084460, Train: 1.0000, Val: 0.7340, Test: 0.7330\n",
      "Epoch: 166,train_loss:0.0052282, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch: 167,train_loss:0.0114928, Train: 1.0000, Val: 0.7260, Test: 0.7320\n",
      "Epoch: 168,train_loss:0.0256553, Train: 1.0000, Val: 0.7200, Test: 0.7350\n",
      "Epoch: 169,train_loss:0.0124470, Train: 1.0000, Val: 0.7300, Test: 0.7320\n",
      "Epoch: 170,train_loss:0.0077175, Train: 1.0000, Val: 0.7300, Test: 0.7360\n",
      "Epoch: 171,train_loss:0.0191908, Train: 1.0000, Val: 0.7360, Test: 0.7280\n",
      "Epoch: 172,train_loss:0.0119423, Train: 1.0000, Val: 0.7360, Test: 0.7210\n",
      "Epoch: 173,train_loss:0.0094187, Train: 1.0000, Val: 0.7460, Test: 0.7130\n",
      "Epoch: 174,train_loss:0.0125609, Train: 1.0000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 175,train_loss:0.0224459, Train: 1.0000, Val: 0.7400, Test: 0.7170\n",
      "Epoch: 176,train_loss:0.0091970, Train: 1.0000, Val: 0.7260, Test: 0.7270\n",
      "Epoch: 177,train_loss:0.0084148, Train: 1.0000, Val: 0.7340, Test: 0.7450\n",
      "Epoch: 178,train_loss:0.0172618, Train: 1.0000, Val: 0.7300, Test: 0.7440\n",
      "Epoch: 179,train_loss:0.0185099, Train: 1.0000, Val: 0.7320, Test: 0.7440\n",
      "Epoch: 180,train_loss:0.0138251, Train: 1.0000, Val: 0.7300, Test: 0.7460\n",
      "Epoch: 181,train_loss:0.0084022, Train: 1.0000, Val: 0.7380, Test: 0.7330\n",
      "Epoch: 182,train_loss:0.0113196, Train: 1.0000, Val: 0.7440, Test: 0.7290\n",
      "Epoch: 183,train_loss:0.0085322, Train: 1.0000, Val: 0.7420, Test: 0.7270\n",
      "Epoch: 184,train_loss:0.0135005, Train: 1.0000, Val: 0.7440, Test: 0.7300\n",
      "Epoch: 185,train_loss:0.0052293, Train: 1.0000, Val: 0.7360, Test: 0.7370\n",
      "Epoch: 186,train_loss:0.0121162, Train: 1.0000, Val: 0.7420, Test: 0.7450\n",
      "Epoch: 187,train_loss:0.0091931, Train: 1.0000, Val: 0.7380, Test: 0.7480\n",
      "Epoch: 188,train_loss:0.0166860, Train: 1.0000, Val: 0.7380, Test: 0.7480\n",
      "Epoch: 189,train_loss:0.0084478, Train: 1.0000, Val: 0.7280, Test: 0.7520\n",
      "Epoch: 190,train_loss:0.0150897, Train: 1.0000, Val: 0.7300, Test: 0.7510\n",
      "Epoch: 191,train_loss:0.0199656, Train: 1.0000, Val: 0.7420, Test: 0.7510\n",
      "Epoch: 192,train_loss:0.0145426, Train: 1.0000, Val: 0.7320, Test: 0.7500\n",
      "Epoch: 193,train_loss:0.0145969, Train: 1.0000, Val: 0.7360, Test: 0.7510\n",
      "Epoch: 194,train_loss:0.0106599, Train: 1.0000, Val: 0.7420, Test: 0.7480\n",
      "Epoch: 195,train_loss:0.0143247, Train: 1.0000, Val: 0.7440, Test: 0.7510\n",
      "Epoch: 196,train_loss:0.0126465, Train: 1.0000, Val: 0.7460, Test: 0.7500\n",
      "Epoch: 197,train_loss:0.0182744, Train: 1.0000, Val: 0.7380, Test: 0.7480\n",
      "Epoch: 198,train_loss:0.0112923, Train: 1.0000, Val: 0.7420, Test: 0.7480\n",
      "Epoch: 199,train_loss:0.0122018, Train: 1.0000, Val: 0.7420, Test: 0.7520\n",
      "Epoch: 200,train_loss:0.0081807, Train: 1.0000, Val: 0.7400, Test: 0.7510\n",
      "Epoch: 201,train_loss:0.0092683, Train: 1.0000, Val: 0.7380, Test: 0.7490\n",
      "Epoch: 202,train_loss:0.0132352, Train: 1.0000, Val: 0.7400, Test: 0.7470\n",
      "Epoch: 203,train_loss:0.0162934, Train: 1.0000, Val: 0.7440, Test: 0.7430\n",
      "Epoch: 204,train_loss:0.0100822, Train: 1.0000, Val: 0.7440, Test: 0.7420\n",
      "Epoch: 205,train_loss:0.0115199, Train: 1.0000, Val: 0.7420, Test: 0.7470\n",
      "Epoch: 206,train_loss:0.0135583, Train: 1.0000, Val: 0.7400, Test: 0.7560\n",
      "Epoch: 207,train_loss:0.0095122, Train: 1.0000, Val: 0.7400, Test: 0.7530\n",
      "Epoch: 208,train_loss:0.0053784, Train: 1.0000, Val: 0.7420, Test: 0.7390\n",
      "Epoch: 209,train_loss:0.0148658, Train: 1.0000, Val: 0.7500, Test: 0.7380\n",
      "Epoch: 210,train_loss:0.0117805, Train: 1.0000, Val: 0.7500, Test: 0.7390\n",
      "Epoch: 211,train_loss:0.0096025, Train: 1.0000, Val: 0.7500, Test: 0.7420\n",
      "Epoch: 212,train_loss:0.0130067, Train: 1.0000, Val: 0.7440, Test: 0.7410\n",
      "Epoch: 213,train_loss:0.0118778, Train: 1.0000, Val: 0.7460, Test: 0.7510\n",
      "Epoch: 214,train_loss:0.0051900, Train: 1.0000, Val: 0.7400, Test: 0.7540\n",
      "Epoch: 215,train_loss:0.0062341, Train: 1.0000, Val: 0.7380, Test: 0.7590\n",
      "Epoch: 216,train_loss:0.0133026, Train: 1.0000, Val: 0.7400, Test: 0.7570\n",
      "Epoch: 217,train_loss:0.0081312, Train: 1.0000, Val: 0.7380, Test: 0.7560\n",
      "Epoch: 218,train_loss:0.0081905, Train: 1.0000, Val: 0.7400, Test: 0.7540\n",
      "Epoch: 219,train_loss:0.0075542, Train: 1.0000, Val: 0.7420, Test: 0.7480\n",
      "Epoch: 220,train_loss:0.0120076, Train: 1.0000, Val: 0.7420, Test: 0.7490\n",
      "Epoch: 221,train_loss:0.0257982, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch: 222,train_loss:0.0067097, Train: 1.0000, Val: 0.7340, Test: 0.7540\n",
      "Epoch: 223,train_loss:0.0078655, Train: 1.0000, Val: 0.7460, Test: 0.7420\n",
      "Epoch: 224,train_loss:0.0090944, Train: 1.0000, Val: 0.7480, Test: 0.7360\n",
      "Epoch: 225,train_loss:0.0073940, Train: 1.0000, Val: 0.7500, Test: 0.7260\n",
      "Epoch: 226,train_loss:0.0086609, Train: 1.0000, Val: 0.7480, Test: 0.7260\n",
      "Epoch: 227,train_loss:0.0125188, Train: 1.0000, Val: 0.7500, Test: 0.7240\n",
      "Epoch: 228,train_loss:0.0142879, Train: 1.0000, Val: 0.7520, Test: 0.7360\n",
      "Epoch: 229,train_loss:0.0064568, Train: 1.0000, Val: 0.7400, Test: 0.7440\n",
      "Epoch: 230,train_loss:0.0110068, Train: 1.0000, Val: 0.7440, Test: 0.7490\n",
      "Epoch: 231,train_loss:0.0075857, Train: 1.0000, Val: 0.7440, Test: 0.7520\n",
      "Epoch: 232,train_loss:0.0163230, Train: 1.0000, Val: 0.7460, Test: 0.7460\n",
      "Epoch: 233,train_loss:0.0103390, Train: 1.0000, Val: 0.7440, Test: 0.7460\n",
      "Epoch: 234,train_loss:0.0099847, Train: 1.0000, Val: 0.7460, Test: 0.7560\n",
      "Epoch: 235,train_loss:0.0063409, Train: 1.0000, Val: 0.7440, Test: 0.7550\n",
      "Epoch: 236,train_loss:0.0144442, Train: 1.0000, Val: 0.7340, Test: 0.7490\n",
      "Epoch: 237,train_loss:0.0065783, Train: 1.0000, Val: 0.7500, Test: 0.7430\n",
      "Epoch: 238,train_loss:0.0127767, Train: 1.0000, Val: 0.7560, Test: 0.7420\n",
      "Epoch: 239,train_loss:0.0104488, Train: 1.0000, Val: 0.7540, Test: 0.7430\n",
      "Epoch: 240,train_loss:0.0087436, Train: 1.0000, Val: 0.7520, Test: 0.7490\n",
      "Epoch: 241,train_loss:0.0106644, Train: 1.0000, Val: 0.7460, Test: 0.7470\n",
      "Epoch: 242,train_loss:0.0056762, Train: 1.0000, Val: 0.7320, Test: 0.7490\n",
      "Epoch: 243,train_loss:0.0098923, Train: 1.0000, Val: 0.7360, Test: 0.7530\n",
      "Epoch: 244,train_loss:0.0087554, Train: 1.0000, Val: 0.7340, Test: 0.7500\n",
      "Epoch: 245,train_loss:0.0070160, Train: 1.0000, Val: 0.7360, Test: 0.7470\n",
      "Epoch: 246,train_loss:0.0217972, Train: 1.0000, Val: 0.7400, Test: 0.7460\n",
      "Epoch: 247,train_loss:0.0079252, Train: 1.0000, Val: 0.7400, Test: 0.7460\n",
      "Epoch: 248,train_loss:0.0109683, Train: 1.0000, Val: 0.7400, Test: 0.7450\n",
      "Epoch: 249,train_loss:0.0172450, Train: 1.0000, Val: 0.7400, Test: 0.7510\n",
      "Epoch: 250,train_loss:0.0046449, Train: 1.0000, Val: 0.7360, Test: 0.7490\n",
      "Epoch: 251,train_loss:0.0483584, Train: 1.0000, Val: 0.7440, Test: 0.7470\n",
      "Epoch: 252,train_loss:0.0121668, Train: 1.0000, Val: 0.7500, Test: 0.7490\n",
      "Epoch: 253,train_loss:0.0087939, Train: 1.0000, Val: 0.7500, Test: 0.7510\n",
      "Epoch: 254,train_loss:0.0087754, Train: 1.0000, Val: 0.7520, Test: 0.7520\n",
      "Epoch: 255,train_loss:0.0626372, Train: 1.0000, Val: 0.7460, Test: 0.7540\n",
      "Epoch: 256,train_loss:0.0105845, Train: 1.0000, Val: 0.7420, Test: 0.7510\n",
      "Epoch: 257,train_loss:0.0399878, Train: 1.0000, Val: 0.7520, Test: 0.7510\n",
      "Epoch: 258,train_loss:0.0132147, Train: 1.0000, Val: 0.7640, Test: 0.7500\n",
      "Epoch: 259,train_loss:0.0152262, Train: 1.0000, Val: 0.7580, Test: 0.7510\n",
      "Epoch: 260,train_loss:0.0146115, Train: 1.0000, Val: 0.7520, Test: 0.7540\n",
      "Epoch: 261,train_loss:0.0133669, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
      "Epoch: 262,train_loss:0.0117265, Train: 1.0000, Val: 0.7580, Test: 0.7620\n",
      "Epoch: 263,train_loss:0.0098792, Train: 1.0000, Val: 0.7580, Test: 0.7550\n",
      "Epoch: 264,train_loss:0.0120811, Train: 1.0000, Val: 0.7560, Test: 0.7600\n",
      "Epoch: 265,train_loss:0.0068118, Train: 1.0000, Val: 0.7560, Test: 0.7580\n",
      "Epoch: 266,train_loss:0.0099371, Train: 1.0000, Val: 0.7560, Test: 0.7590\n",
      "Epoch: 267,train_loss:0.0131225, Train: 1.0000, Val: 0.7560, Test: 0.7610\n",
      "Epoch: 268,train_loss:0.0049805, Train: 1.0000, Val: 0.7560, Test: 0.7570\n",
      "Epoch: 269,train_loss:0.0065406, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
      "Epoch: 270,train_loss:0.0077702, Train: 1.0000, Val: 0.7600, Test: 0.7500\n",
      "Epoch: 271,train_loss:0.0081723, Train: 1.0000, Val: 0.7600, Test: 0.7500\n",
      "Epoch: 272,train_loss:0.0095972, Train: 1.0000, Val: 0.7580, Test: 0.7460\n",
      "Epoch: 273,train_loss:0.0124265, Train: 1.0000, Val: 0.7500, Test: 0.7550\n",
      "Epoch: 274,train_loss:0.0087109, Train: 1.0000, Val: 0.7540, Test: 0.7570\n",
      "Epoch: 275,train_loss:0.0084604, Train: 1.0000, Val: 0.7580, Test: 0.7570\n",
      "Epoch: 276,train_loss:0.0064632, Train: 1.0000, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 277,train_loss:0.0097937, Train: 1.0000, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 278,train_loss:0.0078657, Train: 1.0000, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 279,train_loss:0.0103710, Train: 1.0000, Val: 0.7540, Test: 0.7570\n",
      "Epoch: 280,train_loss:0.0060833, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
      "Epoch: 281,train_loss:0.0085096, Train: 1.0000, Val: 0.7560, Test: 0.7580\n",
      "Epoch: 282,train_loss:0.0070294, Train: 1.0000, Val: 0.7560, Test: 0.7570\n",
      "Epoch: 283,train_loss:0.0133720, Train: 1.0000, Val: 0.7560, Test: 0.7550\n",
      "Epoch: 284,train_loss:0.0103707, Train: 1.0000, Val: 0.7500, Test: 0.7540\n",
      "Epoch: 285,train_loss:0.0139906, Train: 1.0000, Val: 0.7520, Test: 0.7530\n",
      "Epoch: 286,train_loss:0.0098002, Train: 1.0000, Val: 0.7500, Test: 0.7540\n",
      "Epoch: 287,train_loss:0.0156831, Train: 1.0000, Val: 0.7460, Test: 0.7550\n",
      "Epoch: 288,train_loss:0.0106443, Train: 1.0000, Val: 0.7500, Test: 0.7500\n",
      "Epoch: 289,train_loss:0.0113999, Train: 1.0000, Val: 0.7700, Test: 0.7480\n",
      "Epoch: 290,train_loss:0.0123583, Train: 1.0000, Val: 0.7620, Test: 0.7460\n",
      "Epoch: 291,train_loss:0.0145929, Train: 1.0000, Val: 0.7600, Test: 0.7480\n",
      "Epoch: 292,train_loss:0.0186674, Train: 1.0000, Val: 0.7640, Test: 0.7470\n",
      "Epoch: 293,train_loss:0.0128230, Train: 1.0000, Val: 0.7540, Test: 0.7490\n",
      "Epoch: 294,train_loss:0.0107279, Train: 1.0000, Val: 0.7420, Test: 0.7490\n",
      "Epoch: 295,train_loss:0.0078894, Train: 1.0000, Val: 0.7420, Test: 0.7530\n",
      "Epoch: 296,train_loss:0.0173257, Train: 1.0000, Val: 0.7460, Test: 0.7510\n",
      "Epoch: 297,train_loss:0.0431889, Train: 1.0000, Val: 0.7460, Test: 0.7450\n",
      "Epoch: 298,train_loss:0.0115104, Train: 1.0000, Val: 0.7380, Test: 0.7260\n",
      "Epoch: 299,train_loss:0.0344660, Train: 1.0000, Val: 0.7400, Test: 0.7390\n",
      "Epoch: 300,train_loss:0.0177219, Train: 1.0000, Val: 0.7520, Test: 0.7530\n",
      "Epoch: 301,train_loss:0.0289190, Train: 1.0000, Val: 0.7660, Test: 0.7610\n",
      "Epoch: 302,train_loss:0.0074505, Train: 1.0000, Val: 0.7460, Test: 0.7350\n",
      "Epoch: 303,train_loss:0.0217119, Train: 1.0000, Val: 0.7360, Test: 0.7250\n",
      "Epoch: 304,train_loss:0.0246550, Train: 1.0000, Val: 0.7520, Test: 0.7380\n",
      "Epoch: 305,train_loss:0.0176732, Train: 1.0000, Val: 0.7740, Test: 0.7530\n",
      "Epoch: 306,train_loss:0.0074734, Train: 1.0000, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 307,train_loss:0.0067767, Train: 1.0000, Val: 0.7660, Test: 0.7620\n",
      "Epoch: 308,train_loss:0.0096485, Train: 1.0000, Val: 0.7540, Test: 0.7530\n",
      "Epoch: 309,train_loss:0.0092508, Train: 1.0000, Val: 0.7420, Test: 0.7380\n",
      "Epoch: 310,train_loss:0.0054104, Train: 1.0000, Val: 0.7440, Test: 0.7330\n",
      "Epoch: 311,train_loss:0.0149832, Train: 1.0000, Val: 0.7440, Test: 0.7400\n",
      "Epoch: 312,train_loss:0.0092292, Train: 1.0000, Val: 0.7540, Test: 0.7550\n",
      "Epoch: 313,train_loss:0.0079709, Train: 1.0000, Val: 0.7620, Test: 0.7600\n",
      "Epoch: 314,train_loss:0.0102487, Train: 1.0000, Val: 0.7640, Test: 0.7620\n",
      "Epoch: 315,train_loss:0.0053590, Train: 1.0000, Val: 0.7660, Test: 0.7640\n",
      "Epoch: 316,train_loss:0.0039936, Train: 1.0000, Val: 0.7720, Test: 0.7560\n",
      "Epoch: 317,train_loss:0.0082347, Train: 1.0000, Val: 0.7700, Test: 0.7530\n",
      "Epoch: 318,train_loss:0.0144789, Train: 1.0000, Val: 0.7700, Test: 0.7580\n",
      "Epoch: 319,train_loss:0.0066426, Train: 1.0000, Val: 0.7740, Test: 0.7590\n",
      "Epoch: 320,train_loss:0.0083026, Train: 1.0000, Val: 0.7740, Test: 0.7630\n",
      "Epoch: 321,train_loss:0.0055549, Train: 1.0000, Val: 0.7800, Test: 0.7660\n",
      "Epoch: 322,train_loss:0.0064976, Train: 1.0000, Val: 0.7800, Test: 0.7650\n",
      "Epoch: 323,train_loss:0.0094110, Train: 1.0000, Val: 0.7820, Test: 0.7640\n",
      "Epoch: 324,train_loss:0.0084146, Train: 1.0000, Val: 0.7820, Test: 0.7640\n",
      "Epoch: 325,train_loss:0.0062395, Train: 1.0000, Val: 0.7820, Test: 0.7630\n",
      "Epoch: 326,train_loss:0.0094372, Train: 1.0000, Val: 0.7820, Test: 0.7630\n",
      "Epoch: 327,train_loss:0.0103155, Train: 1.0000, Val: 0.7820, Test: 0.7650\n",
      "Epoch: 328,train_loss:0.0069182, Train: 1.0000, Val: 0.7820, Test: 0.7650\n",
      "Epoch: 329,train_loss:0.0073988, Train: 1.0000, Val: 0.7840, Test: 0.7660\n",
      "Epoch: 330,train_loss:0.0140545, Train: 1.0000, Val: 0.7740, Test: 0.7670\n",
      "Epoch: 331,train_loss:0.0198864, Train: 1.0000, Val: 0.7740, Test: 0.7670\n",
      "Epoch: 332,train_loss:0.0072212, Train: 1.0000, Val: 0.7700, Test: 0.7670\n",
      "Epoch: 333,train_loss:0.0100725, Train: 1.0000, Val: 0.7720, Test: 0.7560\n",
      "Epoch: 334,train_loss:0.0092008, Train: 1.0000, Val: 0.7740, Test: 0.7540\n",
      "Epoch: 335,train_loss:0.0100551, Train: 1.0000, Val: 0.7740, Test: 0.7540\n",
      "Epoch: 336,train_loss:0.0083667, Train: 1.0000, Val: 0.7740, Test: 0.7520\n",
      "Epoch: 337,train_loss:0.0110805, Train: 1.0000, Val: 0.7720, Test: 0.7570\n",
      "Epoch: 338,train_loss:0.0086454, Train: 1.0000, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 339,train_loss:0.0152671, Train: 1.0000, Val: 0.7800, Test: 0.7670\n",
      "Epoch: 340,train_loss:0.0247597, Train: 1.0000, Val: 0.7780, Test: 0.7660\n",
      "Epoch: 341,train_loss:0.0100789, Train: 1.0000, Val: 0.7820, Test: 0.7670\n",
      "Epoch: 342,train_loss:0.0119994, Train: 1.0000, Val: 0.7820, Test: 0.7640\n",
      "Epoch: 343,train_loss:0.0106901, Train: 1.0000, Val: 0.7780, Test: 0.7650\n",
      "Epoch: 344,train_loss:0.0077536, Train: 1.0000, Val: 0.7680, Test: 0.7630\n",
      "Epoch: 345,train_loss:0.0099909, Train: 1.0000, Val: 0.7780, Test: 0.7560\n",
      "Epoch: 346,train_loss:0.0092818, Train: 1.0000, Val: 0.7760, Test: 0.7580\n",
      "Epoch: 347,train_loss:0.0058928, Train: 1.0000, Val: 0.7800, Test: 0.7580\n",
      "Epoch: 348,train_loss:0.0082256, Train: 1.0000, Val: 0.7800, Test: 0.7580\n",
      "Epoch: 349,train_loss:0.0078544, Train: 1.0000, Val: 0.7780, Test: 0.7550\n",
      "Epoch: 350,train_loss:0.0102234, Train: 1.0000, Val: 0.7720, Test: 0.7590\n",
      "Epoch: 351,train_loss:0.0138542, Train: 1.0000, Val: 0.7740, Test: 0.7670\n",
      "Epoch: 352,train_loss:0.0081399, Train: 1.0000, Val: 0.7760, Test: 0.7630\n",
      "Epoch: 353,train_loss:0.0060549, Train: 1.0000, Val: 0.7660, Test: 0.7690\n",
      "Epoch: 354,train_loss:0.0064247, Train: 1.0000, Val: 0.7700, Test: 0.7660\n",
      "Epoch: 355,train_loss:0.0070861, Train: 1.0000, Val: 0.7720, Test: 0.7670\n",
      "Epoch: 356,train_loss:0.0034229, Train: 1.0000, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 357,train_loss:0.0099970, Train: 1.0000, Val: 0.7680, Test: 0.7680\n",
      "Epoch: 358,train_loss:0.0096226, Train: 1.0000, Val: 0.7700, Test: 0.7690\n",
      "Epoch: 359,train_loss:0.0053960, Train: 1.0000, Val: 0.7720, Test: 0.7640\n",
      "Epoch: 360,train_loss:0.0105283, Train: 1.0000, Val: 0.7780, Test: 0.7670\n",
      "Epoch: 361,train_loss:0.0077684, Train: 1.0000, Val: 0.7680, Test: 0.7630\n",
      "Epoch: 362,train_loss:0.0085378, Train: 1.0000, Val: 0.7720, Test: 0.7570\n",
      "Epoch: 363,train_loss:0.0054583, Train: 1.0000, Val: 0.7800, Test: 0.7590\n",
      "Epoch: 364,train_loss:0.0114850, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 365,train_loss:0.0105382, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
      "Epoch: 366,train_loss:0.0097567, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
      "Epoch: 367,train_loss:0.0146441, Train: 1.0000, Val: 0.7660, Test: 0.7610\n",
      "Epoch: 368,train_loss:0.0117346, Train: 1.0000, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 369,train_loss:0.0094256, Train: 1.0000, Val: 0.7660, Test: 0.7620\n",
      "Epoch: 370,train_loss:0.0071443, Train: 1.0000, Val: 0.7620, Test: 0.7650\n",
      "Epoch: 371,train_loss:0.0133276, Train: 1.0000, Val: 0.7620, Test: 0.7720\n",
      "Epoch: 372,train_loss:0.0180013, Train: 1.0000, Val: 0.7660, Test: 0.7650\n",
      "Epoch: 373,train_loss:0.0089271, Train: 1.0000, Val: 0.7660, Test: 0.7630\n",
      "Epoch: 374,train_loss:0.0142133, Train: 1.0000, Val: 0.7680, Test: 0.7670\n",
      "Epoch: 375,train_loss:0.0066249, Train: 1.0000, Val: 0.7700, Test: 0.7700\n",
      "Epoch: 376,train_loss:0.0051659, Train: 1.0000, Val: 0.7720, Test: 0.7710\n",
      "Epoch: 377,train_loss:0.0105786, Train: 1.0000, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 378,train_loss:0.0075773, Train: 1.0000, Val: 0.7660, Test: 0.7730\n",
      "Epoch: 379,train_loss:0.0058181, Train: 1.0000, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 380,train_loss:0.0065863, Train: 1.0000, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 381,train_loss:0.0119484, Train: 1.0000, Val: 0.7680, Test: 0.7730\n",
      "Epoch: 382,train_loss:0.0079593, Train: 1.0000, Val: 0.7680, Test: 0.7730\n",
      "Epoch: 383,train_loss:0.0115421, Train: 1.0000, Val: 0.7660, Test: 0.7750\n",
      "Epoch: 384,train_loss:0.0156090, Train: 1.0000, Val: 0.7620, Test: 0.7680\n",
      "Epoch: 385,train_loss:0.0095700, Train: 1.0000, Val: 0.7720, Test: 0.7690\n",
      "Epoch: 386,train_loss:0.0097308, Train: 1.0000, Val: 0.7780, Test: 0.7720\n",
      "Epoch: 387,train_loss:0.0092193, Train: 1.0000, Val: 0.7760, Test: 0.7690\n",
      "Epoch: 388,train_loss:0.0067196, Train: 1.0000, Val: 0.7780, Test: 0.7710\n",
      "Epoch: 389,train_loss:0.0077280, Train: 1.0000, Val: 0.7760, Test: 0.7720\n",
      "Epoch: 390,train_loss:0.0074007, Train: 1.0000, Val: 0.7780, Test: 0.7750\n",
      "Epoch: 391,train_loss:0.0060353, Train: 1.0000, Val: 0.7720, Test: 0.7720\n",
      "Epoch: 392,train_loss:0.0077890, Train: 1.0000, Val: 0.7720, Test: 0.7690\n",
      "Epoch: 393,train_loss:0.0076256, Train: 1.0000, Val: 0.7680, Test: 0.7700\n",
      "Epoch: 394,train_loss:0.0111148, Train: 1.0000, Val: 0.7660, Test: 0.7700\n",
      "Epoch: 395,train_loss:0.0061998, Train: 1.0000, Val: 0.7660, Test: 0.7690\n",
      "Epoch: 396,train_loss:0.0072554, Train: 1.0000, Val: 0.7680, Test: 0.7760\n",
      "Epoch: 397,train_loss:0.0064274, Train: 1.0000, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 398,train_loss:0.0104798, Train: 1.0000, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 399,train_loss:0.0101004, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 400,train_loss:0.0056249, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 401,train_loss:0.0078494, Train: 1.0000, Val: 0.7700, Test: 0.7730\n",
      "Epoch: 402,train_loss:0.0167303, Train: 1.0000, Val: 0.7700, Test: 0.7750\n",
      "Epoch: 403,train_loss:0.0088098, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 404,train_loss:0.0049020, Train: 1.0000, Val: 0.7660, Test: 0.7710\n",
      "Epoch: 405,train_loss:0.0150533, Train: 1.0000, Val: 0.7660, Test: 0.7720\n",
      "Epoch: 406,train_loss:0.0098614, Train: 1.0000, Val: 0.7680, Test: 0.7710\n",
      "Epoch: 407,train_loss:0.0093838, Train: 1.0000, Val: 0.7640, Test: 0.7680\n",
      "Epoch: 408,train_loss:0.0139057, Train: 1.0000, Val: 0.7700, Test: 0.7680\n",
      "Epoch: 409,train_loss:0.0092781, Train: 1.0000, Val: 0.7640, Test: 0.7680\n",
      "Epoch: 410,train_loss:0.0057577, Train: 1.0000, Val: 0.7620, Test: 0.7640\n",
      "Epoch: 411,train_loss:0.0125093, Train: 1.0000, Val: 0.7640, Test: 0.7630\n",
      "Epoch: 412,train_loss:0.0070935, Train: 1.0000, Val: 0.7660, Test: 0.7620\n",
      "Epoch: 413,train_loss:0.0090285, Train: 1.0000, Val: 0.7660, Test: 0.7650\n",
      "Epoch: 414,train_loss:0.0062569, Train: 1.0000, Val: 0.7640, Test: 0.7660\n",
      "Epoch: 415,train_loss:0.0152230, Train: 1.0000, Val: 0.7660, Test: 0.7670\n",
      "Epoch: 416,train_loss:0.0086981, Train: 1.0000, Val: 0.7660, Test: 0.7690\n",
      "Epoch: 417,train_loss:0.0052269, Train: 1.0000, Val: 0.7680, Test: 0.7710\n",
      "Epoch: 418,train_loss:0.0052645, Train: 1.0000, Val: 0.7640, Test: 0.7720\n",
      "Epoch: 419,train_loss:0.0214697, Train: 1.0000, Val: 0.7680, Test: 0.7750\n",
      "Epoch: 420,train_loss:0.0096513, Train: 1.0000, Val: 0.7660, Test: 0.7700\n",
      "Epoch: 421,train_loss:0.0054781, Train: 1.0000, Val: 0.7540, Test: 0.7690\n",
      "Epoch: 422,train_loss:0.0084728, Train: 1.0000, Val: 0.7560, Test: 0.7690\n",
      "Epoch: 423,train_loss:0.0117385, Train: 1.0000, Val: 0.7580, Test: 0.7670\n",
      "Epoch: 424,train_loss:0.0124486, Train: 1.0000, Val: 0.7660, Test: 0.7710\n",
      "Epoch: 425,train_loss:0.0033662, Train: 1.0000, Val: 0.7720, Test: 0.7710\n",
      "Epoch: 426,train_loss:0.0145879, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 427,train_loss:0.0108330, Train: 1.0000, Val: 0.7740, Test: 0.7730\n",
      "Epoch: 428,train_loss:0.0092645, Train: 1.0000, Val: 0.7760, Test: 0.7690\n",
      "Epoch: 429,train_loss:0.0096195, Train: 1.0000, Val: 0.7800, Test: 0.7670\n",
      "Epoch: 430,train_loss:0.0134926, Train: 1.0000, Val: 0.7780, Test: 0.7750\n",
      "Epoch: 431,train_loss:0.0066929, Train: 1.0000, Val: 0.7740, Test: 0.7750\n",
      "Epoch: 432,train_loss:0.0063167, Train: 1.0000, Val: 0.7780, Test: 0.7770\n",
      "Epoch: 433,train_loss:0.0079067, Train: 1.0000, Val: 0.7720, Test: 0.7770\n",
      "Epoch: 434,train_loss:0.0129924, Train: 1.0000, Val: 0.7720, Test: 0.7770\n",
      "Epoch: 435,train_loss:0.0093267, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 436,train_loss:0.0064614, Train: 1.0000, Val: 0.7740, Test: 0.7780\n",
      "Epoch: 437,train_loss:0.0078332, Train: 1.0000, Val: 0.7780, Test: 0.7790\n",
      "Epoch: 438,train_loss:0.0071873, Train: 1.0000, Val: 0.7780, Test: 0.7760\n",
      "Epoch: 439,train_loss:0.0065587, Train: 1.0000, Val: 0.7800, Test: 0.7750\n",
      "Epoch: 440,train_loss:0.0112320, Train: 1.0000, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 441,train_loss:0.0063180, Train: 1.0000, Val: 0.7860, Test: 0.7750\n",
      "Epoch: 442,train_loss:0.0108549, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 443,train_loss:0.0129852, Train: 1.0000, Val: 0.7740, Test: 0.7750\n",
      "Epoch: 444,train_loss:0.0097744, Train: 1.0000, Val: 0.7760, Test: 0.7760\n",
      "Epoch: 445,train_loss:0.0090983, Train: 1.0000, Val: 0.7740, Test: 0.7750\n",
      "Epoch: 446,train_loss:0.0181891, Train: 1.0000, Val: 0.7700, Test: 0.7720\n",
      "Epoch: 447,train_loss:0.0097277, Train: 1.0000, Val: 0.7680, Test: 0.7710\n",
      "Epoch: 448,train_loss:0.0069591, Train: 1.0000, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 449,train_loss:0.0075012, Train: 1.0000, Val: 0.7700, Test: 0.7740\n",
      "Epoch: 450,train_loss:0.0087453, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 451,train_loss:0.0054898, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 452,train_loss:0.0086344, Train: 1.0000, Val: 0.7640, Test: 0.7730\n",
      "Epoch: 453,train_loss:0.0050156, Train: 1.0000, Val: 0.7640, Test: 0.7740\n",
      "Epoch: 454,train_loss:0.0137169, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 455,train_loss:0.0062019, Train: 1.0000, Val: 0.7620, Test: 0.7730\n",
      "Epoch: 456,train_loss:0.0076728, Train: 1.0000, Val: 0.7580, Test: 0.7710\n",
      "Epoch: 457,train_loss:0.0083254, Train: 1.0000, Val: 0.7520, Test: 0.7730\n",
      "Epoch: 458,train_loss:0.0155358, Train: 1.0000, Val: 0.7600, Test: 0.7720\n",
      "Epoch: 459,train_loss:0.0106000, Train: 1.0000, Val: 0.7700, Test: 0.7770\n",
      "Epoch: 460,train_loss:0.0059211, Train: 1.0000, Val: 0.7680, Test: 0.7780\n",
      "Epoch: 461,train_loss:0.0092985, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 462,train_loss:0.0095771, Train: 1.0000, Val: 0.7660, Test: 0.7810\n",
      "Epoch: 463,train_loss:0.0093610, Train: 1.0000, Val: 0.7700, Test: 0.7760\n",
      "Epoch: 464,train_loss:0.0089025, Train: 1.0000, Val: 0.7740, Test: 0.7740\n",
      "Epoch: 465,train_loss:0.0050418, Train: 1.0000, Val: 0.7760, Test: 0.7740\n",
      "Epoch: 466,train_loss:0.0064306, Train: 1.0000, Val: 0.7760, Test: 0.7770\n",
      "Epoch: 467,train_loss:0.0056107, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 468,train_loss:0.0046859, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 469,train_loss:0.0103646, Train: 1.0000, Val: 0.7740, Test: 0.7740\n",
      "Epoch: 470,train_loss:0.0079703, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 471,train_loss:0.0058481, Train: 1.0000, Val: 0.7720, Test: 0.7760\n",
      "Epoch: 472,train_loss:0.0068772, Train: 1.0000, Val: 0.7700, Test: 0.7750\n",
      "Epoch: 473,train_loss:0.0087042, Train: 1.0000, Val: 0.7720, Test: 0.7740\n",
      "Epoch: 474,train_loss:0.0139051, Train: 1.0000, Val: 0.7720, Test: 0.7760\n",
      "Epoch: 475,train_loss:0.0065825, Train: 1.0000, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 476,train_loss:0.0092702, Train: 1.0000, Val: 0.7740, Test: 0.7790\n",
      "Epoch: 477,train_loss:0.0060051, Train: 1.0000, Val: 0.7760, Test: 0.7760\n",
      "Epoch: 478,train_loss:0.0119139, Train: 1.0000, Val: 0.7780, Test: 0.7770\n",
      "Epoch: 479,train_loss:0.0092337, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 480,train_loss:0.0140528, Train: 1.0000, Val: 0.7700, Test: 0.7790\n",
      "Epoch: 481,train_loss:0.0067832, Train: 1.0000, Val: 0.7740, Test: 0.7760\n",
      "Epoch: 482,train_loss:0.0059551, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 483,train_loss:0.0080232, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 484,train_loss:0.0096256, Train: 1.0000, Val: 0.7700, Test: 0.7760\n",
      "Epoch: 485,train_loss:0.0068594, Train: 1.0000, Val: 0.7720, Test: 0.7750\n",
      "Epoch: 486,train_loss:0.0063021, Train: 1.0000, Val: 0.7740, Test: 0.7760\n",
      "Epoch: 487,train_loss:0.0124685, Train: 1.0000, Val: 0.7720, Test: 0.7760\n",
      "Epoch: 488,train_loss:0.0075054, Train: 1.0000, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 489,train_loss:0.0073760, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 490,train_loss:0.0064475, Train: 1.0000, Val: 0.7800, Test: 0.7790\n",
      "Epoch: 491,train_loss:0.0070911, Train: 1.0000, Val: 0.7800, Test: 0.7790\n",
      "Epoch: 492,train_loss:0.0062292, Train: 1.0000, Val: 0.7800, Test: 0.7800\n",
      "Epoch: 493,train_loss:0.0093575, Train: 1.0000, Val: 0.7800, Test: 0.7790\n",
      "Epoch: 494,train_loss:0.0129429, Train: 1.0000, Val: 0.7780, Test: 0.7800\n",
      "Epoch: 495,train_loss:0.0083106, Train: 1.0000, Val: 0.7740, Test: 0.7790\n",
      "Epoch: 496,train_loss:0.0077844, Train: 1.0000, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 497,train_loss:0.0145010, Train: 1.0000, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 498,train_loss:0.0069784, Train: 1.0000, Val: 0.7800, Test: 0.7770\n",
      "Epoch: 499,train_loss:0.0122318, Train: 1.0000, Val: 0.7760, Test: 0.7780\n",
      "Epoch: 500,train_loss:0.0094097, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 501,train_loss:0.0086241, Train: 1.0000, Val: 0.7740, Test: 0.7740\n",
      "Epoch: 502,train_loss:0.0128748, Train: 1.0000, Val: 0.7740, Test: 0.7760\n",
      "Epoch: 503,train_loss:0.0090578, Train: 1.0000, Val: 0.7760, Test: 0.7730\n",
      "Epoch: 504,train_loss:0.0105785, Train: 1.0000, Val: 0.7720, Test: 0.7730\n",
      "Epoch: 505,train_loss:0.0059761, Train: 1.0000, Val: 0.7700, Test: 0.7750\n",
      "Epoch: 506,train_loss:0.0094454, Train: 1.0000, Val: 0.7680, Test: 0.7730\n",
      "Epoch: 507,train_loss:0.0063291, Train: 1.0000, Val: 0.7700, Test: 0.7730\n",
      "Epoch: 508,train_loss:0.0053476, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 509,train_loss:0.0067172, Train: 1.0000, Val: 0.7660, Test: 0.7740\n",
      "Epoch: 510,train_loss:0.0172040, Train: 1.0000, Val: 0.7620, Test: 0.7780\n",
      "Epoch: 511,train_loss:0.0080518, Train: 1.0000, Val: 0.7620, Test: 0.7790\n",
      "Epoch: 512,train_loss:0.0120683, Train: 1.0000, Val: 0.7640, Test: 0.7820\n",
      "Epoch: 513,train_loss:0.0134917, Train: 1.0000, Val: 0.7680, Test: 0.7800\n",
      "Epoch: 514,train_loss:0.0085427, Train: 1.0000, Val: 0.7740, Test: 0.7770\n",
      "Epoch: 515,train_loss:0.0083594, Train: 1.0000, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 516,train_loss:0.0122324, Train: 1.0000, Val: 0.7820, Test: 0.7750\n",
      "Epoch: 517,train_loss:0.0127897, Train: 1.0000, Val: 0.7820, Test: 0.7690\n",
      "Epoch: 518,train_loss:0.0075644, Train: 1.0000, Val: 0.7820, Test: 0.7690\n",
      "Epoch: 519,train_loss:0.0150752, Train: 1.0000, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 520,train_loss:0.0106012, Train: 1.0000, Val: 0.7860, Test: 0.7810\n",
      "Epoch: 521,train_loss:0.0080654, Train: 1.0000, Val: 0.7800, Test: 0.7860\n",
      "Epoch: 522,train_loss:0.0148704, Train: 1.0000, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 523,train_loss:0.0230388, Train: 1.0000, Val: 0.7820, Test: 0.7890\n",
      "Epoch: 524,train_loss:0.0174634, Train: 1.0000, Val: 0.7820, Test: 0.7810\n",
      "Epoch: 525,train_loss:0.0059350, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 526,train_loss:0.0242019, Train: 1.0000, Val: 0.7800, Test: 0.7660\n",
      "Epoch: 527,train_loss:0.0231484, Train: 1.0000, Val: 0.7900, Test: 0.7760\n",
      "Epoch: 528,train_loss:0.0055636, Train: 1.0000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 529,train_loss:0.0042575, Train: 1.0000, Val: 0.7740, Test: 0.7740\n",
      "Epoch: 530,train_loss:0.0078413, Train: 1.0000, Val: 0.7740, Test: 0.7760\n",
      "Epoch: 531,train_loss:0.0071449, Train: 1.0000, Val: 0.7700, Test: 0.7740\n",
      "Epoch: 532,train_loss:0.0078097, Train: 1.0000, Val: 0.7680, Test: 0.7740\n",
      "Epoch: 533,train_loss:0.0258095, Train: 1.0000, Val: 0.7800, Test: 0.7820\n",
      "Epoch: 534,train_loss:0.0079783, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 535,train_loss:0.0091300, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 536,train_loss:0.0059516, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 537,train_loss:0.0116690, Train: 1.0000, Val: 0.7880, Test: 0.7800\n",
      "Epoch: 538,train_loss:0.0102829, Train: 1.0000, Val: 0.7920, Test: 0.7800\n",
      "Epoch: 539,train_loss:0.0132022, Train: 1.0000, Val: 0.7960, Test: 0.7850\n",
      "Epoch: 540,train_loss:0.0056365, Train: 1.0000, Val: 0.7940, Test: 0.7920\n",
      "Epoch: 541,train_loss:0.0068789, Train: 1.0000, Val: 0.7900, Test: 0.7910\n",
      "Epoch: 542,train_loss:0.0102541, Train: 1.0000, Val: 0.7800, Test: 0.7860\n",
      "Epoch: 543,train_loss:0.0106768, Train: 1.0000, Val: 0.7760, Test: 0.7880\n",
      "Epoch: 544,train_loss:0.0157884, Train: 1.0000, Val: 0.7920, Test: 0.7930\n",
      "Epoch: 545,train_loss:0.0105056, Train: 1.0000, Val: 0.7920, Test: 0.7870\n",
      "Epoch: 546,train_loss:0.0128293, Train: 1.0000, Val: 0.7860, Test: 0.7810\n",
      "Epoch: 547,train_loss:0.0058893, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 548,train_loss:0.0067322, Train: 1.0000, Val: 0.7840, Test: 0.7710\n",
      "Epoch: 549,train_loss:0.0089409, Train: 1.0000, Val: 0.7780, Test: 0.7660\n",
      "Epoch: 550,train_loss:0.0118749, Train: 1.0000, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 551,train_loss:0.0127440, Train: 1.0000, Val: 0.7920, Test: 0.7740\n",
      "Epoch: 552,train_loss:0.0101061, Train: 1.0000, Val: 0.7800, Test: 0.7760\n",
      "Epoch: 553,train_loss:0.0095484, Train: 1.0000, Val: 0.7900, Test: 0.7890\n",
      "Epoch: 554,train_loss:0.0047208, Train: 1.0000, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 555,train_loss:0.0079243, Train: 1.0000, Val: 0.7740, Test: 0.7790\n",
      "Epoch: 556,train_loss:0.0119900, Train: 1.0000, Val: 0.7720, Test: 0.7790\n",
      "Epoch: 557,train_loss:0.0109953, Train: 1.0000, Val: 0.7820, Test: 0.7890\n",
      "Epoch: 558,train_loss:0.0127299, Train: 1.0000, Val: 0.7880, Test: 0.7910\n",
      "Epoch: 559,train_loss:0.0079082, Train: 1.0000, Val: 0.7900, Test: 0.7850\n",
      "Epoch: 560,train_loss:0.0089448, Train: 1.0000, Val: 0.7840, Test: 0.7820\n",
      "Epoch: 561,train_loss:0.0079879, Train: 1.0000, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 562,train_loss:0.0198445, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 563,train_loss:0.0062185, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 564,train_loss:0.0062794, Train: 1.0000, Val: 0.7800, Test: 0.7750\n",
      "Epoch: 565,train_loss:0.0056300, Train: 1.0000, Val: 0.7800, Test: 0.7730\n",
      "Epoch: 566,train_loss:0.0085233, Train: 1.0000, Val: 0.7780, Test: 0.7740\n",
      "Epoch: 567,train_loss:0.0077162, Train: 1.0000, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 568,train_loss:0.0067504, Train: 1.0000, Val: 0.7760, Test: 0.7720\n",
      "Epoch: 569,train_loss:0.0068088, Train: 1.0000, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 570,train_loss:0.0205108, Train: 1.0000, Val: 0.7760, Test: 0.7800\n",
      "Epoch: 571,train_loss:0.0126510, Train: 1.0000, Val: 0.7760, Test: 0.7800\n",
      "Epoch: 572,train_loss:0.0100478, Train: 1.0000, Val: 0.7760, Test: 0.7770\n",
      "Epoch: 573,train_loss:0.0149463, Train: 1.0000, Val: 0.7820, Test: 0.7760\n",
      "Epoch: 574,train_loss:0.0083723, Train: 1.0000, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 575,train_loss:0.0092422, Train: 1.0000, Val: 0.7860, Test: 0.7810\n",
      "Epoch: 576,train_loss:0.0113472, Train: 1.0000, Val: 0.7840, Test: 0.7820\n",
      "Epoch: 577,train_loss:0.0073247, Train: 1.0000, Val: 0.7820, Test: 0.7830\n",
      "Epoch: 578,train_loss:0.0186858, Train: 1.0000, Val: 0.7840, Test: 0.7830\n",
      "Epoch: 579,train_loss:0.0256602, Train: 1.0000, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 580,train_loss:0.0058844, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 581,train_loss:0.0133216, Train: 1.0000, Val: 0.7880, Test: 0.7810\n",
      "Epoch: 582,train_loss:0.0090743, Train: 1.0000, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 583,train_loss:0.0046045, Train: 1.0000, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 584,train_loss:0.0064967, Train: 1.0000, Val: 0.7900, Test: 0.7780\n",
      "Epoch: 585,train_loss:0.0066540, Train: 1.0000, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 586,train_loss:0.0076938, Train: 1.0000, Val: 0.7880, Test: 0.7880\n",
      "Epoch: 587,train_loss:0.0057842, Train: 1.0000, Val: 0.7840, Test: 0.7910\n",
      "Epoch: 588,train_loss:0.0085911, Train: 1.0000, Val: 0.7860, Test: 0.7870\n",
      "Epoch: 589,train_loss:0.0069921, Train: 1.0000, Val: 0.7840, Test: 0.7870\n",
      "Epoch: 590,train_loss:0.0076541, Train: 1.0000, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 591,train_loss:0.0069990, Train: 1.0000, Val: 0.7740, Test: 0.7830\n",
      "Epoch: 592,train_loss:0.0126065, Train: 1.0000, Val: 0.7740, Test: 0.7830\n",
      "Epoch: 593,train_loss:0.0085968, Train: 1.0000, Val: 0.7780, Test: 0.7870\n",
      "Epoch: 594,train_loss:0.0104193, Train: 1.0000, Val: 0.7860, Test: 0.7930\n",
      "Epoch: 595,train_loss:0.0087213, Train: 1.0000, Val: 0.7920, Test: 0.7910\n",
      "Epoch: 596,train_loss:0.0089477, Train: 1.0000, Val: 0.7900, Test: 0.7880\n",
      "Epoch: 597,train_loss:0.0057396, Train: 1.0000, Val: 0.7880, Test: 0.7880\n",
      "Epoch: 598,train_loss:0.0124699, Train: 1.0000, Val: 0.7880, Test: 0.7890\n",
      "Epoch: 599,train_loss:0.0050257, Train: 1.0000, Val: 0.7900, Test: 0.7900\n",
      "Epoch: 600,train_loss:0.0077677, Train: 1.0000, Val: 0.7880, Test: 0.7920\n",
      "Epoch: 601,train_loss:0.0054559, Train: 1.0000, Val: 0.7900, Test: 0.7910\n",
      "Epoch: 602,train_loss:0.0059781, Train: 1.0000, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 603,train_loss:0.0096625, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 604,train_loss:0.0072146, Train: 1.0000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 605,train_loss:0.0121223, Train: 1.0000, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 606,train_loss:0.0097398, Train: 1.0000, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 607,train_loss:0.0100965, Train: 1.0000, Val: 0.7880, Test: 0.7800\n",
      "Epoch: 608,train_loss:0.0059683, Train: 1.0000, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 609,train_loss:0.0118342, Train: 1.0000, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 610,train_loss:0.0111370, Train: 1.0000, Val: 0.7900, Test: 0.7900\n",
      "Epoch: 611,train_loss:0.0078188, Train: 1.0000, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 612,train_loss:0.0142703, Train: 1.0000, Val: 0.7880, Test: 0.7860\n",
      "Epoch: 613,train_loss:0.0055071, Train: 1.0000, Val: 0.7880, Test: 0.7880\n",
      "Epoch: 614,train_loss:0.0101467, Train: 1.0000, Val: 0.7860, Test: 0.7840\n",
      "Epoch: 615,train_loss:0.0072932, Train: 1.0000, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 616,train_loss:0.0074826, Train: 1.0000, Val: 0.7920, Test: 0.7760\n",
      "Epoch: 617,train_loss:0.0088337, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 618,train_loss:0.0097932, Train: 1.0000, Val: 0.7780, Test: 0.7750\n",
      "Epoch: 619,train_loss:0.0103310, Train: 1.0000, Val: 0.7820, Test: 0.7730\n",
      "Epoch: 620,train_loss:0.0074627, Train: 1.0000, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 621,train_loss:0.0075655, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 622,train_loss:0.0080424, Train: 1.0000, Val: 0.8000, Test: 0.7900\n",
      "Epoch: 623,train_loss:0.0127746, Train: 1.0000, Val: 0.7880, Test: 0.7890\n",
      "Epoch: 624,train_loss:0.0084886, Train: 1.0000, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 625,train_loss:0.0049534, Train: 1.0000, Val: 0.7840, Test: 0.7830\n",
      "Epoch: 626,train_loss:0.0077273, Train: 1.0000, Val: 0.7820, Test: 0.7810\n",
      "Epoch: 627,train_loss:0.0057525, Train: 1.0000, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 628,train_loss:0.0053808, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 629,train_loss:0.0104710, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 630,train_loss:0.0053128, Train: 1.0000, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 631,train_loss:0.0073407, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 632,train_loss:0.0095182, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 633,train_loss:0.0060029, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 634,train_loss:0.0085699, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 635,train_loss:0.0072551, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 636,train_loss:0.0094849, Train: 1.0000, Val: 0.7960, Test: 0.7830\n",
      "Epoch: 637,train_loss:0.0091699, Train: 1.0000, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 638,train_loss:0.0080202, Train: 1.0000, Val: 0.7940, Test: 0.7890\n",
      "Epoch: 639,train_loss:0.0068696, Train: 1.0000, Val: 0.7940, Test: 0.7890\n",
      "Epoch: 640,train_loss:0.0055112, Train: 1.0000, Val: 0.7940, Test: 0.7890\n",
      "Epoch: 641,train_loss:0.0088115, Train: 1.0000, Val: 0.7940, Test: 0.7880\n",
      "Epoch: 642,train_loss:0.0077115, Train: 1.0000, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 643,train_loss:0.0085330, Train: 1.0000, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 644,train_loss:0.0090254, Train: 1.0000, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 645,train_loss:0.0100809, Train: 1.0000, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 646,train_loss:0.0065882, Train: 1.0000, Val: 0.7960, Test: 0.7840\n",
      "Epoch: 647,train_loss:0.0083297, Train: 1.0000, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 648,train_loss:0.0124279, Train: 1.0000, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 649,train_loss:0.0068398, Train: 1.0000, Val: 0.7900, Test: 0.7880\n",
      "Epoch: 650,train_loss:0.0122128, Train: 1.0000, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 651,train_loss:0.0056177, Train: 1.0000, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 652,train_loss:0.0076847, Train: 1.0000, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 653,train_loss:0.0140784, Train: 1.0000, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 654,train_loss:0.0051273, Train: 1.0000, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 655,train_loss:0.0078617, Train: 1.0000, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 656,train_loss:0.0180359, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 657,train_loss:0.0081908, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 658,train_loss:0.0124469, Train: 1.0000, Val: 0.7800, Test: 0.7800\n",
      "Epoch: 659,train_loss:0.0077484, Train: 1.0000, Val: 0.7800, Test: 0.7770\n",
      "Epoch: 660,train_loss:0.0075415, Train: 1.0000, Val: 0.7820, Test: 0.7760\n",
      "Epoch: 661,train_loss:0.0107433, Train: 1.0000, Val: 0.7780, Test: 0.7750\n",
      "Epoch: 662,train_loss:0.0198991, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
      "Epoch: 663,train_loss:0.0057994, Train: 1.0000, Val: 0.7800, Test: 0.7780\n",
      "Epoch: 664,train_loss:0.0067657, Train: 1.0000, Val: 0.7860, Test: 0.7750\n",
      "Epoch: 665,train_loss:0.0075599, Train: 1.0000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 666,train_loss:0.0083130, Train: 1.0000, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 667,train_loss:0.0076807, Train: 1.0000, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 668,train_loss:0.0079679, Train: 1.0000, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 669,train_loss:0.0079840, Train: 1.0000, Val: 0.8000, Test: 0.7780\n",
      "Epoch: 670,train_loss:0.0075324, Train: 1.0000, Val: 0.7960, Test: 0.7770\n",
      "Epoch: 671,train_loss:0.0064804, Train: 1.0000, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 672,train_loss:0.0047568, Train: 1.0000, Val: 0.7960, Test: 0.7800\n",
      "Epoch: 673,train_loss:0.0066357, Train: 1.0000, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 674,train_loss:0.0114650, Train: 1.0000, Val: 0.7940, Test: 0.7830\n",
      "Epoch: 675,train_loss:0.0167523, Train: 1.0000, Val: 0.7980, Test: 0.7810\n",
      "Epoch: 676,train_loss:0.0080164, Train: 1.0000, Val: 0.7920, Test: 0.7800\n",
      "Epoch: 677,train_loss:0.0055633, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 678,train_loss:0.0142527, Train: 1.0000, Val: 0.7780, Test: 0.7740\n",
      "Epoch: 679,train_loss:0.0104298, Train: 1.0000, Val: 0.7760, Test: 0.7710\n",
      "Epoch: 680,train_loss:0.0097899, Train: 1.0000, Val: 0.7720, Test: 0.7680\n",
      "Epoch: 681,train_loss:0.0100750, Train: 1.0000, Val: 0.7740, Test: 0.7690\n",
      "Epoch: 682,train_loss:0.0109337, Train: 1.0000, Val: 0.7800, Test: 0.7730\n",
      "Epoch: 683,train_loss:0.0061789, Train: 1.0000, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 684,train_loss:0.0033196, Train: 1.0000, Val: 0.7860, Test: 0.7810\n",
      "Epoch: 685,train_loss:0.0053545, Train: 1.0000, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 686,train_loss:0.0052049, Train: 1.0000, Val: 0.7900, Test: 0.7820\n",
      "Epoch: 687,train_loss:0.0067037, Train: 1.0000, Val: 0.7940, Test: 0.7820\n",
      "Epoch: 688,train_loss:0.0103646, Train: 1.0000, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 689,train_loss:0.0070848, Train: 1.0000, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 690,train_loss:0.0072191, Train: 1.0000, Val: 0.7940, Test: 0.7820\n",
      "Epoch: 691,train_loss:0.0139438, Train: 1.0000, Val: 0.7900, Test: 0.7810\n",
      "Epoch: 692,train_loss:0.0106791, Train: 1.0000, Val: 0.7940, Test: 0.7820\n",
      "Epoch: 693,train_loss:0.0091454, Train: 1.0000, Val: 0.7900, Test: 0.7750\n",
      "Epoch: 694,train_loss:0.0153720, Train: 1.0000, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 695,train_loss:0.0059308, Train: 1.0000, Val: 0.7780, Test: 0.7760\n",
      "Epoch: 696,train_loss:0.0101421, Train: 1.0000, Val: 0.7700, Test: 0.7730\n",
      "Epoch: 697,train_loss:0.0095832, Train: 1.0000, Val: 0.7720, Test: 0.7670\n",
      "Epoch: 698,train_loss:0.0173209, Train: 1.0000, Val: 0.7740, Test: 0.7650\n",
      "Epoch: 699,train_loss:0.0147537, Train: 1.0000, Val: 0.7700, Test: 0.7700\n",
      "Epoch: 700,train_loss:0.0064836, Train: 1.0000, Val: 0.7820, Test: 0.7750\n",
      "Epoch: 701,train_loss:0.0075947, Train: 1.0000, Val: 0.7840, Test: 0.7740\n",
      "Epoch: 702,train_loss:0.0052133, Train: 1.0000, Val: 0.7900, Test: 0.7700\n",
      "Epoch: 703,train_loss:0.0064736, Train: 1.0000, Val: 0.7940, Test: 0.7680\n",
      "Epoch: 704,train_loss:0.0065388, Train: 1.0000, Val: 0.7900, Test: 0.7730\n",
      "Epoch: 705,train_loss:0.0178032, Train: 1.0000, Val: 0.7940, Test: 0.7700\n",
      "Epoch: 706,train_loss:0.0103392, Train: 1.0000, Val: 0.7940, Test: 0.7710\n",
      "Epoch: 707,train_loss:0.0087308, Train: 1.0000, Val: 0.7980, Test: 0.7730\n",
      "Epoch: 708,train_loss:0.0070074, Train: 1.0000, Val: 0.7880, Test: 0.7740\n",
      "Epoch: 709,train_loss:0.0090251, Train: 1.0000, Val: 0.7820, Test: 0.7760\n",
      "Epoch: 710,train_loss:0.0108044, Train: 1.0000, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 711,train_loss:0.0051988, Train: 1.0000, Val: 0.7920, Test: 0.7710\n",
      "Epoch: 712,train_loss:0.0120228, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 713,train_loss:0.0073513, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 714,train_loss:0.0163881, Train: 1.0000, Val: 0.7820, Test: 0.7750\n",
      "Epoch: 715,train_loss:0.0097901, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 716,train_loss:0.0057465, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 717,train_loss:0.0107353, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 718,train_loss:0.0059215, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 719,train_loss:0.0052316, Train: 1.0000, Val: 0.7900, Test: 0.7730\n",
      "Epoch: 720,train_loss:0.0099765, Train: 1.0000, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 721,train_loss:0.0047117, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 722,train_loss:0.0104878, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 723,train_loss:0.0048825, Train: 1.0000, Val: 0.7840, Test: 0.7750\n",
      "Epoch: 724,train_loss:0.0067870, Train: 1.0000, Val: 0.7900, Test: 0.7760\n",
      "Epoch: 725,train_loss:0.0065233, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 726,train_loss:0.0058659, Train: 1.0000, Val: 0.7860, Test: 0.7730\n",
      "Epoch: 727,train_loss:0.0056913, Train: 1.0000, Val: 0.7860, Test: 0.7720\n",
      "Epoch: 728,train_loss:0.0070550, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 729,train_loss:0.0084900, Train: 1.0000, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 730,train_loss:0.0091267, Train: 1.0000, Val: 0.7920, Test: 0.7740\n",
      "Epoch: 731,train_loss:0.0077114, Train: 1.0000, Val: 0.7920, Test: 0.7730\n",
      "Epoch: 732,train_loss:0.0083623, Train: 1.0000, Val: 0.7920, Test: 0.7730\n",
      "Epoch: 733,train_loss:0.0130409, Train: 1.0000, Val: 0.7920, Test: 0.7710\n",
      "Epoch: 734,train_loss:0.0092655, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 735,train_loss:0.0113949, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 736,train_loss:0.0073363, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 737,train_loss:0.0125672, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 738,train_loss:0.0076251, Train: 1.0000, Val: 0.7860, Test: 0.7780\n",
      "Epoch: 739,train_loss:0.0085686, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 740,train_loss:0.0062781, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 741,train_loss:0.0066814, Train: 1.0000, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 742,train_loss:0.0075990, Train: 1.0000, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 743,train_loss:0.0130258, Train: 1.0000, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 744,train_loss:0.0085632, Train: 1.0000, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 745,train_loss:0.0064676, Train: 1.0000, Val: 0.7940, Test: 0.7850\n",
      "Epoch: 746,train_loss:0.0098012, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 747,train_loss:0.0065320, Train: 1.0000, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 748,train_loss:0.0141880, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 749,train_loss:0.0094166, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 750,train_loss:0.0161175, Train: 1.0000, Val: 0.7820, Test: 0.7760\n",
      "Epoch: 751,train_loss:0.0067406, Train: 1.0000, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 752,train_loss:0.0068735, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 753,train_loss:0.0061870, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 754,train_loss:0.0109095, Train: 1.0000, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 755,train_loss:0.0097025, Train: 1.0000, Val: 0.7960, Test: 0.7820\n",
      "Epoch: 756,train_loss:0.0104755, Train: 1.0000, Val: 0.7980, Test: 0.7820\n",
      "Epoch: 757,train_loss:0.0070560, Train: 1.0000, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 758,train_loss:0.0087098, Train: 1.0000, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 759,train_loss:0.0070177, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 760,train_loss:0.0111728, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 761,train_loss:0.0078980, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 762,train_loss:0.0043226, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 763,train_loss:0.0087326, Train: 1.0000, Val: 0.7900, Test: 0.7750\n",
      "Epoch: 764,train_loss:0.0093844, Train: 1.0000, Val: 0.7940, Test: 0.7740\n",
      "Epoch: 765,train_loss:0.0110559, Train: 1.0000, Val: 0.7880, Test: 0.7710\n",
      "Epoch: 766,train_loss:0.0060138, Train: 1.0000, Val: 0.7880, Test: 0.7690\n",
      "Epoch: 767,train_loss:0.0058979, Train: 1.0000, Val: 0.7880, Test: 0.7670\n",
      "Epoch: 768,train_loss:0.0064172, Train: 1.0000, Val: 0.7860, Test: 0.7650\n",
      "Epoch: 769,train_loss:0.0066742, Train: 1.0000, Val: 0.7900, Test: 0.7670\n",
      "Epoch: 770,train_loss:0.0085203, Train: 1.0000, Val: 0.7920, Test: 0.7710\n",
      "Epoch: 771,train_loss:0.0082849, Train: 1.0000, Val: 0.7920, Test: 0.7710\n",
      "Epoch: 772,train_loss:0.0079199, Train: 1.0000, Val: 0.7940, Test: 0.7720\n",
      "Epoch: 773,train_loss:0.0066605, Train: 1.0000, Val: 0.7940, Test: 0.7710\n",
      "Epoch: 774,train_loss:0.0122147, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 775,train_loss:0.0078366, Train: 1.0000, Val: 0.7800, Test: 0.7760\n",
      "Epoch: 776,train_loss:0.0071444, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 777,train_loss:0.0121737, Train: 1.0000, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 778,train_loss:0.0137133, Train: 1.0000, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 779,train_loss:0.0133920, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 780,train_loss:0.0070293, Train: 1.0000, Val: 0.7940, Test: 0.7710\n",
      "Epoch: 781,train_loss:0.0054670, Train: 1.0000, Val: 0.7900, Test: 0.7670\n",
      "Epoch: 782,train_loss:0.0074060, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 783,train_loss:0.0077277, Train: 1.0000, Val: 0.7780, Test: 0.7610\n",
      "Epoch: 784,train_loss:0.0127393, Train: 1.0000, Val: 0.7860, Test: 0.7670\n",
      "Epoch: 785,train_loss:0.0074906, Train: 1.0000, Val: 0.7820, Test: 0.7680\n",
      "Epoch: 786,train_loss:0.0064808, Train: 1.0000, Val: 0.7800, Test: 0.7690\n",
      "Epoch: 787,train_loss:0.0106783, Train: 1.0000, Val: 0.7780, Test: 0.7740\n",
      "Epoch: 788,train_loss:0.0103048, Train: 1.0000, Val: 0.7900, Test: 0.7780\n",
      "Epoch: 789,train_loss:0.0123953, Train: 1.0000, Val: 0.7880, Test: 0.7780\n",
      "Epoch: 790,train_loss:0.0076954, Train: 1.0000, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 791,train_loss:0.0080587, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 792,train_loss:0.0122399, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
      "Epoch: 793,train_loss:0.0115554, Train: 1.0000, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 794,train_loss:0.0091274, Train: 1.0000, Val: 0.7880, Test: 0.7700\n",
      "Epoch: 795,train_loss:0.0090618, Train: 1.0000, Val: 0.7880, Test: 0.7720\n",
      "Epoch: 796,train_loss:0.0116441, Train: 1.0000, Val: 0.7860, Test: 0.7720\n",
      "Epoch: 797,train_loss:0.0054382, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 798,train_loss:0.0060512, Train: 1.0000, Val: 0.7840, Test: 0.7740\n",
      "Epoch: 799,train_loss:0.0090310, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
      "Epoch: 800,train_loss:0.0076713, Train: 1.0000, Val: 0.7740, Test: 0.7790\n",
      "Epoch: 801,train_loss:0.0067200, Train: 1.0000, Val: 0.7620, Test: 0.7770\n",
      "Epoch: 802,train_loss:0.0065011, Train: 1.0000, Val: 0.7620, Test: 0.7780\n",
      "Epoch: 803,train_loss:0.0160262, Train: 1.0000, Val: 0.7700, Test: 0.7780\n",
      "Epoch: 804,train_loss:0.0060544, Train: 1.0000, Val: 0.7780, Test: 0.7720\n",
      "Epoch: 805,train_loss:0.0048703, Train: 1.0000, Val: 0.7780, Test: 0.7720\n",
      "Epoch: 806,train_loss:0.0071864, Train: 1.0000, Val: 0.7840, Test: 0.7680\n",
      "Epoch: 807,train_loss:0.0082349, Train: 1.0000, Val: 0.7840, Test: 0.7690\n",
      "Epoch: 808,train_loss:0.0074664, Train: 1.0000, Val: 0.7840, Test: 0.7660\n",
      "Epoch: 809,train_loss:0.0103442, Train: 1.0000, Val: 0.7840, Test: 0.7710\n",
      "Epoch: 810,train_loss:0.0111101, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 811,train_loss:0.0070792, Train: 1.0000, Val: 0.7900, Test: 0.7790\n",
      "Epoch: 812,train_loss:0.0065165, Train: 1.0000, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 813,train_loss:0.0120562, Train: 1.0000, Val: 0.7920, Test: 0.7770\n",
      "Epoch: 814,train_loss:0.0057516, Train: 1.0000, Val: 0.7940, Test: 0.7770\n",
      "Epoch: 815,train_loss:0.0129343, Train: 1.0000, Val: 0.7900, Test: 0.7780\n",
      "Epoch: 816,train_loss:0.0079395, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 817,train_loss:0.0061175, Train: 1.0000, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 818,train_loss:0.0103208, Train: 1.0000, Val: 0.7880, Test: 0.7780\n",
      "Epoch: 819,train_loss:0.0047980, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 820,train_loss:0.0072230, Train: 1.0000, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 821,train_loss:0.0108623, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 822,train_loss:0.0089276, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 823,train_loss:0.0080698, Train: 1.0000, Val: 0.7840, Test: 0.7740\n",
      "Epoch: 824,train_loss:0.0069965, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 825,train_loss:0.0059461, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 826,train_loss:0.0086287, Train: 1.0000, Val: 0.7840, Test: 0.7730\n",
      "Epoch: 827,train_loss:0.0053701, Train: 1.0000, Val: 0.7780, Test: 0.7760\n",
      "Epoch: 828,train_loss:0.0078498, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 829,train_loss:0.0066193, Train: 1.0000, Val: 0.7880, Test: 0.7730\n",
      "Epoch: 830,train_loss:0.0104560, Train: 1.0000, Val: 0.7880, Test: 0.7730\n",
      "Epoch: 831,train_loss:0.0122453, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 832,train_loss:0.0088823, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 833,train_loss:0.0055361, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 834,train_loss:0.0082454, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 835,train_loss:0.0054175, Train: 1.0000, Val: 0.7820, Test: 0.7750\n",
      "Epoch: 836,train_loss:0.0080659, Train: 1.0000, Val: 0.7800, Test: 0.7740\n",
      "Epoch: 837,train_loss:0.0073625, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 838,train_loss:0.0104813, Train: 1.0000, Val: 0.7860, Test: 0.7730\n",
      "Epoch: 839,train_loss:0.0070799, Train: 1.0000, Val: 0.7860, Test: 0.7700\n",
      "Epoch: 840,train_loss:0.0057333, Train: 1.0000, Val: 0.7900, Test: 0.7690\n",
      "Epoch: 841,train_loss:0.0060368, Train: 1.0000, Val: 0.7940, Test: 0.7680\n",
      "Epoch: 842,train_loss:0.0086488, Train: 1.0000, Val: 0.7940, Test: 0.7680\n",
      "Epoch: 843,train_loss:0.0082078, Train: 1.0000, Val: 0.7960, Test: 0.7700\n",
      "Epoch: 844,train_loss:0.0130794, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 845,train_loss:0.0054496, Train: 1.0000, Val: 0.7780, Test: 0.7740\n",
      "Epoch: 846,train_loss:0.0083750, Train: 1.0000, Val: 0.7760, Test: 0.7720\n",
      "Epoch: 847,train_loss:0.0152943, Train: 1.0000, Val: 0.7700, Test: 0.7710\n",
      "Epoch: 848,train_loss:0.0083613, Train: 1.0000, Val: 0.7840, Test: 0.7690\n",
      "Epoch: 849,train_loss:0.0067736, Train: 1.0000, Val: 0.7820, Test: 0.7660\n",
      "Epoch: 850,train_loss:0.0085258, Train: 1.0000, Val: 0.7820, Test: 0.7690\n",
      "Epoch: 851,train_loss:0.0108125, Train: 1.0000, Val: 0.7760, Test: 0.7700\n",
      "Epoch: 852,train_loss:0.0068628, Train: 1.0000, Val: 0.7820, Test: 0.7670\n",
      "Epoch: 853,train_loss:0.0090455, Train: 1.0000, Val: 0.7820, Test: 0.7660\n",
      "Epoch: 854,train_loss:0.0056051, Train: 1.0000, Val: 0.7840, Test: 0.7710\n",
      "Epoch: 855,train_loss:0.0054648, Train: 1.0000, Val: 0.7780, Test: 0.7730\n",
      "Epoch: 856,train_loss:0.0053089, Train: 1.0000, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 857,train_loss:0.0109580, Train: 1.0000, Val: 0.7760, Test: 0.7770\n",
      "Epoch: 858,train_loss:0.0093008, Train: 1.0000, Val: 0.7780, Test: 0.7760\n",
      "Epoch: 859,train_loss:0.0101315, Train: 1.0000, Val: 0.7840, Test: 0.7610\n",
      "Epoch: 860,train_loss:0.0050386, Train: 1.0000, Val: 0.7780, Test: 0.7610\n",
      "Epoch: 861,train_loss:0.0111937, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 862,train_loss:0.0233820, Train: 1.0000, Val: 0.7760, Test: 0.7650\n",
      "Epoch: 863,train_loss:0.0071580, Train: 1.0000, Val: 0.7740, Test: 0.7610\n",
      "Epoch: 864,train_loss:0.0070912, Train: 1.0000, Val: 0.7680, Test: 0.7570\n",
      "Epoch: 865,train_loss:0.0075601, Train: 1.0000, Val: 0.7600, Test: 0.7560\n",
      "Epoch: 866,train_loss:0.0092639, Train: 1.0000, Val: 0.7580, Test: 0.7620\n",
      "Epoch: 867,train_loss:0.0154109, Train: 1.0000, Val: 0.7620, Test: 0.7570\n",
      "Epoch: 868,train_loss:0.0113830, Train: 1.0000, Val: 0.7640, Test: 0.7570\n",
      "Epoch: 869,train_loss:0.0122663, Train: 1.0000, Val: 0.7660, Test: 0.7570\n",
      "Epoch: 870,train_loss:0.0075318, Train: 1.0000, Val: 0.7780, Test: 0.7610\n",
      "Epoch: 871,train_loss:0.0093075, Train: 1.0000, Val: 0.7860, Test: 0.7630\n",
      "Epoch: 872,train_loss:0.0052835, Train: 1.0000, Val: 0.7860, Test: 0.7680\n",
      "Epoch: 873,train_loss:0.0053869, Train: 1.0000, Val: 0.7860, Test: 0.7730\n",
      "Epoch: 874,train_loss:0.0090771, Train: 1.0000, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 875,train_loss:0.0062354, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 876,train_loss:0.0090798, Train: 1.0000, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 877,train_loss:0.0101215, Train: 1.0000, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 878,train_loss:0.0078236, Train: 1.0000, Val: 0.7920, Test: 0.7800\n",
      "Epoch: 879,train_loss:0.0063369, Train: 1.0000, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 880,train_loss:0.0064703, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 881,train_loss:0.0076271, Train: 1.0000, Val: 0.7860, Test: 0.7650\n",
      "Epoch: 882,train_loss:0.0070025, Train: 1.0000, Val: 0.7840, Test: 0.7670\n",
      "Epoch: 883,train_loss:0.0059885, Train: 1.0000, Val: 0.7860, Test: 0.7690\n",
      "Epoch: 884,train_loss:0.0096185, Train: 1.0000, Val: 0.7820, Test: 0.7700\n",
      "Epoch: 885,train_loss:0.0065310, Train: 1.0000, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 886,train_loss:0.0089699, Train: 1.0000, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 887,train_loss:0.0092931, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 888,train_loss:0.0059894, Train: 1.0000, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 889,train_loss:0.0071107, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 890,train_loss:0.0046798, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 891,train_loss:0.0093072, Train: 1.0000, Val: 0.7880, Test: 0.7750\n",
      "Epoch: 892,train_loss:0.0088211, Train: 1.0000, Val: 0.7820, Test: 0.7730\n",
      "Epoch: 893,train_loss:0.0103134, Train: 1.0000, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 894,train_loss:0.0116139, Train: 1.0000, Val: 0.7820, Test: 0.7710\n",
      "Epoch: 895,train_loss:0.0057976, Train: 1.0000, Val: 0.7800, Test: 0.7700\n",
      "Epoch: 896,train_loss:0.0068611, Train: 1.0000, Val: 0.7860, Test: 0.7750\n",
      "Epoch: 897,train_loss:0.0062516, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 898,train_loss:0.0126160, Train: 1.0000, Val: 0.7880, Test: 0.7800\n",
      "Epoch: 899,train_loss:0.0095430, Train: 1.0000, Val: 0.7820, Test: 0.7790\n",
      "Epoch: 900,train_loss:0.0057135, Train: 1.0000, Val: 0.7760, Test: 0.7760\n",
      "Epoch: 901,train_loss:0.0069127, Train: 1.0000, Val: 0.7760, Test: 0.7760\n",
      "Epoch: 902,train_loss:0.0055460, Train: 1.0000, Val: 0.7760, Test: 0.7750\n",
      "Epoch: 903,train_loss:0.0115318, Train: 1.0000, Val: 0.7740, Test: 0.7690\n",
      "Epoch: 904,train_loss:0.0072060, Train: 1.0000, Val: 0.7760, Test: 0.7650\n",
      "Epoch: 905,train_loss:0.0175437, Train: 1.0000, Val: 0.7800, Test: 0.7690\n",
      "Epoch: 906,train_loss:0.0073950, Train: 1.0000, Val: 0.7880, Test: 0.7740\n",
      "Epoch: 907,train_loss:0.0093399, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 908,train_loss:0.0064277, Train: 1.0000, Val: 0.7860, Test: 0.7750\n",
      "Epoch: 909,train_loss:0.0056197, Train: 1.0000, Val: 0.7800, Test: 0.7780\n",
      "Epoch: 910,train_loss:0.0107475, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 911,train_loss:0.0167361, Train: 1.0000, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 912,train_loss:0.0050243, Train: 1.0000, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 913,train_loss:0.0093953, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 914,train_loss:0.0120615, Train: 1.0000, Val: 0.7840, Test: 0.7660\n",
      "Epoch: 915,train_loss:0.0052947, Train: 1.0000, Val: 0.7820, Test: 0.7670\n",
      "Epoch: 916,train_loss:0.0110092, Train: 1.0000, Val: 0.7840, Test: 0.7740\n",
      "Epoch: 917,train_loss:0.0071987, Train: 1.0000, Val: 0.7880, Test: 0.7770\n",
      "Epoch: 918,train_loss:0.0126711, Train: 1.0000, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 919,train_loss:0.0087587, Train: 1.0000, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 920,train_loss:0.0061557, Train: 1.0000, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 921,train_loss:0.0212318, Train: 1.0000, Val: 0.7840, Test: 0.7830\n",
      "Epoch: 922,train_loss:0.0045980, Train: 1.0000, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 923,train_loss:0.0047858, Train: 1.0000, Val: 0.7880, Test: 0.7780\n",
      "Epoch: 924,train_loss:0.0104186, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 925,train_loss:0.0125260, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 926,train_loss:0.0088907, Train: 1.0000, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 927,train_loss:0.0101052, Train: 1.0000, Val: 0.7900, Test: 0.7900\n",
      "Epoch: 928,train_loss:0.0110749, Train: 1.0000, Val: 0.7900, Test: 0.7920\n",
      "Epoch: 929,train_loss:0.0067628, Train: 1.0000, Val: 0.7860, Test: 0.7880\n",
      "Epoch: 930,train_loss:0.0067684, Train: 1.0000, Val: 0.7740, Test: 0.7850\n",
      "Epoch: 931,train_loss:0.0067569, Train: 1.0000, Val: 0.7760, Test: 0.7840\n",
      "Epoch: 932,train_loss:0.0099198, Train: 1.0000, Val: 0.7740, Test: 0.7840\n",
      "Epoch: 933,train_loss:0.0098676, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 934,train_loss:0.0102280, Train: 1.0000, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 935,train_loss:0.0104273, Train: 1.0000, Val: 0.7800, Test: 0.7830\n",
      "Epoch: 936,train_loss:0.0062796, Train: 1.0000, Val: 0.7800, Test: 0.7850\n",
      "Epoch: 937,train_loss:0.0090310, Train: 1.0000, Val: 0.7820, Test: 0.7830\n",
      "Epoch: 938,train_loss:0.0063116, Train: 1.0000, Val: 0.7840, Test: 0.7820\n",
      "Epoch: 939,train_loss:0.0068002, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 940,train_loss:0.0070304, Train: 1.0000, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 941,train_loss:0.0049522, Train: 1.0000, Val: 0.7880, Test: 0.7780\n",
      "Epoch: 942,train_loss:0.0075267, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 943,train_loss:0.0093103, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 944,train_loss:0.0069542, Train: 1.0000, Val: 0.7840, Test: 0.7800\n",
      "Epoch: 945,train_loss:0.0063511, Train: 1.0000, Val: 0.7860, Test: 0.7780\n",
      "Epoch: 946,train_loss:0.0056281, Train: 1.0000, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 947,train_loss:0.0090680, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 948,train_loss:0.0095079, Train: 1.0000, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 949,train_loss:0.0068282, Train: 1.0000, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 950,train_loss:0.0079581, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 951,train_loss:0.0096792, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 952,train_loss:0.0080591, Train: 1.0000, Val: 0.7840, Test: 0.7790\n",
      "Epoch: 953,train_loss:0.0072759, Train: 1.0000, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 954,train_loss:0.0063858, Train: 1.0000, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 955,train_loss:0.0080723, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 956,train_loss:0.0066721, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
      "Epoch: 957,train_loss:0.0038881, Train: 1.0000, Val: 0.7780, Test: 0.7770\n",
      "Epoch: 958,train_loss:0.0094892, Train: 1.0000, Val: 0.7760, Test: 0.7780\n",
      "Epoch: 959,train_loss:0.0062365, Train: 1.0000, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 960,train_loss:0.0094469, Train: 1.0000, Val: 0.7860, Test: 0.7720\n",
      "Epoch: 961,train_loss:0.0098493, Train: 1.0000, Val: 0.7800, Test: 0.7760\n",
      "Epoch: 962,train_loss:0.0059481, Train: 1.0000, Val: 0.7820, Test: 0.7730\n",
      "Epoch: 963,train_loss:0.0097882, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 964,train_loss:0.0063184, Train: 1.0000, Val: 0.7840, Test: 0.7740\n",
      "Epoch: 965,train_loss:0.0142019, Train: 1.0000, Val: 0.7800, Test: 0.7710\n",
      "Epoch: 966,train_loss:0.0069953, Train: 1.0000, Val: 0.7800, Test: 0.7730\n",
      "Epoch: 967,train_loss:0.0069730, Train: 1.0000, Val: 0.7840, Test: 0.7690\n",
      "Epoch: 968,train_loss:0.0069258, Train: 1.0000, Val: 0.7880, Test: 0.7690\n",
      "Epoch: 969,train_loss:0.0059758, Train: 1.0000, Val: 0.7840, Test: 0.7750\n",
      "Epoch: 970,train_loss:0.0062827, Train: 1.0000, Val: 0.7800, Test: 0.7790\n",
      "Epoch: 971,train_loss:0.0067178, Train: 1.0000, Val: 0.7780, Test: 0.7770\n",
      "Epoch: 972,train_loss:0.0104960, Train: 1.0000, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 973,train_loss:0.0093885, Train: 1.0000, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 974,train_loss:0.0078455, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 975,train_loss:0.0083682, Train: 1.0000, Val: 0.7900, Test: 0.7760\n",
      "Epoch: 976,train_loss:0.0064738, Train: 1.0000, Val: 0.7900, Test: 0.7750\n",
      "Epoch: 977,train_loss:0.0055676, Train: 1.0000, Val: 0.7880, Test: 0.7710\n",
      "Epoch: 978,train_loss:0.0078319, Train: 1.0000, Val: 0.7880, Test: 0.7730\n",
      "Epoch: 979,train_loss:0.0159985, Train: 1.0000, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 980,train_loss:0.0130670, Train: 1.0000, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 981,train_loss:0.0089891, Train: 1.0000, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 982,train_loss:0.0068995, Train: 1.0000, Val: 0.7820, Test: 0.7810\n",
      "Epoch: 983,train_loss:0.0083943, Train: 1.0000, Val: 0.7820, Test: 0.7830\n",
      "Epoch: 984,train_loss:0.0084400, Train: 1.0000, Val: 0.7900, Test: 0.7840\n",
      "Epoch: 985,train_loss:0.0077140, Train: 1.0000, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 986,train_loss:0.0060167, Train: 1.0000, Val: 0.7880, Test: 0.7810\n",
      "Epoch: 987,train_loss:0.0069872, Train: 1.0000, Val: 0.7920, Test: 0.7740\n",
      "Epoch: 988,train_loss:0.0141338, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 989,train_loss:0.0059969, Train: 1.0000, Val: 0.7840, Test: 0.7720\n",
      "Epoch: 990,train_loss:0.0057788, Train: 1.0000, Val: 0.7820, Test: 0.7680\n",
      "Epoch: 991,train_loss:0.0098979, Train: 1.0000, Val: 0.7800, Test: 0.7680\n",
      "Epoch: 992,train_loss:0.0087375, Train: 1.0000, Val: 0.7800, Test: 0.7700\n",
      "Epoch: 993,train_loss:0.0149770, Train: 1.0000, Val: 0.7800, Test: 0.7710\n",
      "Epoch: 994,train_loss:0.0075736, Train: 1.0000, Val: 0.7840, Test: 0.7750\n",
      "Epoch: 995,train_loss:0.0212114, Train: 1.0000, Val: 0.7900, Test: 0.7790\n",
      "Epoch: 996,train_loss:0.0084186, Train: 1.0000, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 997,train_loss:0.0162923, Train: 1.0000, Val: 0.7980, Test: 0.7810\n",
      "Epoch: 998,train_loss:0.0160153, Train: 1.0000, Val: 0.7960, Test: 0.7840\n",
      "Epoch: 999,train_loss:0.0147439, Train: 1.0000, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 1000,train_loss:0.0105697, Train: 1.0000, Val: 0.7900, Test: 0.7750\n",
      "CPU times: user 1min 2s, sys: 11 s, total: 1min 13s\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "457734"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Pubmed'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 3 #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "        # self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_pubmed_mymodel,testacclist_pubmed_mymodel=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_pubmed_mymodel.append(loss)\n",
    "    testacclist_pubmed_mymodel.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# # %matplotlib inline\n",
    "\n",
    "# plt.plot(losslist)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000pubmed\n",
    "Epoch: 953,train_loss:0.0053930, Train: 1.0000, Val: 0.7780, Test: 0.7650\n",
    "Epoch: 954,train_loss:0.0064971, Train: 1.0000, Val: 0.7780, Test: 0.7690\n",
    "Epoch: 955,train_loss:0.0062360, Train: 1.0000, Val: 0.7760, Test: 0.7690\n",
    "Epoch: 956,train_loss:0.0064282, Train: 1.0000, Val: 0.7800, Test: 0.7680\n",
    "Epoch: 957,train_loss:0.0059630, Train: 1.0000, Val: 0.7820, Test: 0.7660\n",
    "Epoch: 958,train_loss:0.0057412, Train: 1.0000, Val: 0.7880, Test: 0.7610\n",
    "Epoch: 959,train_loss:0.0070204, Train: 1.0000, Val: 0.7900, Test: 0.7640\n",
    "Epoch: 960,train_loss:0.0083166, Train: 1.0000, Val: 0.7920, Test: 0.7640\n",
    "Epoch: 961,train_loss:0.0066219, Train: 1.0000, Val: 0.7880, Test: 0.7670\n",
    "Epoch: 962,train_loss:0.0060282, Train: 1.0000, Val: 0.7860, Test: 0.7740\n",
    "Epoch: 963,train_loss:0.0080318, Train: 1.0000, Val: 0.7840, Test: 0.7710\n",
    "Epoch: 964,train_loss:0.0082156, Train: 1.0000, Val: 0.7780, Test: 0.7690\n",
    "Epoch: 965,train_loss:0.0075362, Train: 1.0000, Val: 0.7780, Test: 0.7690\n",
    "Epoch: 966,train_loss:0.0105927, Train: 1.0000, Val: 0.7780, Test: 0.7660\n",
    "Epoch: 967,train_loss:0.0077958, Train: 1.0000, Val: 0.7820, Test: 0.7650\n",
    "Epoch: 968,train_loss:0.0078789, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
    "Epoch: 969,train_loss:0.0089889, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
    "Epoch: 970,train_loss:0.0056428, Train: 1.0000, Val: 0.7840, Test: 0.7590\n",
    "Epoch: 971,train_loss:0.0057153, Train: 1.0000, Val: 0.7860, Test: 0.7610\n",
    "Epoch: 972,train_loss:0.0203651, Train: 1.0000, Val: 0.7820, Test: 0.7650\n",
    "Epoch: 973,train_loss:0.0103905, Train: 1.0000, Val: 0.7820, Test: 0.7720\n",
    "Epoch: 974,train_loss:0.0083350, Train: 1.0000, Val: 0.7780, Test: 0.7740\n",
    "Epoch: 975,train_loss:0.0184127, Train: 1.0000, Val: 0.7840, Test: 0.7700\n",
    "Epoch: 976,train_loss:0.0083088, Train: 1.0000, Val: 0.7900, Test: 0.7690\n",
    "Epoch: 977,train_loss:0.0063906, Train: 1.0000, Val: 0.7860, Test: 0.7660\n",
    "Epoch: 978,train_loss:0.0057222, Train: 1.0000, Val: 0.7860, Test: 0.7600\n",
    "Epoch: 979,train_loss:0.0058686, Train: 1.0000, Val: 0.7780, Test: 0.7550\n",
    "Epoch: 980,train_loss:0.0092487, Train: 1.0000, Val: 0.7780, Test: 0.7580\n",
    "Epoch: 981,train_loss:0.0098447, Train: 1.0000, Val: 0.7780, Test: 0.7670\n",
    "Epoch: 982,train_loss:0.0063194, Train: 1.0000, Val: 0.7920, Test: 0.7770\n",
    "Epoch: 983,train_loss:0.0091629, Train: 1.0000, Val: 0.8000, Test: 0.7780\n",
    "Epoch: 984,train_loss:0.0086891, Train: 1.0000, Val: 0.7960, Test: 0.7810\n",
    "Epoch: 985,train_loss:0.0057028, Train: 1.0000, Val: 0.7900, Test: 0.7770\n",
    "Epoch: 986,train_loss:0.0026957, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
    "Epoch: 987,train_loss:0.0140978, Train: 1.0000, Val: 0.7820, Test: 0.7770\n",
    "Epoch: 988,train_loss:0.0056254, Train: 1.0000, Val: 0.7800, Test: 0.7720\n",
    "Epoch: 989,train_loss:0.0121286, Train: 1.0000, Val: 0.7800, Test: 0.7710\n",
    "Epoch: 990,train_loss:0.0141445, Train: 1.0000, Val: 0.7880, Test: 0.7790\n",
    "Epoch: 991,train_loss:0.0080615, Train: 1.0000, Val: 0.7900, Test: 0.7770\n",
    "Epoch: 992,train_loss:0.0101339, Train: 1.0000, Val: 0.7760, Test: 0.7660\n",
    "Epoch: 993,train_loss:0.0132652, Train: 1.0000, Val: 0.7740, Test: 0.7520\n",
    "Epoch: 994,train_loss:0.0110487, Train: 1.0000, Val: 0.7760, Test: 0.7520\n",
    "Epoch: 995,train_loss:0.0126923, Train: 1.0000, Val: 0.7860, Test: 0.7730\n",
    "Epoch: 996,train_loss:0.0105120, Train: 1.0000, Val: 0.7940, Test: 0.7790\n",
    "Epoch: 997,train_loss:0.0083987, Train: 1.0000, Val: 0.7900, Test: 0.7720\n",
    "Epoch: 998,train_loss:0.0067350, Train: 1.0000, Val: 0.7520, Test: 0.7580\n",
    "Epoch: 999,train_loss:0.0056399, Train: 1.0000, Val: 0.7420, Test: 0.7470\n",
    "Epoch: 1000,train_loss:0.0237899, Train: 1.0000, Val: 0.7800, Test: 0.7710\n",
    "CPU times: user 1min 1s, sys: 11.8 s, total: 1min 12s\n",
    "Wall time: 1min 9s   \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sum([torch.numel(param) for param in model.parameters()])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.0987858, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 002,train_loss:1.0985430, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 003,train_loss:1.0986981, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 004,train_loss:1.0982529, Train: 0.3333, Val: 0.3920, Test: 0.4140\n",
      "Epoch: 005,train_loss:1.0985991, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 006,train_loss:1.0984746, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 007,train_loss:1.0983033, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 008,train_loss:1.0983410, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 009,train_loss:1.0980265, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 010,train_loss:1.0982641, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 011,train_loss:1.0977628, Train: 0.3333, Val: 0.3880, Test: 0.4130\n",
      "Epoch: 012,train_loss:1.0981852, Train: 0.3333, Val: 0.3920, Test: 0.4150\n",
      "Epoch: 013,train_loss:1.0981771, Train: 0.6500, Val: 0.5660, Test: 0.5280\n",
      "Epoch: 014,train_loss:1.0980396, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 015,train_loss:1.0985007, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 016,train_loss:1.0980351, Train: 0.3500, Val: 0.1980, Test: 0.1800\n",
      "Epoch: 017,train_loss:1.0965601, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 018,train_loss:1.0964429, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 019,train_loss:1.0955600, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 020,train_loss:1.0939002, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 021,train_loss:1.0956504, Train: 0.3333, Val: 0.1960, Test: 0.1800\n",
      "Epoch: 022,train_loss:1.0903242, Train: 0.3500, Val: 0.1980, Test: 0.1800\n",
      "Epoch: 023,train_loss:1.0732530, Train: 0.4833, Val: 0.2940, Test: 0.2830\n",
      "Epoch: 024,train_loss:1.0455048, Train: 0.6333, Val: 0.4700, Test: 0.4450\n",
      "Epoch: 025,train_loss:1.0212847, Train: 0.6000, Val: 0.5300, Test: 0.5250\n",
      "Epoch: 026,train_loss:0.8797382, Train: 0.6333, Val: 0.5220, Test: 0.5240\n",
      "Epoch: 027,train_loss:0.7255915, Train: 0.6167, Val: 0.5320, Test: 0.5130\n",
      "Epoch: 028,train_loss:0.6006550, Train: 0.6000, Val: 0.5140, Test: 0.4970\n",
      "Epoch: 029,train_loss:0.7505962, Train: 0.6333, Val: 0.5520, Test: 0.5210\n",
      "Epoch: 030,train_loss:0.6334899, Train: 0.6333, Val: 0.5400, Test: 0.5120\n",
      "Epoch: 031,train_loss:0.6612609, Train: 0.6333, Val: 0.5500, Test: 0.5160\n",
      "Epoch: 032,train_loss:0.5427573, Train: 0.7000, Val: 0.5700, Test: 0.5480\n",
      "Epoch: 033,train_loss:0.6607797, Train: 0.6500, Val: 0.5240, Test: 0.5610\n",
      "Epoch: 034,train_loss:0.6444147, Train: 0.6500, Val: 0.5300, Test: 0.5550\n",
      "Epoch: 035,train_loss:0.5625154, Train: 0.6667, Val: 0.5440, Test: 0.5560\n",
      "Epoch: 036,train_loss:0.5631249, Train: 0.6667, Val: 0.5360, Test: 0.5250\n",
      "Epoch: 037,train_loss:0.5249636, Train: 0.6667, Val: 0.5380, Test: 0.5190\n",
      "Epoch: 038,train_loss:0.4937129, Train: 0.7000, Val: 0.5460, Test: 0.5410\n",
      "Epoch: 039,train_loss:0.4942432, Train: 0.6833, Val: 0.5540, Test: 0.5350\n",
      "Epoch: 040,train_loss:0.5049841, Train: 0.7667, Val: 0.5920, Test: 0.5970\n",
      "Epoch: 041,train_loss:0.5253274, Train: 0.7667, Val: 0.5880, Test: 0.5970\n",
      "Epoch: 042,train_loss:0.4705841, Train: 0.7333, Val: 0.5460, Test: 0.5710\n",
      "Epoch: 043,train_loss:0.5045522, Train: 0.7500, Val: 0.5860, Test: 0.5850\n",
      "Epoch: 044,train_loss:0.5310000, Train: 0.7833, Val: 0.5820, Test: 0.5870\n",
      "Epoch: 045,train_loss:0.5065128, Train: 0.7667, Val: 0.5760, Test: 0.5860\n",
      "Epoch: 046,train_loss:0.5170335, Train: 0.7500, Val: 0.5420, Test: 0.5700\n",
      "Epoch: 047,train_loss:0.4737010, Train: 0.7167, Val: 0.5580, Test: 0.5540\n",
      "Epoch: 048,train_loss:0.4855195, Train: 0.7167, Val: 0.5640, Test: 0.5580\n",
      "Epoch: 049,train_loss:0.4802901, Train: 0.7833, Val: 0.5760, Test: 0.5720\n",
      "Epoch: 050,train_loss:0.4626206, Train: 0.8333, Val: 0.6000, Test: 0.6030\n",
      "Epoch: 051,train_loss:0.4875532, Train: 0.8500, Val: 0.6040, Test: 0.6110\n",
      "Epoch: 052,train_loss:0.4887280, Train: 0.8000, Val: 0.5800, Test: 0.5790\n",
      "Epoch: 053,train_loss:0.4578002, Train: 0.7333, Val: 0.5740, Test: 0.5640\n",
      "Epoch: 054,train_loss:0.4427589, Train: 0.7500, Val: 0.5820, Test: 0.5610\n",
      "Epoch: 055,train_loss:0.4788293, Train: 0.7500, Val: 0.5720, Test: 0.5670\n",
      "Epoch: 056,train_loss:0.4544470, Train: 0.8667, Val: 0.5720, Test: 0.6000\n",
      "Epoch: 057,train_loss:0.5204796, Train: 0.8667, Val: 0.6200, Test: 0.6100\n",
      "Epoch: 058,train_loss:0.4369150, Train: 0.8667, Val: 0.6180, Test: 0.6270\n",
      "Epoch: 059,train_loss:0.4242504, Train: 0.7500, Val: 0.5840, Test: 0.5810\n",
      "Epoch: 060,train_loss:0.4859229, Train: 0.7667, Val: 0.5880, Test: 0.5880\n",
      "Epoch: 061,train_loss:0.4156651, Train: 0.8833, Val: 0.6500, Test: 0.6430\n",
      "Epoch: 062,train_loss:0.3983829, Train: 0.8833, Val: 0.6220, Test: 0.5740\n",
      "Epoch: 063,train_loss:0.5120636, Train: 0.9333, Val: 0.6280, Test: 0.6210\n",
      "Epoch: 064,train_loss:0.4170436, Train: 0.9333, Val: 0.6540, Test: 0.6540\n",
      "Epoch: 065,train_loss:0.3710385, Train: 0.9333, Val: 0.6380, Test: 0.6400\n",
      "Epoch: 066,train_loss:0.3693520, Train: 0.9500, Val: 0.6340, Test: 0.6490\n",
      "Epoch: 067,train_loss:0.3322358, Train: 0.9500, Val: 0.6340, Test: 0.6330\n",
      "Epoch: 068,train_loss:0.3666615, Train: 0.9333, Val: 0.5980, Test: 0.5710\n",
      "Epoch: 069,train_loss:0.3486124, Train: 0.9500, Val: 0.5980, Test: 0.5750\n",
      "Epoch: 070,train_loss:0.4252945, Train: 0.9500, Val: 0.6400, Test: 0.6540\n",
      "Epoch: 071,train_loss:0.3621491, Train: 0.9667, Val: 0.6340, Test: 0.6510\n",
      "Epoch: 072,train_loss:0.3233538, Train: 0.9500, Val: 0.6400, Test: 0.6260\n",
      "Epoch: 073,train_loss:0.5336313, Train: 0.8500, Val: 0.5540, Test: 0.5260\n",
      "Epoch: 074,train_loss:0.3832972, Train: 0.9667, Val: 0.6200, Test: 0.6020\n",
      "Epoch: 075,train_loss:0.3113409, Train: 0.9500, Val: 0.6380, Test: 0.6500\n",
      "Epoch: 076,train_loss:0.2837724, Train: 0.8333, Val: 0.6140, Test: 0.6190\n",
      "Epoch: 077,train_loss:0.5733048, Train: 0.9500, Val: 0.6180, Test: 0.6300\n",
      "Epoch: 078,train_loss:0.2989502, Train: 0.8833, Val: 0.5660, Test: 0.5540\n",
      "Epoch: 079,train_loss:0.5556034, Train: 0.9833, Val: 0.6400, Test: 0.6440\n",
      "Epoch: 080,train_loss:0.2358759, Train: 0.9667, Val: 0.6560, Test: 0.6350\n",
      "Epoch: 081,train_loss:0.4802554, Train: 0.9000, Val: 0.6020, Test: 0.6210\n",
      "Epoch: 082,train_loss:0.6046602, Train: 1.0000, Val: 0.6720, Test: 0.6500\n",
      "Epoch: 083,train_loss:0.3896060, Train: 0.9667, Val: 0.6340, Test: 0.6210\n",
      "Epoch: 084,train_loss:0.2807529, Train: 0.9333, Val: 0.5800, Test: 0.5520\n",
      "Epoch: 085,train_loss:0.3069272, Train: 0.9500, Val: 0.5920, Test: 0.5920\n",
      "Epoch: 086,train_loss:0.4380687, Train: 1.0000, Val: 0.6560, Test: 0.6570\n",
      "Epoch: 087,train_loss:0.4696665, Train: 0.8833, Val: 0.6140, Test: 0.6250\n",
      "Epoch: 088,train_loss:0.3442340, Train: 0.8333, Val: 0.6060, Test: 0.6220\n",
      "Epoch: 089,train_loss:0.4235101, Train: 0.9833, Val: 0.6780, Test: 0.6720\n",
      "Epoch: 090,train_loss:0.2960952, Train: 0.9667, Val: 0.6320, Test: 0.6250\n",
      "Epoch: 091,train_loss:0.3620652, Train: 0.9333, Val: 0.6220, Test: 0.5940\n",
      "Epoch: 092,train_loss:0.2897614, Train: 0.9833, Val: 0.6500, Test: 0.6450\n",
      "Epoch: 093,train_loss:0.3674345, Train: 0.9833, Val: 0.7000, Test: 0.6840\n",
      "Epoch: 094,train_loss:0.3511004, Train: 0.9667, Val: 0.6960, Test: 0.6800\n",
      "Epoch: 095,train_loss:0.2170317, Train: 0.9667, Val: 0.6940, Test: 0.6840\n",
      "Epoch: 096,train_loss:0.2686158, Train: 1.0000, Val: 0.7040, Test: 0.6830\n",
      "Epoch: 097,train_loss:0.1699475, Train: 1.0000, Val: 0.6860, Test: 0.6710\n",
      "Epoch: 098,train_loss:0.2817784, Train: 1.0000, Val: 0.6720, Test: 0.6570\n",
      "Epoch: 099,train_loss:0.2923279, Train: 1.0000, Val: 0.6500, Test: 0.6490\n",
      "Epoch: 100,train_loss:0.1777799, Train: 1.0000, Val: 0.6760, Test: 0.6730\n",
      "Epoch: 101,train_loss:0.3414239, Train: 1.0000, Val: 0.7000, Test: 0.6960\n",
      "Epoch: 102,train_loss:0.2291166, Train: 1.0000, Val: 0.7020, Test: 0.6860\n",
      "Epoch: 103,train_loss:0.3110541, Train: 1.0000, Val: 0.6920, Test: 0.6810\n",
      "Epoch: 104,train_loss:0.2138973, Train: 1.0000, Val: 0.6680, Test: 0.6660\n",
      "Epoch: 105,train_loss:0.1760315, Train: 1.0000, Val: 0.6860, Test: 0.6780\n",
      "Epoch: 106,train_loss:0.2183900, Train: 1.0000, Val: 0.7040, Test: 0.6900\n",
      "Epoch: 107,train_loss:0.2318441, Train: 1.0000, Val: 0.6900, Test: 0.6880\n",
      "Epoch: 108,train_loss:0.2973166, Train: 1.0000, Val: 0.6880, Test: 0.6800\n",
      "Epoch: 109,train_loss:0.1543571, Train: 1.0000, Val: 0.6620, Test: 0.6640\n",
      "Epoch: 110,train_loss:0.1219896, Train: 1.0000, Val: 0.6860, Test: 0.6740\n",
      "Epoch: 111,train_loss:0.1630692, Train: 1.0000, Val: 0.6960, Test: 0.6850\n",
      "Epoch: 112,train_loss:0.2044293, Train: 1.0000, Val: 0.7100, Test: 0.6880\n",
      "Epoch: 113,train_loss:0.1107984, Train: 1.0000, Val: 0.7080, Test: 0.6900\n",
      "Epoch: 114,train_loss:0.1020503, Train: 1.0000, Val: 0.6980, Test: 0.6920\n",
      "Epoch: 115,train_loss:0.0591668, Train: 1.0000, Val: 0.6920, Test: 0.6840\n",
      "Epoch: 116,train_loss:0.2455505, Train: 1.0000, Val: 0.6740, Test: 0.6760\n",
      "Epoch: 117,train_loss:0.1606519, Train: 1.0000, Val: 0.6880, Test: 0.6870\n",
      "Epoch: 118,train_loss:0.0754961, Train: 0.9833, Val: 0.7140, Test: 0.6820\n",
      "Epoch: 119,train_loss:0.1752537, Train: 0.9833, Val: 0.7100, Test: 0.6860\n",
      "Epoch: 120,train_loss:0.1689033, Train: 0.9833, Val: 0.7120, Test: 0.6930\n",
      "Epoch: 121,train_loss:0.1092744, Train: 1.0000, Val: 0.7080, Test: 0.6870\n",
      "Epoch: 122,train_loss:0.1030323, Train: 1.0000, Val: 0.6780, Test: 0.6720\n",
      "Epoch: 123,train_loss:0.0790240, Train: 0.9667, Val: 0.6600, Test: 0.6550\n",
      "Epoch: 124,train_loss:0.3506062, Train: 1.0000, Val: 0.7080, Test: 0.6880\n",
      "Epoch: 125,train_loss:0.1177238, Train: 1.0000, Val: 0.7180, Test: 0.7020\n",
      "Epoch: 126,train_loss:0.0572776, Train: 0.9833, Val: 0.7280, Test: 0.7080\n",
      "Epoch: 127,train_loss:0.0821495, Train: 0.9833, Val: 0.7240, Test: 0.6950\n",
      "Epoch: 128,train_loss:0.1485897, Train: 0.9833, Val: 0.7340, Test: 0.7020\n",
      "Epoch: 129,train_loss:0.1426937, Train: 0.9833, Val: 0.7420, Test: 0.7160\n",
      "Epoch: 130,train_loss:0.0628612, Train: 1.0000, Val: 0.7380, Test: 0.7220\n",
      "Epoch: 131,train_loss:0.0742662, Train: 1.0000, Val: 0.7360, Test: 0.7140\n",
      "Epoch: 132,train_loss:0.0707197, Train: 1.0000, Val: 0.6840, Test: 0.6620\n",
      "Epoch: 133,train_loss:0.1078556, Train: 1.0000, Val: 0.7320, Test: 0.7160\n",
      "Epoch: 134,train_loss:0.0500517, Train: 0.9833, Val: 0.7280, Test: 0.7090\n",
      "Epoch: 135,train_loss:0.0575364, Train: 0.9833, Val: 0.6900, Test: 0.6950\n",
      "Epoch: 136,train_loss:0.1435270, Train: 0.9833, Val: 0.7320, Test: 0.7170\n",
      "Epoch: 137,train_loss:0.0436710, Train: 1.0000, Val: 0.7400, Test: 0.7270\n",
      "Epoch: 138,train_loss:0.1005411, Train: 1.0000, Val: 0.7400, Test: 0.7200\n",
      "Epoch: 139,train_loss:0.0519680, Train: 1.0000, Val: 0.7200, Test: 0.7030\n",
      "Epoch: 140,train_loss:0.0780762, Train: 1.0000, Val: 0.7240, Test: 0.7150\n",
      "Epoch: 141,train_loss:0.0479879, Train: 1.0000, Val: 0.7600, Test: 0.7280\n",
      "Epoch: 142,train_loss:0.0823765, Train: 1.0000, Val: 0.7860, Test: 0.7540\n",
      "Epoch: 143,train_loss:0.0862355, Train: 0.9833, Val: 0.7660, Test: 0.7510\n",
      "Epoch: 144,train_loss:0.1672398, Train: 0.9833, Val: 0.7760, Test: 0.7540\n",
      "Epoch: 145,train_loss:0.0856787, Train: 0.9833, Val: 0.7780, Test: 0.7630\n",
      "Epoch: 146,train_loss:0.0616753, Train: 1.0000, Val: 0.7900, Test: 0.7600\n",
      "Epoch: 147,train_loss:0.0444048, Train: 1.0000, Val: 0.7620, Test: 0.7360\n",
      "Epoch: 148,train_loss:0.0313969, Train: 1.0000, Val: 0.7100, Test: 0.6870\n",
      "Epoch: 149,train_loss:0.1277252, Train: 1.0000, Val: 0.7460, Test: 0.7200\n",
      "Epoch: 150,train_loss:0.0821597, Train: 1.0000, Val: 0.7820, Test: 0.7610\n",
      "Epoch: 151,train_loss:0.0102170, Train: 1.0000, Val: 0.7960, Test: 0.7710\n",
      "Epoch: 152,train_loss:0.0116216, Train: 1.0000, Val: 0.7920, Test: 0.7670\n",
      "Epoch: 153,train_loss:0.0276337, Train: 1.0000, Val: 0.7900, Test: 0.7680\n",
      "Epoch: 154,train_loss:0.0358198, Train: 1.0000, Val: 0.7860, Test: 0.7700\n",
      "Epoch: 155,train_loss:0.0426830, Train: 1.0000, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 156,train_loss:0.0076332, Train: 1.0000, Val: 0.7520, Test: 0.7390\n",
      "Epoch: 157,train_loss:0.0266784, Train: 1.0000, Val: 0.7300, Test: 0.7250\n",
      "Epoch: 158,train_loss:0.0155536, Train: 1.0000, Val: 0.7260, Test: 0.7130\n",
      "Epoch: 159,train_loss:0.0582816, Train: 1.0000, Val: 0.7320, Test: 0.7120\n",
      "Epoch: 160,train_loss:0.0676164, Train: 1.0000, Val: 0.7400, Test: 0.7330\n",
      "Epoch: 161,train_loss:0.0662490, Train: 1.0000, Val: 0.7660, Test: 0.7390\n",
      "Epoch: 162,train_loss:0.0237364, Train: 1.0000, Val: 0.7820, Test: 0.7490\n",
      "Epoch: 163,train_loss:0.0145839, Train: 1.0000, Val: 0.7600, Test: 0.7480\n",
      "Epoch: 164,train_loss:0.0339323, Train: 1.0000, Val: 0.7580, Test: 0.7430\n",
      "Epoch: 165,train_loss:0.0793115, Train: 1.0000, Val: 0.7580, Test: 0.7480\n",
      "Epoch: 166,train_loss:0.0541286, Train: 1.0000, Val: 0.7760, Test: 0.7570\n",
      "Epoch: 167,train_loss:0.0265718, Train: 1.0000, Val: 0.7840, Test: 0.7620\n",
      "Epoch: 168,train_loss:0.0186205, Train: 1.0000, Val: 0.7760, Test: 0.7590\n",
      "Epoch: 169,train_loss:0.0222742, Train: 1.0000, Val: 0.7640, Test: 0.7510\n",
      "Epoch: 170,train_loss:0.0405913, Train: 1.0000, Val: 0.7580, Test: 0.7450\n",
      "Epoch: 171,train_loss:0.0084625, Train: 1.0000, Val: 0.7520, Test: 0.7500\n",
      "Epoch: 172,train_loss:0.0154008, Train: 1.0000, Val: 0.7500, Test: 0.7410\n",
      "Epoch: 173,train_loss:0.0478091, Train: 1.0000, Val: 0.7420, Test: 0.7380\n",
      "Epoch: 174,train_loss:0.0369220, Train: 1.0000, Val: 0.7420, Test: 0.7390\n",
      "Epoch: 175,train_loss:0.0273929, Train: 1.0000, Val: 0.7620, Test: 0.7430\n",
      "Epoch: 176,train_loss:0.0470602, Train: 1.0000, Val: 0.7600, Test: 0.7510\n",
      "Epoch: 177,train_loss:0.0065577, Train: 1.0000, Val: 0.7640, Test: 0.7500\n",
      "Epoch: 178,train_loss:0.0110702, Train: 1.0000, Val: 0.7500, Test: 0.7380\n",
      "Epoch: 179,train_loss:0.0386906, Train: 1.0000, Val: 0.7460, Test: 0.7340\n",
      "Epoch: 180,train_loss:0.0246420, Train: 1.0000, Val: 0.7480, Test: 0.7390\n",
      "Epoch: 181,train_loss:0.0195179, Train: 1.0000, Val: 0.7440, Test: 0.7390\n",
      "Epoch: 182,train_loss:0.0244414, Train: 1.0000, Val: 0.7460, Test: 0.7340\n",
      "Epoch: 183,train_loss:0.0567034, Train: 1.0000, Val: 0.7520, Test: 0.7450\n",
      "Epoch: 184,train_loss:0.0076152, Train: 1.0000, Val: 0.7560, Test: 0.7530\n",
      "Epoch: 185,train_loss:0.0576228, Train: 1.0000, Val: 0.7800, Test: 0.7550\n",
      "Epoch: 186,train_loss:0.0044660, Train: 1.0000, Val: 0.7860, Test: 0.7600\n",
      "Epoch: 187,train_loss:0.0271201, Train: 1.0000, Val: 0.7760, Test: 0.7620\n",
      "Epoch: 188,train_loss:0.0184516, Train: 1.0000, Val: 0.7620, Test: 0.7580\n",
      "Epoch: 189,train_loss:0.0251183, Train: 1.0000, Val: 0.7660, Test: 0.7540\n",
      "Epoch: 190,train_loss:0.0469388, Train: 1.0000, Val: 0.7660, Test: 0.7560\n",
      "Epoch: 191,train_loss:0.0072491, Train: 1.0000, Val: 0.7640, Test: 0.7550\n",
      "Epoch: 192,train_loss:0.0246513, Train: 1.0000, Val: 0.7600, Test: 0.7540\n",
      "Epoch: 193,train_loss:0.0079739, Train: 1.0000, Val: 0.7560, Test: 0.7500\n",
      "Epoch: 194,train_loss:0.0075925, Train: 1.0000, Val: 0.7580, Test: 0.7510\n",
      "Epoch: 195,train_loss:0.0301439, Train: 1.0000, Val: 0.7620, Test: 0.7580\n",
      "Epoch: 196,train_loss:0.0109535, Train: 1.0000, Val: 0.7680, Test: 0.7560\n",
      "Epoch: 197,train_loss:0.0226853, Train: 1.0000, Val: 0.7660, Test: 0.7570\n",
      "Epoch: 198,train_loss:0.0396653, Train: 1.0000, Val: 0.7540, Test: 0.7470\n",
      "Epoch: 199,train_loss:0.0442147, Train: 1.0000, Val: 0.7460, Test: 0.7490\n",
      "Epoch: 200,train_loss:0.0602749, Train: 1.0000, Val: 0.7280, Test: 0.7430\n",
      "Epoch: 201,train_loss:0.0335521, Train: 1.0000, Val: 0.7300, Test: 0.7350\n",
      "Epoch: 202,train_loss:0.0540533, Train: 1.0000, Val: 0.7260, Test: 0.7270\n",
      "Epoch: 203,train_loss:0.0627535, Train: 1.0000, Val: 0.7240, Test: 0.7280\n",
      "Epoch: 204,train_loss:0.0488412, Train: 1.0000, Val: 0.7420, Test: 0.7350\n",
      "Epoch: 205,train_loss:0.0386872, Train: 1.0000, Val: 0.7640, Test: 0.7530\n",
      "Epoch: 206,train_loss:0.0087168, Train: 1.0000, Val: 0.7600, Test: 0.7520\n",
      "Epoch: 207,train_loss:0.0156790, Train: 1.0000, Val: 0.7660, Test: 0.7540\n",
      "Epoch: 208,train_loss:0.0705817, Train: 1.0000, Val: 0.7780, Test: 0.7560\n",
      "Epoch: 209,train_loss:0.1373227, Train: 1.0000, Val: 0.7700, Test: 0.7510\n",
      "Epoch: 210,train_loss:0.0461394, Train: 1.0000, Val: 0.7700, Test: 0.7550\n",
      "Epoch: 211,train_loss:0.0273808, Train: 1.0000, Val: 0.7720, Test: 0.7510\n",
      "Epoch: 212,train_loss:0.0049374, Train: 1.0000, Val: 0.7740, Test: 0.7500\n",
      "Epoch: 213,train_loss:0.0692231, Train: 1.0000, Val: 0.7680, Test: 0.7580\n",
      "Epoch: 214,train_loss:0.0092446, Train: 1.0000, Val: 0.7540, Test: 0.7420\n",
      "Epoch: 215,train_loss:0.0335558, Train: 1.0000, Val: 0.7320, Test: 0.7360\n",
      "Epoch: 216,train_loss:0.0767215, Train: 1.0000, Val: 0.7260, Test: 0.7320\n",
      "Epoch: 217,train_loss:0.0848383, Train: 1.0000, Val: 0.7300, Test: 0.7300\n",
      "Epoch: 218,train_loss:0.0485714, Train: 1.0000, Val: 0.7520, Test: 0.7430\n",
      "Epoch: 219,train_loss:0.0443192, Train: 1.0000, Val: 0.7760, Test: 0.7680\n",
      "Epoch: 220,train_loss:0.0288885, Train: 1.0000, Val: 0.7800, Test: 0.7640\n",
      "Epoch: 221,train_loss:0.0363135, Train: 1.0000, Val: 0.7780, Test: 0.7470\n",
      "Epoch: 222,train_loss:0.0381838, Train: 1.0000, Val: 0.7620, Test: 0.7330\n",
      "Epoch: 223,train_loss:0.0827126, Train: 1.0000, Val: 0.7700, Test: 0.7430\n",
      "Epoch: 224,train_loss:0.0341481, Train: 1.0000, Val: 0.7880, Test: 0.7630\n",
      "Epoch: 225,train_loss:0.0126838, Train: 1.0000, Val: 0.7940, Test: 0.7660\n",
      "Epoch: 226,train_loss:0.0071926, Train: 1.0000, Val: 0.7840, Test: 0.7630\n",
      "Epoch: 227,train_loss:0.1382293, Train: 1.0000, Val: 0.8000, Test: 0.7690\n",
      "Epoch: 228,train_loss:0.0572020, Train: 1.0000, Val: 0.7780, Test: 0.7520\n",
      "Epoch: 229,train_loss:0.0074623, Train: 1.0000, Val: 0.7360, Test: 0.7210\n",
      "Epoch: 230,train_loss:0.0316847, Train: 1.0000, Val: 0.6580, Test: 0.6680\n",
      "Epoch: 231,train_loss:0.0381998, Train: 1.0000, Val: 0.6580, Test: 0.6550\n",
      "Epoch: 232,train_loss:0.0450960, Train: 1.0000, Val: 0.7000, Test: 0.6990\n",
      "Epoch: 233,train_loss:0.0482211, Train: 1.0000, Val: 0.7520, Test: 0.7270\n",
      "Epoch: 234,train_loss:0.0171746, Train: 1.0000, Val: 0.7560, Test: 0.7380\n",
      "Epoch: 235,train_loss:0.0862495, Train: 1.0000, Val: 0.7460, Test: 0.7400\n",
      "Epoch: 236,train_loss:0.0913402, Train: 1.0000, Val: 0.7340, Test: 0.7450\n",
      "Epoch: 237,train_loss:0.0394601, Train: 1.0000, Val: 0.7340, Test: 0.7440\n",
      "Epoch: 238,train_loss:0.1005879, Train: 1.0000, Val: 0.7380, Test: 0.7420\n",
      "Epoch: 239,train_loss:0.0434735, Train: 1.0000, Val: 0.7520, Test: 0.7500\n",
      "Epoch: 240,train_loss:0.0295375, Train: 1.0000, Val: 0.7680, Test: 0.7620\n",
      "Epoch: 241,train_loss:0.0137325, Train: 1.0000, Val: 0.7720, Test: 0.7560\n",
      "Epoch: 242,train_loss:0.0111958, Train: 1.0000, Val: 0.7780, Test: 0.7540\n",
      "Epoch: 243,train_loss:0.0303542, Train: 1.0000, Val: 0.7820, Test: 0.7660\n",
      "Epoch: 244,train_loss:0.0210011, Train: 1.0000, Val: 0.7840, Test: 0.7660\n",
      "Epoch: 245,train_loss:0.0204546, Train: 1.0000, Val: 0.7840, Test: 0.7570\n",
      "Epoch: 246,train_loss:0.0125718, Train: 1.0000, Val: 0.7780, Test: 0.7540\n",
      "Epoch: 247,train_loss:0.0020968, Train: 1.0000, Val: 0.7820, Test: 0.7480\n",
      "Epoch: 248,train_loss:0.0298051, Train: 1.0000, Val: 0.7780, Test: 0.7480\n",
      "Epoch: 249,train_loss:0.0317240, Train: 1.0000, Val: 0.7800, Test: 0.7550\n",
      "Epoch: 250,train_loss:0.0119241, Train: 1.0000, Val: 0.7880, Test: 0.7640\n",
      "Epoch: 251,train_loss:0.0016647, Train: 1.0000, Val: 0.7840, Test: 0.7650\n",
      "Epoch: 252,train_loss:0.0037656, Train: 1.0000, Val: 0.7760, Test: 0.7630\n",
      "Epoch: 253,train_loss:0.0050851, Train: 1.0000, Val: 0.7780, Test: 0.7550\n",
      "Epoch: 254,train_loss:0.0212292, Train: 1.0000, Val: 0.7640, Test: 0.7430\n",
      "Epoch: 255,train_loss:0.0280159, Train: 1.0000, Val: 0.7560, Test: 0.7430\n",
      "Epoch: 256,train_loss:0.0066426, Train: 1.0000, Val: 0.7560, Test: 0.7400\n",
      "Epoch: 257,train_loss:0.0422071, Train: 1.0000, Val: 0.7560, Test: 0.7380\n",
      "Epoch: 258,train_loss:0.0186439, Train: 1.0000, Val: 0.7600, Test: 0.7480\n",
      "Epoch: 259,train_loss:0.0041049, Train: 1.0000, Val: 0.7620, Test: 0.7490\n",
      "Epoch: 260,train_loss:0.0146798, Train: 1.0000, Val: 0.7640, Test: 0.7520\n",
      "Epoch: 261,train_loss:0.0604682, Train: 1.0000, Val: 0.7660, Test: 0.7440\n",
      "Epoch: 262,train_loss:0.0231433, Train: 1.0000, Val: 0.7680, Test: 0.7520\n",
      "Epoch: 263,train_loss:0.0029929, Train: 1.0000, Val: 0.7740, Test: 0.7490\n",
      "Epoch: 264,train_loss:0.0765144, Train: 1.0000, Val: 0.7720, Test: 0.7600\n",
      "Epoch: 265,train_loss:0.0251166, Train: 1.0000, Val: 0.7800, Test: 0.7570\n",
      "Epoch: 266,train_loss:0.0406348, Train: 1.0000, Val: 0.7780, Test: 0.7520\n",
      "Epoch: 267,train_loss:0.0146924, Train: 1.0000, Val: 0.7720, Test: 0.7540\n",
      "Epoch: 268,train_loss:0.0331980, Train: 1.0000, Val: 0.7660, Test: 0.7530\n",
      "Epoch: 269,train_loss:0.0256241, Train: 1.0000, Val: 0.7680, Test: 0.7540\n",
      "Epoch: 270,train_loss:0.0198976, Train: 1.0000, Val: 0.7700, Test: 0.7480\n",
      "Epoch: 271,train_loss:0.0885289, Train: 1.0000, Val: 0.7760, Test: 0.7600\n",
      "Epoch: 272,train_loss:0.1107488, Train: 1.0000, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 273,train_loss:0.0587365, Train: 1.0000, Val: 0.7840, Test: 0.7660\n",
      "Epoch: 274,train_loss:0.0175268, Train: 1.0000, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 275,train_loss:0.0183205, Train: 1.0000, Val: 0.7560, Test: 0.7520\n",
      "Epoch: 276,train_loss:0.0224233, Train: 1.0000, Val: 0.7560, Test: 0.7530\n",
      "Epoch: 277,train_loss:0.0134061, Train: 1.0000, Val: 0.7560, Test: 0.7520\n",
      "Epoch: 278,train_loss:0.0571026, Train: 1.0000, Val: 0.7660, Test: 0.7650\n",
      "Epoch: 279,train_loss:0.0253416, Train: 1.0000, Val: 0.7600, Test: 0.7520\n",
      "Epoch: 280,train_loss:0.0734608, Train: 1.0000, Val: 0.7380, Test: 0.7200\n",
      "Epoch: 281,train_loss:0.0229966, Train: 1.0000, Val: 0.6500, Test: 0.6640\n",
      "Epoch: 282,train_loss:0.0250097, Train: 1.0000, Val: 0.6140, Test: 0.6140\n",
      "Epoch: 283,train_loss:0.0613291, Train: 1.0000, Val: 0.6560, Test: 0.6590\n",
      "Epoch: 284,train_loss:0.1010856, Train: 1.0000, Val: 0.7340, Test: 0.7230\n",
      "Epoch: 285,train_loss:0.0143199, Train: 1.0000, Val: 0.7480, Test: 0.7530\n",
      "Epoch: 286,train_loss:0.0220425, Train: 1.0000, Val: 0.7540, Test: 0.7600\n",
      "Epoch: 287,train_loss:0.0394388, Train: 1.0000, Val: 0.7700, Test: 0.7500\n",
      "Epoch: 288,train_loss:0.0414304, Train: 1.0000, Val: 0.7620, Test: 0.7420\n",
      "Epoch: 289,train_loss:0.1214090, Train: 1.0000, Val: 0.7700, Test: 0.7470\n",
      "Epoch: 290,train_loss:0.0315616, Train: 1.0000, Val: 0.7680, Test: 0.7600\n",
      "Epoch: 291,train_loss:0.0103196, Train: 1.0000, Val: 0.7780, Test: 0.7570\n",
      "Epoch: 292,train_loss:0.0308553, Train: 1.0000, Val: 0.7820, Test: 0.7590\n",
      "Epoch: 293,train_loss:0.0935973, Train: 1.0000, Val: 0.7820, Test: 0.7550\n",
      "Epoch: 294,train_loss:0.0237485, Train: 1.0000, Val: 0.7800, Test: 0.7530\n",
      "Epoch: 295,train_loss:0.0083893, Train: 1.0000, Val: 0.7440, Test: 0.7330\n",
      "Epoch: 296,train_loss:0.0654358, Train: 1.0000, Val: 0.7520, Test: 0.7360\n",
      "Epoch: 297,train_loss:0.0087145, Train: 1.0000, Val: 0.7500, Test: 0.7280\n",
      "Epoch: 298,train_loss:0.0607094, Train: 1.0000, Val: 0.7420, Test: 0.7290\n",
      "Epoch: 299,train_loss:0.0109765, Train: 1.0000, Val: 0.7500, Test: 0.7280\n",
      "Epoch: 300,train_loss:0.0374011, Train: 1.0000, Val: 0.7520, Test: 0.7380\n",
      "Epoch: 301,train_loss:0.0202040, Train: 1.0000, Val: 0.7460, Test: 0.7290\n",
      "Epoch: 302,train_loss:0.0105824, Train: 1.0000, Val: 0.7440, Test: 0.7220\n",
      "Epoch: 303,train_loss:0.0327294, Train: 1.0000, Val: 0.7320, Test: 0.7160\n",
      "Epoch: 304,train_loss:0.1119296, Train: 1.0000, Val: 0.7400, Test: 0.7220\n",
      "Epoch: 305,train_loss:0.0108931, Train: 1.0000, Val: 0.7440, Test: 0.7320\n",
      "Epoch: 306,train_loss:0.0183361, Train: 1.0000, Val: 0.7520, Test: 0.7380\n",
      "Epoch: 307,train_loss:0.0593050, Train: 1.0000, Val: 0.7540, Test: 0.7370\n",
      "Epoch: 308,train_loss:0.0076574, Train: 1.0000, Val: 0.7540, Test: 0.7420\n",
      "Epoch: 309,train_loss:0.0763452, Train: 1.0000, Val: 0.7580, Test: 0.7440\n",
      "Epoch: 310,train_loss:0.0134079, Train: 1.0000, Val: 0.7560, Test: 0.7400\n",
      "Epoch: 311,train_loss:0.0801332, Train: 1.0000, Val: 0.7600, Test: 0.7440\n",
      "Epoch: 312,train_loss:0.0342608, Train: 1.0000, Val: 0.7580, Test: 0.7490\n",
      "Epoch: 313,train_loss:0.0468473, Train: 1.0000, Val: 0.7600, Test: 0.7530\n",
      "Epoch: 314,train_loss:0.0301517, Train: 1.0000, Val: 0.7640, Test: 0.7510\n",
      "Epoch: 315,train_loss:0.0852111, Train: 1.0000, Val: 0.7680, Test: 0.7570\n",
      "Epoch: 316,train_loss:0.0089226, Train: 1.0000, Val: 0.7680, Test: 0.7580\n",
      "Epoch: 317,train_loss:0.0262665, Train: 1.0000, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 318,train_loss:0.0209211, Train: 1.0000, Val: 0.7600, Test: 0.7540\n",
      "Epoch: 319,train_loss:0.0195144, Train: 1.0000, Val: 0.7500, Test: 0.7510\n",
      "Epoch: 320,train_loss:0.0057370, Train: 1.0000, Val: 0.7700, Test: 0.7430\n",
      "Epoch: 321,train_loss:0.0424002, Train: 1.0000, Val: 0.7640, Test: 0.7320\n",
      "Epoch: 322,train_loss:0.0071748, Train: 1.0000, Val: 0.7500, Test: 0.7230\n",
      "Epoch: 323,train_loss:0.0122479, Train: 1.0000, Val: 0.7620, Test: 0.7290\n",
      "Epoch: 324,train_loss:0.0374618, Train: 1.0000, Val: 0.7740, Test: 0.7480\n",
      "Epoch: 325,train_loss:0.0107669, Train: 1.0000, Val: 0.7780, Test: 0.7570\n",
      "Epoch: 326,train_loss:0.0204958, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
      "Epoch: 327,train_loss:0.0238874, Train: 1.0000, Val: 0.7900, Test: 0.7640\n",
      "Epoch: 328,train_loss:0.0397959, Train: 1.0000, Val: 0.7900, Test: 0.7690\n",
      "Epoch: 329,train_loss:0.0081201, Train: 1.0000, Val: 0.7820, Test: 0.7640\n",
      "Epoch: 330,train_loss:0.0654448, Train: 1.0000, Val: 0.7660, Test: 0.7470\n",
      "Epoch: 331,train_loss:0.0447430, Train: 0.9833, Val: 0.7520, Test: 0.7330\n",
      "Epoch: 332,train_loss:0.0438488, Train: 1.0000, Val: 0.7760, Test: 0.7610\n",
      "Epoch: 333,train_loss:0.0044504, Train: 1.0000, Val: 0.7400, Test: 0.7290\n",
      "Epoch: 334,train_loss:0.0591025, Train: 1.0000, Val: 0.7400, Test: 0.7350\n",
      "Epoch: 335,train_loss:0.0061215, Train: 1.0000, Val: 0.7380, Test: 0.7290\n",
      "Epoch: 336,train_loss:0.1440897, Train: 1.0000, Val: 0.7440, Test: 0.7420\n",
      "Epoch: 337,train_loss:0.0584031, Train: 1.0000, Val: 0.7520, Test: 0.7460\n",
      "Epoch: 338,train_loss:0.0429048, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 339,train_loss:0.0870449, Train: 1.0000, Val: 0.7660, Test: 0.7450\n",
      "Epoch: 340,train_loss:0.0427065, Train: 1.0000, Val: 0.7700, Test: 0.7500\n",
      "Epoch: 341,train_loss:0.0612466, Train: 1.0000, Val: 0.7760, Test: 0.7560\n",
      "Epoch: 342,train_loss:0.0396791, Train: 1.0000, Val: 0.7840, Test: 0.7600\n",
      "Epoch: 343,train_loss:0.0065512, Train: 1.0000, Val: 0.7880, Test: 0.7590\n",
      "Epoch: 344,train_loss:0.0268153, Train: 1.0000, Val: 0.7820, Test: 0.7600\n",
      "Epoch: 345,train_loss:0.0100523, Train: 1.0000, Val: 0.7880, Test: 0.7630\n",
      "Epoch: 346,train_loss:0.0418584, Train: 1.0000, Val: 0.7920, Test: 0.7630\n",
      "Epoch: 347,train_loss:0.0325709, Train: 1.0000, Val: 0.7800, Test: 0.7690\n",
      "Epoch: 348,train_loss:0.0279982, Train: 1.0000, Val: 0.7620, Test: 0.7660\n",
      "Epoch: 349,train_loss:0.0319430, Train: 1.0000, Val: 0.7580, Test: 0.7560\n",
      "Epoch: 350,train_loss:0.0207234, Train: 1.0000, Val: 0.7560, Test: 0.7550\n",
      "Epoch: 351,train_loss:0.0271604, Train: 1.0000, Val: 0.7600, Test: 0.7540\n",
      "Epoch: 352,train_loss:0.0165412, Train: 1.0000, Val: 0.7580, Test: 0.7540\n",
      "Epoch: 353,train_loss:0.0262823, Train: 1.0000, Val: 0.7560, Test: 0.7550\n",
      "Epoch: 354,train_loss:0.0172305, Train: 1.0000, Val: 0.7560, Test: 0.7610\n",
      "Epoch: 355,train_loss:0.0465053, Train: 1.0000, Val: 0.7840, Test: 0.7640\n",
      "Epoch: 356,train_loss:0.0294261, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 357,train_loss:0.0355254, Train: 1.0000, Val: 0.7760, Test: 0.7620\n",
      "Epoch: 358,train_loss:0.0053201, Train: 1.0000, Val: 0.7720, Test: 0.7660\n",
      "Epoch: 359,train_loss:0.0080386, Train: 1.0000, Val: 0.7700, Test: 0.7660\n",
      "Epoch: 360,train_loss:0.0140977, Train: 1.0000, Val: 0.7760, Test: 0.7650\n",
      "Epoch: 361,train_loss:0.0261410, Train: 1.0000, Val: 0.7700, Test: 0.7690\n",
      "Epoch: 362,train_loss:0.0313024, Train: 1.0000, Val: 0.7660, Test: 0.7660\n",
      "Epoch: 363,train_loss:0.0695128, Train: 1.0000, Val: 0.7740, Test: 0.7640\n",
      "Epoch: 364,train_loss:0.0247273, Train: 1.0000, Val: 0.7680, Test: 0.7650\n",
      "Epoch: 365,train_loss:0.0642289, Train: 1.0000, Val: 0.7840, Test: 0.7590\n",
      "Epoch: 366,train_loss:0.0242697, Train: 1.0000, Val: 0.7660, Test: 0.7540\n",
      "Epoch: 367,train_loss:0.0434048, Train: 1.0000, Val: 0.7660, Test: 0.7530\n",
      "Epoch: 368,train_loss:0.0507988, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 369,train_loss:0.0026486, Train: 1.0000, Val: 0.7520, Test: 0.7340\n",
      "Epoch: 370,train_loss:0.0342571, Train: 1.0000, Val: 0.7560, Test: 0.7560\n",
      "Epoch: 371,train_loss:0.0234769, Train: 1.0000, Val: 0.7720, Test: 0.7580\n",
      "Epoch: 372,train_loss:0.0422919, Train: 1.0000, Val: 0.7940, Test: 0.7640\n",
      "Epoch: 373,train_loss:0.0103836, Train: 1.0000, Val: 0.7900, Test: 0.7650\n",
      "Epoch: 374,train_loss:0.0141395, Train: 1.0000, Val: 0.7720, Test: 0.7630\n",
      "Epoch: 375,train_loss:0.0278972, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
      "Epoch: 376,train_loss:0.0592066, Train: 1.0000, Val: 0.7660, Test: 0.7510\n",
      "Epoch: 377,train_loss:0.0506517, Train: 1.0000, Val: 0.7540, Test: 0.7400\n",
      "Epoch: 378,train_loss:0.0338117, Train: 1.0000, Val: 0.7180, Test: 0.7200\n",
      "Epoch: 379,train_loss:0.0290590, Train: 1.0000, Val: 0.6460, Test: 0.6440\n",
      "Epoch: 380,train_loss:0.0526399, Train: 1.0000, Val: 0.6660, Test: 0.6530\n",
      "Epoch: 381,train_loss:0.0137335, Train: 1.0000, Val: 0.7000, Test: 0.6930\n",
      "Epoch: 382,train_loss:0.0045667, Train: 1.0000, Val: 0.7320, Test: 0.7170\n",
      "Epoch: 383,train_loss:0.0276117, Train: 1.0000, Val: 0.7540, Test: 0.7400\n",
      "Epoch: 384,train_loss:0.0064169, Train: 1.0000, Val: 0.7760, Test: 0.7480\n",
      "Epoch: 385,train_loss:0.0176756, Train: 1.0000, Val: 0.7680, Test: 0.7600\n",
      "Epoch: 386,train_loss:0.0210869, Train: 1.0000, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 387,train_loss:0.0081565, Train: 1.0000, Val: 0.7700, Test: 0.7650\n",
      "Epoch: 388,train_loss:0.0025038, Train: 1.0000, Val: 0.7700, Test: 0.7670\n",
      "Epoch: 389,train_loss:0.0050325, Train: 1.0000, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 390,train_loss:0.0031037, Train: 1.0000, Val: 0.7620, Test: 0.7550\n",
      "Epoch: 391,train_loss:0.0067126, Train: 1.0000, Val: 0.7560, Test: 0.7520\n",
      "Epoch: 392,train_loss:0.0344205, Train: 1.0000, Val: 0.7420, Test: 0.7420\n",
      "Epoch: 393,train_loss:0.0020521, Train: 1.0000, Val: 0.7320, Test: 0.7340\n",
      "Epoch: 394,train_loss:0.0198797, Train: 1.0000, Val: 0.7440, Test: 0.7320\n",
      "Epoch: 395,train_loss:0.0109336, Train: 1.0000, Val: 0.7520, Test: 0.7300\n",
      "Epoch: 396,train_loss:0.0704151, Train: 1.0000, Val: 0.7800, Test: 0.7660\n",
      "Epoch: 397,train_loss:0.0666924, Train: 1.0000, Val: 0.7680, Test: 0.7520\n",
      "Epoch: 398,train_loss:0.0618884, Train: 1.0000, Val: 0.7440, Test: 0.7300\n",
      "Epoch: 399,train_loss:0.1140690, Train: 1.0000, Val: 0.7800, Test: 0.7700\n",
      "Epoch: 400,train_loss:0.0330844, Train: 1.0000, Val: 0.7680, Test: 0.7530\n",
      "Epoch: 401,train_loss:0.0377611, Train: 1.0000, Val: 0.7080, Test: 0.7220\n",
      "Epoch: 402,train_loss:0.0130360, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 403,train_loss:0.0151652, Train: 0.9833, Val: 0.5920, Test: 0.6030\n",
      "Epoch: 404,train_loss:0.2247552, Train: 1.0000, Val: 0.7080, Test: 0.7230\n",
      "Epoch: 405,train_loss:0.0126212, Train: 1.0000, Val: 0.7700, Test: 0.7650\n",
      "Epoch: 406,train_loss:0.0073850, Train: 1.0000, Val: 0.7600, Test: 0.7610\n",
      "Epoch: 407,train_loss:0.0724186, Train: 1.0000, Val: 0.7440, Test: 0.7390\n",
      "Epoch: 408,train_loss:0.1183506, Train: 1.0000, Val: 0.7480, Test: 0.7510\n",
      "Epoch: 409,train_loss:0.0639044, Train: 1.0000, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 410,train_loss:0.0385276, Train: 1.0000, Val: 0.7720, Test: 0.7590\n",
      "Epoch: 411,train_loss:0.0202933, Train: 1.0000, Val: 0.7380, Test: 0.7410\n",
      "Epoch: 412,train_loss:0.0626800, Train: 1.0000, Val: 0.6940, Test: 0.7090\n",
      "Epoch: 413,train_loss:0.0629785, Train: 1.0000, Val: 0.6980, Test: 0.6980\n",
      "Epoch: 414,train_loss:0.0177537, Train: 1.0000, Val: 0.6900, Test: 0.6980\n",
      "Epoch: 415,train_loss:0.1857271, Train: 1.0000, Val: 0.7180, Test: 0.7270\n",
      "Epoch: 416,train_loss:0.0507086, Train: 1.0000, Val: 0.7620, Test: 0.7520\n",
      "Epoch: 417,train_loss:0.0117872, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
      "Epoch: 418,train_loss:0.0193660, Train: 1.0000, Val: 0.7580, Test: 0.7590\n",
      "Epoch: 419,train_loss:0.0808779, Train: 1.0000, Val: 0.7500, Test: 0.7570\n",
      "Epoch: 420,train_loss:0.0255116, Train: 1.0000, Val: 0.7400, Test: 0.7550\n",
      "Epoch: 421,train_loss:0.0159355, Train: 1.0000, Val: 0.7400, Test: 0.7360\n",
      "Epoch: 422,train_loss:0.0403622, Train: 1.0000, Val: 0.7500, Test: 0.7480\n",
      "Epoch: 423,train_loss:0.0504147, Train: 1.0000, Val: 0.7540, Test: 0.7610\n",
      "Epoch: 424,train_loss:0.0245160, Train: 1.0000, Val: 0.7620, Test: 0.7630\n",
      "Epoch: 425,train_loss:0.0016631, Train: 1.0000, Val: 0.7620, Test: 0.7530\n",
      "Epoch: 426,train_loss:0.0137453, Train: 1.0000, Val: 0.7400, Test: 0.7330\n",
      "Epoch: 427,train_loss:0.0064103, Train: 1.0000, Val: 0.7120, Test: 0.7170\n",
      "Epoch: 428,train_loss:0.0347756, Train: 1.0000, Val: 0.6820, Test: 0.6920\n",
      "Epoch: 429,train_loss:0.0174622, Train: 1.0000, Val: 0.6600, Test: 0.6720\n",
      "Epoch: 430,train_loss:0.1556922, Train: 1.0000, Val: 0.7160, Test: 0.7160\n",
      "Epoch: 431,train_loss:0.0091074, Train: 1.0000, Val: 0.7360, Test: 0.7380\n",
      "Epoch: 432,train_loss:0.0091579, Train: 1.0000, Val: 0.7520, Test: 0.7500\n",
      "Epoch: 433,train_loss:0.0501694, Train: 1.0000, Val: 0.7680, Test: 0.7560\n",
      "Epoch: 434,train_loss:0.0167538, Train: 1.0000, Val: 0.7720, Test: 0.7630\n",
      "Epoch: 435,train_loss:0.0396881, Train: 1.0000, Val: 0.7700, Test: 0.7530\n",
      "Epoch: 436,train_loss:0.0046316, Train: 1.0000, Val: 0.7680, Test: 0.7530\n",
      "Epoch: 437,train_loss:0.0198508, Train: 1.0000, Val: 0.7680, Test: 0.7500\n",
      "Epoch: 438,train_loss:0.0097727, Train: 1.0000, Val: 0.7660, Test: 0.7520\n",
      "Epoch: 439,train_loss:0.0878375, Train: 1.0000, Val: 0.7660, Test: 0.7620\n",
      "Epoch: 440,train_loss:0.0531366, Train: 1.0000, Val: 0.7620, Test: 0.7560\n",
      "Epoch: 441,train_loss:0.0141189, Train: 1.0000, Val: 0.7660, Test: 0.7530\n",
      "Epoch: 442,train_loss:0.0304804, Train: 1.0000, Val: 0.7640, Test: 0.7470\n",
      "Epoch: 443,train_loss:0.0569734, Train: 1.0000, Val: 0.7460, Test: 0.7360\n",
      "Epoch: 444,train_loss:0.0041579, Train: 1.0000, Val: 0.7180, Test: 0.7270\n",
      "Epoch: 445,train_loss:0.0036573, Train: 1.0000, Val: 0.6980, Test: 0.7210\n",
      "Epoch: 446,train_loss:0.0167512, Train: 1.0000, Val: 0.6800, Test: 0.7050\n",
      "Epoch: 447,train_loss:0.0388355, Train: 1.0000, Val: 0.6780, Test: 0.7000\n",
      "Epoch: 448,train_loss:0.0194029, Train: 1.0000, Val: 0.6800, Test: 0.7100\n",
      "Epoch: 449,train_loss:0.1577111, Train: 1.0000, Val: 0.7480, Test: 0.7480\n",
      "Epoch: 450,train_loss:0.0255283, Train: 1.0000, Val: 0.7340, Test: 0.7480\n",
      "Epoch: 451,train_loss:0.0272211, Train: 1.0000, Val: 0.7340, Test: 0.7260\n",
      "Epoch: 452,train_loss:0.0571520, Train: 1.0000, Val: 0.7380, Test: 0.7180\n",
      "Epoch: 453,train_loss:0.1192438, Train: 1.0000, Val: 0.7560, Test: 0.7640\n",
      "Epoch: 454,train_loss:0.0155848, Train: 1.0000, Val: 0.7820, Test: 0.7570\n",
      "Epoch: 455,train_loss:0.0083044, Train: 1.0000, Val: 0.7700, Test: 0.7480\n",
      "Epoch: 456,train_loss:0.0272445, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 457,train_loss:0.0103746, Train: 1.0000, Val: 0.7040, Test: 0.7250\n",
      "Epoch: 458,train_loss:0.0263334, Train: 1.0000, Val: 0.6860, Test: 0.7040\n",
      "Epoch: 459,train_loss:0.0313072, Train: 1.0000, Val: 0.6820, Test: 0.7000\n",
      "Epoch: 460,train_loss:0.0487964, Train: 1.0000, Val: 0.6860, Test: 0.7020\n",
      "Epoch: 461,train_loss:0.0924556, Train: 1.0000, Val: 0.7340, Test: 0.7210\n",
      "Epoch: 462,train_loss:0.1352289, Train: 0.9667, Val: 0.7460, Test: 0.7300\n",
      "Epoch: 463,train_loss:0.1259062, Train: 0.9667, Val: 0.7560, Test: 0.7370\n",
      "Epoch: 464,train_loss:0.2486175, Train: 0.9833, Val: 0.7560, Test: 0.7530\n",
      "Epoch: 465,train_loss:0.0322592, Train: 1.0000, Val: 0.7420, Test: 0.7140\n",
      "Epoch: 466,train_loss:0.1238223, Train: 1.0000, Val: 0.7320, Test: 0.7140\n",
      "Epoch: 467,train_loss:0.0384865, Train: 1.0000, Val: 0.7260, Test: 0.7030\n",
      "Epoch: 468,train_loss:0.1333862, Train: 1.0000, Val: 0.6960, Test: 0.6910\n",
      "Epoch: 469,train_loss:0.0417704, Train: 1.0000, Val: 0.7560, Test: 0.7410\n",
      "Epoch: 470,train_loss:0.0207765, Train: 1.0000, Val: 0.7260, Test: 0.7200\n",
      "Epoch: 471,train_loss:0.1144769, Train: 1.0000, Val: 0.6720, Test: 0.6840\n",
      "Epoch: 472,train_loss:0.0923083, Train: 0.9667, Val: 0.6440, Test: 0.6450\n",
      "Epoch: 473,train_loss:0.2264715, Train: 1.0000, Val: 0.7100, Test: 0.7130\n",
      "Epoch: 474,train_loss:0.0247618, Train: 1.0000, Val: 0.7680, Test: 0.7560\n",
      "Epoch: 475,train_loss:0.0278792, Train: 1.0000, Val: 0.7840, Test: 0.7650\n",
      "Epoch: 476,train_loss:0.0256655, Train: 1.0000, Val: 0.7880, Test: 0.7620\n",
      "Epoch: 477,train_loss:0.0443320, Train: 1.0000, Val: 0.7800, Test: 0.7530\n",
      "Epoch: 478,train_loss:0.0222716, Train: 1.0000, Val: 0.7720, Test: 0.7380\n",
      "Epoch: 479,train_loss:0.0112757, Train: 1.0000, Val: 0.7620, Test: 0.7320\n",
      "Epoch: 480,train_loss:0.0859641, Train: 1.0000, Val: 0.7740, Test: 0.7390\n",
      "Epoch: 481,train_loss:0.1259191, Train: 0.9833, Val: 0.7720, Test: 0.7520\n",
      "Epoch: 482,train_loss:0.1123636, Train: 1.0000, Val: 0.7280, Test: 0.7180\n",
      "Epoch: 483,train_loss:0.0471263, Train: 1.0000, Val: 0.7580, Test: 0.7340\n",
      "Epoch: 484,train_loss:0.0076313, Train: 1.0000, Val: 0.7280, Test: 0.7310\n",
      "Epoch: 485,train_loss:0.1026903, Train: 1.0000, Val: 0.7380, Test: 0.7280\n",
      "Epoch: 486,train_loss:0.0274242, Train: 1.0000, Val: 0.6720, Test: 0.6710\n",
      "Epoch: 487,train_loss:0.0132261, Train: 1.0000, Val: 0.6120, Test: 0.5970\n",
      "Epoch: 488,train_loss:0.0192751, Train: 0.9833, Val: 0.5620, Test: 0.5550\n",
      "Epoch: 489,train_loss:0.1707022, Train: 1.0000, Val: 0.7300, Test: 0.7150\n",
      "Epoch: 490,train_loss:0.0396155, Train: 1.0000, Val: 0.7480, Test: 0.7370\n",
      "Epoch: 491,train_loss:0.0797833, Train: 1.0000, Val: 0.7580, Test: 0.7410\n",
      "Epoch: 492,train_loss:0.0174371, Train: 1.0000, Val: 0.7640, Test: 0.7440\n",
      "Epoch: 493,train_loss:0.0291097, Train: 1.0000, Val: 0.7620, Test: 0.7460\n",
      "Epoch: 494,train_loss:0.0471691, Train: 1.0000, Val: 0.7340, Test: 0.7280\n",
      "Epoch: 495,train_loss:0.0504469, Train: 1.0000, Val: 0.7100, Test: 0.7140\n",
      "Epoch: 496,train_loss:0.0161770, Train: 1.0000, Val: 0.7080, Test: 0.7080\n",
      "Epoch: 497,train_loss:0.0274060, Train: 1.0000, Val: 0.6900, Test: 0.7040\n",
      "Epoch: 498,train_loss:0.0123829, Train: 1.0000, Val: 0.6880, Test: 0.6890\n",
      "Epoch: 499,train_loss:0.0599827, Train: 0.9833, Val: 0.6980, Test: 0.6870\n",
      "Epoch: 500,train_loss:0.1053216, Train: 1.0000, Val: 0.7360, Test: 0.7180\n",
      "Epoch: 501,train_loss:0.0610106, Train: 1.0000, Val: 0.7680, Test: 0.7420\n",
      "Epoch: 502,train_loss:0.1182532, Train: 1.0000, Val: 0.7680, Test: 0.7560\n",
      "Epoch: 503,train_loss:0.0280349, Train: 1.0000, Val: 0.7660, Test: 0.7650\n",
      "Epoch: 504,train_loss:0.0012017, Train: 1.0000, Val: 0.7660, Test: 0.7730\n",
      "Epoch: 505,train_loss:0.0195201, Train: 1.0000, Val: 0.7760, Test: 0.7650\n",
      "Epoch: 506,train_loss:0.1524245, Train: 1.0000, Val: 0.7760, Test: 0.7650\n",
      "Epoch: 507,train_loss:0.0177313, Train: 1.0000, Val: 0.7700, Test: 0.7710\n",
      "Epoch: 508,train_loss:0.0289650, Train: 1.0000, Val: 0.7740, Test: 0.7640\n",
      "Epoch: 509,train_loss:0.0253426, Train: 1.0000, Val: 0.7680, Test: 0.7660\n",
      "Epoch: 510,train_loss:0.0676312, Train: 1.0000, Val: 0.7760, Test: 0.7690\n",
      "Epoch: 511,train_loss:0.0402269, Train: 1.0000, Val: 0.7740, Test: 0.7610\n",
      "Epoch: 512,train_loss:0.0100485, Train: 1.0000, Val: 0.7640, Test: 0.7590\n",
      "Epoch: 513,train_loss:0.0144332, Train: 1.0000, Val: 0.7400, Test: 0.7480\n",
      "Epoch: 514,train_loss:0.0193943, Train: 1.0000, Val: 0.7160, Test: 0.7300\n",
      "Epoch: 515,train_loss:0.0043536, Train: 1.0000, Val: 0.7080, Test: 0.7170\n",
      "Epoch: 516,train_loss:0.1077336, Train: 1.0000, Val: 0.7240, Test: 0.7250\n",
      "Epoch: 517,train_loss:0.0688526, Train: 1.0000, Val: 0.7420, Test: 0.7480\n",
      "Epoch: 518,train_loss:0.0354150, Train: 1.0000, Val: 0.7440, Test: 0.7560\n",
      "Epoch: 519,train_loss:0.0062932, Train: 1.0000, Val: 0.7600, Test: 0.7590\n",
      "Epoch: 520,train_loss:0.0207535, Train: 1.0000, Val: 0.7660, Test: 0.7590\n",
      "Epoch: 521,train_loss:0.1077408, Train: 1.0000, Val: 0.7620, Test: 0.7570\n",
      "Epoch: 522,train_loss:0.0564933, Train: 1.0000, Val: 0.7660, Test: 0.7590\n",
      "Epoch: 523,train_loss:0.0478114, Train: 1.0000, Val: 0.7820, Test: 0.7670\n",
      "Epoch: 524,train_loss:0.0555011, Train: 1.0000, Val: 0.7700, Test: 0.7580\n",
      "Epoch: 525,train_loss:0.0245779, Train: 1.0000, Val: 0.7800, Test: 0.7490\n",
      "Epoch: 526,train_loss:0.0264264, Train: 1.0000, Val: 0.7660, Test: 0.7440\n",
      "Epoch: 527,train_loss:0.0706383, Train: 1.0000, Val: 0.7620, Test: 0.7310\n",
      "Epoch: 528,train_loss:0.0157739, Train: 1.0000, Val: 0.7480, Test: 0.7240\n",
      "Epoch: 529,train_loss:0.0431628, Train: 1.0000, Val: 0.7240, Test: 0.7100\n",
      "Epoch: 530,train_loss:0.0309013, Train: 1.0000, Val: 0.7160, Test: 0.7050\n",
      "Epoch: 531,train_loss:0.0515838, Train: 1.0000, Val: 0.7160, Test: 0.7070\n",
      "Epoch: 532,train_loss:0.0094143, Train: 1.0000, Val: 0.7180, Test: 0.7030\n",
      "Epoch: 533,train_loss:0.1626798, Train: 1.0000, Val: 0.7360, Test: 0.7200\n",
      "Epoch: 534,train_loss:0.0446868, Train: 1.0000, Val: 0.7680, Test: 0.7400\n",
      "Epoch: 535,train_loss:0.1073642, Train: 1.0000, Val: 0.7780, Test: 0.7470\n",
      "Epoch: 536,train_loss:0.0330575, Train: 1.0000, Val: 0.7720, Test: 0.7490\n",
      "Epoch: 537,train_loss:0.0198046, Train: 1.0000, Val: 0.7720, Test: 0.7490\n",
      "Epoch: 538,train_loss:0.0704014, Train: 1.0000, Val: 0.7720, Test: 0.7570\n",
      "Epoch: 539,train_loss:0.0047795, Train: 1.0000, Val: 0.7740, Test: 0.7560\n",
      "Epoch: 540,train_loss:0.0511708, Train: 1.0000, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 541,train_loss:0.0352382, Train: 1.0000, Val: 0.7740, Test: 0.7660\n",
      "Epoch: 542,train_loss:0.0079574, Train: 1.0000, Val: 0.7760, Test: 0.7670\n",
      "Epoch: 543,train_loss:0.0085291, Train: 1.0000, Val: 0.7740, Test: 0.7620\n",
      "Epoch: 544,train_loss:0.1259973, Train: 1.0000, Val: 0.7700, Test: 0.7460\n",
      "Epoch: 545,train_loss:0.0048281, Train: 1.0000, Val: 0.7680, Test: 0.7410\n",
      "Epoch: 546,train_loss:0.0092157, Train: 1.0000, Val: 0.7520, Test: 0.7280\n",
      "Epoch: 547,train_loss:0.0175036, Train: 1.0000, Val: 0.7320, Test: 0.7180\n",
      "Epoch: 548,train_loss:0.0041898, Train: 1.0000, Val: 0.7060, Test: 0.7120\n",
      "Epoch: 549,train_loss:0.0201085, Train: 1.0000, Val: 0.6980, Test: 0.7080\n",
      "Epoch: 550,train_loss:0.0258963, Train: 1.0000, Val: 0.7120, Test: 0.7160\n",
      "Epoch: 551,train_loss:0.0227682, Train: 1.0000, Val: 0.7220, Test: 0.7170\n",
      "Epoch: 552,train_loss:0.0709111, Train: 1.0000, Val: 0.7300, Test: 0.7260\n",
      "Epoch: 553,train_loss:0.0090389, Train: 1.0000, Val: 0.7340, Test: 0.7350\n",
      "Epoch: 554,train_loss:0.0514178, Train: 1.0000, Val: 0.7400, Test: 0.7420\n",
      "Epoch: 555,train_loss:0.0022105, Train: 1.0000, Val: 0.7460, Test: 0.7420\n",
      "Epoch: 556,train_loss:0.0162688, Train: 1.0000, Val: 0.7540, Test: 0.7400\n",
      "Epoch: 557,train_loss:0.0138276, Train: 1.0000, Val: 0.7720, Test: 0.7510\n",
      "Epoch: 558,train_loss:0.0019380, Train: 1.0000, Val: 0.7780, Test: 0.7540\n",
      "Epoch: 559,train_loss:0.0203681, Train: 1.0000, Val: 0.7760, Test: 0.7580\n",
      "Epoch: 560,train_loss:0.0335834, Train: 1.0000, Val: 0.7860, Test: 0.7620\n",
      "Epoch: 561,train_loss:0.0046678, Train: 1.0000, Val: 0.7820, Test: 0.7600\n",
      "Epoch: 562,train_loss:0.0609246, Train: 1.0000, Val: 0.7860, Test: 0.7590\n",
      "Epoch: 563,train_loss:0.0314364, Train: 1.0000, Val: 0.7820, Test: 0.7590\n",
      "Epoch: 564,train_loss:0.0145082, Train: 1.0000, Val: 0.7840, Test: 0.7570\n",
      "Epoch: 565,train_loss:0.0098652, Train: 1.0000, Val: 0.7800, Test: 0.7580\n",
      "Epoch: 566,train_loss:0.0028705, Train: 1.0000, Val: 0.7780, Test: 0.7570\n",
      "Epoch: 567,train_loss:0.0229220, Train: 1.0000, Val: 0.7840, Test: 0.7600\n",
      "Epoch: 568,train_loss:0.0272092, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
      "Epoch: 569,train_loss:0.0249780, Train: 1.0000, Val: 0.7760, Test: 0.7640\n",
      "Epoch: 570,train_loss:0.0240620, Train: 1.0000, Val: 0.7840, Test: 0.7640\n",
      "Epoch: 571,train_loss:0.0308181, Train: 1.0000, Val: 0.7800, Test: 0.7650\n",
      "Epoch: 572,train_loss:0.0046774, Train: 1.0000, Val: 0.7800, Test: 0.7650\n",
      "Epoch: 573,train_loss:0.0041491, Train: 1.0000, Val: 0.7740, Test: 0.7660\n",
      "Epoch: 574,train_loss:0.0087055, Train: 1.0000, Val: 0.7720, Test: 0.7670\n",
      "Epoch: 575,train_loss:0.0020618, Train: 1.0000, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 576,train_loss:0.0257215, Train: 1.0000, Val: 0.7660, Test: 0.7680\n",
      "Epoch: 577,train_loss:0.0126792, Train: 1.0000, Val: 0.7700, Test: 0.7690\n",
      "Epoch: 578,train_loss:0.0286610, Train: 1.0000, Val: 0.7740, Test: 0.7710\n",
      "Epoch: 579,train_loss:0.0114457, Train: 1.0000, Val: 0.7700, Test: 0.7720\n",
      "Epoch: 580,train_loss:0.0416494, Train: 1.0000, Val: 0.7720, Test: 0.7710\n",
      "Epoch: 581,train_loss:0.0021291, Train: 1.0000, Val: 0.7720, Test: 0.7710\n",
      "Epoch: 582,train_loss:0.0467624, Train: 1.0000, Val: 0.7780, Test: 0.7680\n",
      "Epoch: 583,train_loss:0.0375727, Train: 1.0000, Val: 0.7800, Test: 0.7660\n",
      "Epoch: 584,train_loss:0.0388614, Train: 1.0000, Val: 0.7760, Test: 0.7540\n",
      "Epoch: 585,train_loss:0.0452680, Train: 1.0000, Val: 0.7740, Test: 0.7560\n",
      "Epoch: 586,train_loss:0.0264210, Train: 1.0000, Val: 0.7720, Test: 0.7570\n",
      "Epoch: 587,train_loss:0.0298419, Train: 1.0000, Val: 0.7640, Test: 0.7540\n",
      "Epoch: 588,train_loss:0.0246731, Train: 1.0000, Val: 0.7720, Test: 0.7620\n",
      "Epoch: 589,train_loss:0.0173744, Train: 1.0000, Val: 0.7800, Test: 0.7590\n",
      "Epoch: 590,train_loss:0.0905283, Train: 1.0000, Val: 0.7760, Test: 0.7570\n",
      "Epoch: 591,train_loss:0.0462905, Train: 1.0000, Val: 0.7840, Test: 0.7550\n",
      "Epoch: 592,train_loss:0.0325919, Train: 1.0000, Val: 0.7780, Test: 0.7530\n",
      "Epoch: 593,train_loss:0.0285512, Train: 1.0000, Val: 0.7780, Test: 0.7530\n",
      "Epoch: 594,train_loss:0.0076475, Train: 1.0000, Val: 0.7840, Test: 0.7500\n",
      "Epoch: 595,train_loss:0.0061710, Train: 1.0000, Val: 0.7820, Test: 0.7500\n",
      "Epoch: 596,train_loss:0.0125120, Train: 1.0000, Val: 0.7760, Test: 0.7500\n",
      "Epoch: 597,train_loss:0.0031517, Train: 1.0000, Val: 0.7680, Test: 0.7500\n",
      "Epoch: 598,train_loss:0.0315840, Train: 1.0000, Val: 0.7680, Test: 0.7520\n",
      "Epoch: 599,train_loss:0.0197601, Train: 1.0000, Val: 0.7640, Test: 0.7520\n",
      "Epoch: 600,train_loss:0.0189359, Train: 1.0000, Val: 0.7620, Test: 0.7520\n",
      "Epoch: 601,train_loss:0.0198737, Train: 1.0000, Val: 0.7620, Test: 0.7540\n",
      "Epoch: 602,train_loss:0.0036408, Train: 1.0000, Val: 0.7620, Test: 0.7550\n",
      "Epoch: 603,train_loss:0.0163474, Train: 1.0000, Val: 0.7660, Test: 0.7560\n",
      "Epoch: 604,train_loss:0.0942414, Train: 1.0000, Val: 0.7800, Test: 0.7570\n",
      "Epoch: 605,train_loss:0.0027011, Train: 1.0000, Val: 0.7880, Test: 0.7570\n",
      "Epoch: 606,train_loss:0.0593942, Train: 1.0000, Val: 0.7920, Test: 0.7610\n",
      "Epoch: 607,train_loss:0.0181818, Train: 1.0000, Val: 0.7940, Test: 0.7580\n",
      "Epoch: 608,train_loss:0.0117607, Train: 1.0000, Val: 0.7860, Test: 0.7610\n",
      "Epoch: 609,train_loss:0.0383934, Train: 1.0000, Val: 0.7920, Test: 0.7600\n",
      "Epoch: 610,train_loss:0.0116667, Train: 1.0000, Val: 0.7940, Test: 0.7560\n",
      "Epoch: 611,train_loss:0.0089420, Train: 1.0000, Val: 0.7820, Test: 0.7610\n",
      "Epoch: 612,train_loss:0.0279831, Train: 1.0000, Val: 0.7740, Test: 0.7590\n",
      "Epoch: 613,train_loss:0.0234219, Train: 1.0000, Val: 0.7640, Test: 0.7540\n",
      "Epoch: 614,train_loss:0.0382503, Train: 1.0000, Val: 0.7640, Test: 0.7510\n",
      "Epoch: 615,train_loss:0.0169756, Train: 1.0000, Val: 0.7640, Test: 0.7500\n",
      "Epoch: 616,train_loss:0.0144204, Train: 1.0000, Val: 0.7600, Test: 0.7530\n",
      "Epoch: 617,train_loss:0.0393609, Train: 1.0000, Val: 0.7700, Test: 0.7540\n",
      "Epoch: 618,train_loss:0.0033911, Train: 1.0000, Val: 0.7640, Test: 0.7520\n",
      "Epoch: 619,train_loss:0.0270841, Train: 1.0000, Val: 0.7700, Test: 0.7640\n",
      "Epoch: 620,train_loss:0.0112034, Train: 1.0000, Val: 0.7620, Test: 0.7700\n",
      "Epoch: 621,train_loss:0.0549923, Train: 1.0000, Val: 0.7600, Test: 0.7620\n",
      "Epoch: 622,train_loss:0.0272449, Train: 1.0000, Val: 0.7640, Test: 0.7640\n",
      "Epoch: 623,train_loss:0.0372010, Train: 1.0000, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 624,train_loss:0.0024187, Train: 1.0000, Val: 0.7700, Test: 0.7670\n",
      "Epoch: 625,train_loss:0.0046525, Train: 1.0000, Val: 0.7720, Test: 0.7660\n",
      "Epoch: 626,train_loss:0.0023910, Train: 1.0000, Val: 0.7720, Test: 0.7690\n",
      "Epoch: 627,train_loss:0.0306791, Train: 1.0000, Val: 0.7780, Test: 0.7670\n",
      "Epoch: 628,train_loss:0.0067588, Train: 1.0000, Val: 0.7740, Test: 0.7680\n",
      "Epoch: 629,train_loss:0.0298185, Train: 1.0000, Val: 0.7800, Test: 0.7720\n",
      "Epoch: 630,train_loss:0.0197419, Train: 1.0000, Val: 0.7820, Test: 0.7740\n",
      "Epoch: 631,train_loss:0.0017294, Train: 1.0000, Val: 0.7840, Test: 0.7730\n",
      "Epoch: 632,train_loss:0.0023150, Train: 1.0000, Val: 0.7840, Test: 0.7720\n",
      "Epoch: 633,train_loss:0.0158163, Train: 1.0000, Val: 0.7880, Test: 0.7730\n",
      "Epoch: 634,train_loss:0.0266083, Train: 1.0000, Val: 0.7940, Test: 0.7710\n",
      "Epoch: 635,train_loss:0.0718386, Train: 1.0000, Val: 0.7940, Test: 0.7580\n",
      "Epoch: 636,train_loss:0.0694564, Train: 1.0000, Val: 0.7980, Test: 0.7570\n",
      "Epoch: 637,train_loss:0.0017268, Train: 1.0000, Val: 0.7780, Test: 0.7520\n",
      "Epoch: 638,train_loss:0.0109866, Train: 1.0000, Val: 0.7600, Test: 0.7490\n",
      "Epoch: 639,train_loss:0.0260383, Train: 1.0000, Val: 0.7640, Test: 0.7490\n",
      "Epoch: 640,train_loss:0.0533369, Train: 1.0000, Val: 0.7540, Test: 0.7490\n",
      "Epoch: 641,train_loss:0.0283570, Train: 1.0000, Val: 0.7560, Test: 0.7510\n",
      "Epoch: 642,train_loss:0.0200173, Train: 1.0000, Val: 0.7560, Test: 0.7510\n",
      "Epoch: 643,train_loss:0.0373340, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 644,train_loss:0.0238654, Train: 1.0000, Val: 0.7600, Test: 0.7460\n",
      "Epoch: 645,train_loss:0.0232880, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 646,train_loss:0.0074556, Train: 1.0000, Val: 0.7600, Test: 0.7460\n",
      "Epoch: 647,train_loss:0.0408155, Train: 1.0000, Val: 0.7580, Test: 0.7450\n",
      "Epoch: 648,train_loss:0.0201130, Train: 1.0000, Val: 0.7760, Test: 0.7460\n",
      "Epoch: 649,train_loss:0.0051255, Train: 1.0000, Val: 0.7820, Test: 0.7490\n",
      "Epoch: 650,train_loss:0.0477301, Train: 1.0000, Val: 0.7840, Test: 0.7580\n",
      "Epoch: 651,train_loss:0.0942750, Train: 1.0000, Val: 0.7700, Test: 0.7590\n",
      "Epoch: 652,train_loss:0.0086499, Train: 1.0000, Val: 0.7480, Test: 0.7590\n",
      "Epoch: 653,train_loss:0.0604828, Train: 1.0000, Val: 0.7480, Test: 0.7560\n",
      "Epoch: 654,train_loss:0.0106214, Train: 1.0000, Val: 0.7420, Test: 0.7530\n",
      "Epoch: 655,train_loss:0.0482591, Train: 1.0000, Val: 0.7420, Test: 0.7490\n",
      "Epoch: 656,train_loss:0.0325067, Train: 1.0000, Val: 0.7420, Test: 0.7400\n",
      "Epoch: 657,train_loss:0.0402532, Train: 1.0000, Val: 0.7460, Test: 0.7310\n",
      "Epoch: 658,train_loss:0.0414350, Train: 1.0000, Val: 0.7380, Test: 0.7320\n",
      "Epoch: 659,train_loss:0.0086516, Train: 1.0000, Val: 0.7400, Test: 0.7370\n",
      "Epoch: 660,train_loss:0.0680832, Train: 1.0000, Val: 0.7320, Test: 0.7340\n",
      "Epoch: 661,train_loss:0.0047065, Train: 1.0000, Val: 0.7420, Test: 0.7310\n",
      "Epoch: 662,train_loss:0.0107399, Train: 1.0000, Val: 0.7440, Test: 0.7330\n",
      "Epoch: 663,train_loss:0.0265240, Train: 1.0000, Val: 0.7580, Test: 0.7480\n",
      "Epoch: 664,train_loss:0.0242736, Train: 1.0000, Val: 0.7720, Test: 0.7480\n",
      "Epoch: 665,train_loss:0.0197634, Train: 1.0000, Val: 0.7860, Test: 0.7460\n",
      "Epoch: 666,train_loss:0.0268544, Train: 1.0000, Val: 0.7940, Test: 0.7540\n",
      "Epoch: 667,train_loss:0.0331783, Train: 1.0000, Val: 0.7880, Test: 0.7650\n",
      "Epoch: 668,train_loss:0.0035948, Train: 1.0000, Val: 0.7820, Test: 0.7690\n",
      "Epoch: 669,train_loss:0.0228630, Train: 1.0000, Val: 0.7740, Test: 0.7730\n",
      "Epoch: 670,train_loss:0.0157639, Train: 1.0000, Val: 0.7700, Test: 0.7740\n",
      "Epoch: 671,train_loss:0.0022701, Train: 1.0000, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 672,train_loss:0.0573196, Train: 1.0000, Val: 0.7860, Test: 0.7640\n",
      "Epoch: 673,train_loss:0.0263635, Train: 1.0000, Val: 0.7760, Test: 0.7480\n",
      "Epoch: 674,train_loss:0.0350773, Train: 1.0000, Val: 0.7640, Test: 0.7440\n",
      "Epoch: 675,train_loss:0.0575840, Train: 1.0000, Val: 0.7440, Test: 0.7360\n",
      "Epoch: 676,train_loss:0.0209916, Train: 1.0000, Val: 0.7260, Test: 0.7260\n",
      "Epoch: 677,train_loss:0.0095953, Train: 1.0000, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 678,train_loss:0.0088451, Train: 1.0000, Val: 0.7200, Test: 0.7170\n",
      "Epoch: 679,train_loss:0.0260939, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch: 680,train_loss:0.0330483, Train: 1.0000, Val: 0.7580, Test: 0.7410\n",
      "Epoch: 681,train_loss:0.0021748, Train: 1.0000, Val: 0.7640, Test: 0.7500\n",
      "Epoch: 682,train_loss:0.0064988, Train: 1.0000, Val: 0.7740, Test: 0.7540\n",
      "Epoch: 683,train_loss:0.0442029, Train: 1.0000, Val: 0.7880, Test: 0.7640\n",
      "Epoch: 684,train_loss:0.0377053, Train: 1.0000, Val: 0.7880, Test: 0.7660\n",
      "Epoch: 685,train_loss:0.0144406, Train: 1.0000, Val: 0.7900, Test: 0.7700\n",
      "Epoch: 686,train_loss:0.0071316, Train: 1.0000, Val: 0.7900, Test: 0.7760\n",
      "Epoch: 687,train_loss:0.0751341, Train: 1.0000, Val: 0.7900, Test: 0.7730\n",
      "Epoch: 688,train_loss:0.0054389, Train: 1.0000, Val: 0.7880, Test: 0.7740\n",
      "Epoch: 689,train_loss:0.0534175, Train: 1.0000, Val: 0.7960, Test: 0.7680\n",
      "Epoch: 690,train_loss:0.0210832, Train: 1.0000, Val: 0.7780, Test: 0.7710\n",
      "Epoch: 691,train_loss:0.0259187, Train: 1.0000, Val: 0.7740, Test: 0.7650\n",
      "Epoch: 692,train_loss:0.0043945, Train: 1.0000, Val: 0.7580, Test: 0.7500\n",
      "Epoch: 693,train_loss:0.0199849, Train: 1.0000, Val: 0.7580, Test: 0.7450\n",
      "Epoch: 694,train_loss:0.0848614, Train: 1.0000, Val: 0.7700, Test: 0.7580\n",
      "Epoch: 695,train_loss:0.0084435, Train: 1.0000, Val: 0.7800, Test: 0.7700\n",
      "Epoch: 696,train_loss:0.0249049, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 697,train_loss:0.0145870, Train: 1.0000, Val: 0.7920, Test: 0.7720\n",
      "Epoch: 698,train_loss:0.0082251, Train: 1.0000, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 699,train_loss:0.0051723, Train: 1.0000, Val: 0.7800, Test: 0.7780\n",
      "Epoch: 700,train_loss:0.0212851, Train: 1.0000, Val: 0.7720, Test: 0.7690\n",
      "Epoch: 701,train_loss:0.0249954, Train: 1.0000, Val: 0.7820, Test: 0.7700\n",
      "Epoch: 702,train_loss:0.0854760, Train: 1.0000, Val: 0.7800, Test: 0.7760\n",
      "Epoch: 703,train_loss:0.0024170, Train: 1.0000, Val: 0.7800, Test: 0.7580\n",
      "Epoch: 704,train_loss:0.0021535, Train: 1.0000, Val: 0.7540, Test: 0.7400\n",
      "Epoch: 705,train_loss:0.0238064, Train: 1.0000, Val: 0.7240, Test: 0.7180\n",
      "Epoch: 706,train_loss:0.0465824, Train: 1.0000, Val: 0.7040, Test: 0.7020\n",
      "Epoch: 707,train_loss:0.1657046, Train: 1.0000, Val: 0.7760, Test: 0.7560\n",
      "Epoch: 708,train_loss:0.0262857, Train: 1.0000, Val: 0.7640, Test: 0.7560\n",
      "Epoch: 709,train_loss:0.1040075, Train: 1.0000, Val: 0.7520, Test: 0.7540\n",
      "Epoch: 710,train_loss:0.0645114, Train: 1.0000, Val: 0.7500, Test: 0.7530\n",
      "Epoch: 711,train_loss:0.0280431, Train: 1.0000, Val: 0.7620, Test: 0.7550\n",
      "Epoch: 712,train_loss:0.0486717, Train: 1.0000, Val: 0.7740, Test: 0.7620\n",
      "Epoch: 713,train_loss:0.0548174, Train: 1.0000, Val: 0.7740, Test: 0.7690\n",
      "Epoch: 714,train_loss:0.0257533, Train: 1.0000, Val: 0.7860, Test: 0.7680\n",
      "Epoch: 715,train_loss:0.0148001, Train: 1.0000, Val: 0.7880, Test: 0.7650\n",
      "Epoch: 716,train_loss:0.0557401, Train: 1.0000, Val: 0.7860, Test: 0.7680\n",
      "Epoch: 717,train_loss:0.0300222, Train: 1.0000, Val: 0.7820, Test: 0.7690\n",
      "Epoch: 718,train_loss:0.0680155, Train: 1.0000, Val: 0.7700, Test: 0.7700\n",
      "Epoch: 719,train_loss:0.0051188, Train: 1.0000, Val: 0.7580, Test: 0.7550\n",
      "Epoch: 720,train_loss:0.0578475, Train: 1.0000, Val: 0.7480, Test: 0.7490\n",
      "Epoch: 721,train_loss:0.0034287, Train: 1.0000, Val: 0.7460, Test: 0.7460\n",
      "Epoch: 722,train_loss:0.0846196, Train: 1.0000, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 723,train_loss:0.0059756, Train: 1.0000, Val: 0.7640, Test: 0.7550\n",
      "Epoch: 724,train_loss:0.0382327, Train: 1.0000, Val: 0.7580, Test: 0.7520\n",
      "Epoch: 725,train_loss:0.0092584, Train: 1.0000, Val: 0.7540, Test: 0.7450\n",
      "Epoch: 726,train_loss:0.0044727, Train: 1.0000, Val: 0.7400, Test: 0.7290\n",
      "Epoch: 727,train_loss:0.0315542, Train: 1.0000, Val: 0.7440, Test: 0.7170\n",
      "Epoch: 728,train_loss:0.0371643, Train: 1.0000, Val: 0.7460, Test: 0.7340\n",
      "Epoch: 729,train_loss:0.0745304, Train: 1.0000, Val: 0.7620, Test: 0.7550\n",
      "Epoch: 730,train_loss:0.0564263, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 731,train_loss:0.0184383, Train: 1.0000, Val: 0.7780, Test: 0.7610\n",
      "Epoch: 732,train_loss:0.0027767, Train: 1.0000, Val: 0.7720, Test: 0.7590\n",
      "Epoch: 733,train_loss:0.0075150, Train: 1.0000, Val: 0.7600, Test: 0.7500\n",
      "Epoch: 734,train_loss:0.0304165, Train: 1.0000, Val: 0.7520, Test: 0.7510\n",
      "Epoch: 735,train_loss:0.0325850, Train: 1.0000, Val: 0.7480, Test: 0.7450\n",
      "Epoch: 736,train_loss:0.0025282, Train: 1.0000, Val: 0.7480, Test: 0.7440\n",
      "Epoch: 737,train_loss:0.0653591, Train: 1.0000, Val: 0.7560, Test: 0.7530\n",
      "Epoch: 738,train_loss:0.0049650, Train: 1.0000, Val: 0.7600, Test: 0.7570\n",
      "Epoch: 739,train_loss:0.0061230, Train: 1.0000, Val: 0.7640, Test: 0.7570\n",
      "Epoch: 740,train_loss:0.0067392, Train: 1.0000, Val: 0.7700, Test: 0.7510\n",
      "Epoch: 741,train_loss:0.0020229, Train: 1.0000, Val: 0.7520, Test: 0.7390\n",
      "Epoch: 742,train_loss:0.0213254, Train: 1.0000, Val: 0.7480, Test: 0.7380\n",
      "Epoch: 743,train_loss:0.0129172, Train: 1.0000, Val: 0.7560, Test: 0.7380\n",
      "Epoch: 744,train_loss:0.0225047, Train: 1.0000, Val: 0.7600, Test: 0.7320\n",
      "Epoch: 745,train_loss:0.0073760, Train: 1.0000, Val: 0.7560, Test: 0.7280\n",
      "Epoch: 746,train_loss:0.0040119, Train: 1.0000, Val: 0.7600, Test: 0.7300\n",
      "Epoch: 747,train_loss:0.0218529, Train: 1.0000, Val: 0.7540, Test: 0.7410\n",
      "Epoch: 748,train_loss:0.0049624, Train: 1.0000, Val: 0.7640, Test: 0.7480\n",
      "Epoch: 749,train_loss:0.0533937, Train: 1.0000, Val: 0.7740, Test: 0.7530\n",
      "Epoch: 750,train_loss:0.0657134, Train: 1.0000, Val: 0.7640, Test: 0.7630\n",
      "Epoch: 751,train_loss:0.0498208, Train: 1.0000, Val: 0.7640, Test: 0.7530\n",
      "Epoch: 752,train_loss:0.0088347, Train: 1.0000, Val: 0.7720, Test: 0.7540\n",
      "Epoch: 753,train_loss:0.0092678, Train: 1.0000, Val: 0.7740, Test: 0.7620\n",
      "Epoch: 754,train_loss:0.0296442, Train: 1.0000, Val: 0.7720, Test: 0.7630\n",
      "Epoch: 755,train_loss:0.0527629, Train: 1.0000, Val: 0.7720, Test: 0.7690\n",
      "Epoch: 756,train_loss:0.0085378, Train: 1.0000, Val: 0.7700, Test: 0.7610\n",
      "Epoch: 757,train_loss:0.0033003, Train: 1.0000, Val: 0.7840, Test: 0.7680\n",
      "Epoch: 758,train_loss:0.0072887, Train: 1.0000, Val: 0.7800, Test: 0.7650\n",
      "Epoch: 759,train_loss:0.0194323, Train: 1.0000, Val: 0.7780, Test: 0.7600\n",
      "Epoch: 760,train_loss:0.0463846, Train: 1.0000, Val: 0.7800, Test: 0.7640\n",
      "Epoch: 761,train_loss:0.0058150, Train: 1.0000, Val: 0.7820, Test: 0.7700\n",
      "Epoch: 762,train_loss:0.0040652, Train: 1.0000, Val: 0.7780, Test: 0.7670\n",
      "Epoch: 763,train_loss:0.0251815, Train: 1.0000, Val: 0.7780, Test: 0.7710\n",
      "Epoch: 764,train_loss:0.0431480, Train: 1.0000, Val: 0.7800, Test: 0.7710\n",
      "Epoch: 765,train_loss:0.0119325, Train: 1.0000, Val: 0.7800, Test: 0.7590\n",
      "Epoch: 766,train_loss:0.0025085, Train: 1.0000, Val: 0.7660, Test: 0.7430\n",
      "Epoch: 767,train_loss:0.0227803, Train: 1.0000, Val: 0.7420, Test: 0.7300\n",
      "Epoch: 768,train_loss:0.0082471, Train: 1.0000, Val: 0.7260, Test: 0.7110\n",
      "Epoch: 769,train_loss:0.0310999, Train: 1.0000, Val: 0.7460, Test: 0.7240\n",
      "Epoch: 770,train_loss:0.0081493, Train: 1.0000, Val: 0.7580, Test: 0.7390\n",
      "Epoch: 771,train_loss:0.0048070, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 772,train_loss:0.0446342, Train: 1.0000, Val: 0.7720, Test: 0.7420\n",
      "Epoch: 773,train_loss:0.0044990, Train: 1.0000, Val: 0.7680, Test: 0.7420\n",
      "Epoch: 774,train_loss:0.0420120, Train: 1.0000, Val: 0.7640, Test: 0.7400\n",
      "Epoch: 775,train_loss:0.0068683, Train: 1.0000, Val: 0.7620, Test: 0.7400\n",
      "Epoch: 776,train_loss:0.0411743, Train: 1.0000, Val: 0.7620, Test: 0.7360\n",
      "Epoch: 777,train_loss:0.0192772, Train: 1.0000, Val: 0.7600, Test: 0.7300\n",
      "Epoch: 778,train_loss:0.0460909, Train: 1.0000, Val: 0.7780, Test: 0.7510\n",
      "Epoch: 779,train_loss:0.0182123, Train: 1.0000, Val: 0.7740, Test: 0.7520\n",
      "Epoch: 780,train_loss:0.0032560, Train: 1.0000, Val: 0.7640, Test: 0.7570\n",
      "Epoch: 781,train_loss:0.0089478, Train: 1.0000, Val: 0.7640, Test: 0.7610\n",
      "Epoch: 782,train_loss:0.0585468, Train: 1.0000, Val: 0.7640, Test: 0.7570\n",
      "Epoch: 783,train_loss:0.0117527, Train: 1.0000, Val: 0.7560, Test: 0.7580\n",
      "Epoch: 784,train_loss:0.0025108, Train: 1.0000, Val: 0.7540, Test: 0.7510\n",
      "Epoch: 785,train_loss:0.0114119, Train: 1.0000, Val: 0.7600, Test: 0.7440\n",
      "Epoch: 786,train_loss:0.0123282, Train: 1.0000, Val: 0.7640, Test: 0.7470\n",
      "Epoch: 787,train_loss:0.0107778, Train: 1.0000, Val: 0.7620, Test: 0.7470\n",
      "Epoch: 788,train_loss:0.0095561, Train: 1.0000, Val: 0.7540, Test: 0.7480\n",
      "Epoch: 789,train_loss:0.0087721, Train: 1.0000, Val: 0.7620, Test: 0.7560\n",
      "Epoch: 790,train_loss:0.0320314, Train: 1.0000, Val: 0.7820, Test: 0.7580\n",
      "Epoch: 791,train_loss:0.0329604, Train: 1.0000, Val: 0.7760, Test: 0.7640\n",
      "Epoch: 792,train_loss:0.0067732, Train: 1.0000, Val: 0.7720, Test: 0.7660\n",
      "Epoch: 793,train_loss:0.0283000, Train: 1.0000, Val: 0.7640, Test: 0.7590\n",
      "Epoch: 794,train_loss:0.0023220, Train: 1.0000, Val: 0.7700, Test: 0.7560\n",
      "Epoch: 795,train_loss:0.0421314, Train: 1.0000, Val: 0.7640, Test: 0.7540\n",
      "Epoch: 796,train_loss:0.0214160, Train: 1.0000, Val: 0.7640, Test: 0.7570\n",
      "Epoch: 797,train_loss:0.0127956, Train: 1.0000, Val: 0.7600, Test: 0.7610\n",
      "Epoch: 798,train_loss:0.0019113, Train: 1.0000, Val: 0.7560, Test: 0.7640\n",
      "Epoch: 799,train_loss:0.0422053, Train: 1.0000, Val: 0.7560, Test: 0.7610\n",
      "Epoch: 800,train_loss:0.0103783, Train: 1.0000, Val: 0.7520, Test: 0.7600\n",
      "Epoch: 801,train_loss:0.0228132, Train: 1.0000, Val: 0.7480, Test: 0.7570\n",
      "Epoch: 802,train_loss:0.0036947, Train: 1.0000, Val: 0.7480, Test: 0.7540\n",
      "Epoch: 803,train_loss:0.0300763, Train: 1.0000, Val: 0.7500, Test: 0.7580\n",
      "Epoch: 804,train_loss:0.0849599, Train: 1.0000, Val: 0.7500, Test: 0.7460\n",
      "Epoch: 805,train_loss:0.0160852, Train: 1.0000, Val: 0.7440, Test: 0.7390\n",
      "Epoch: 806,train_loss:0.0348815, Train: 1.0000, Val: 0.7120, Test: 0.7020\n",
      "Epoch: 807,train_loss:0.0737326, Train: 1.0000, Val: 0.6860, Test: 0.6880\n",
      "Epoch: 808,train_loss:0.0500459, Train: 1.0000, Val: 0.6980, Test: 0.6890\n",
      "Epoch: 809,train_loss:0.0280441, Train: 1.0000, Val: 0.7160, Test: 0.7100\n",
      "Epoch: 810,train_loss:0.0192534, Train: 1.0000, Val: 0.7560, Test: 0.7340\n",
      "Epoch: 811,train_loss:0.0088014, Train: 1.0000, Val: 0.7840, Test: 0.7540\n",
      "Epoch: 812,train_loss:0.0048417, Train: 1.0000, Val: 0.7800, Test: 0.7590\n",
      "Epoch: 813,train_loss:0.0109689, Train: 1.0000, Val: 0.7780, Test: 0.7640\n",
      "Epoch: 814,train_loss:0.0262729, Train: 1.0000, Val: 0.7720, Test: 0.7630\n",
      "Epoch: 815,train_loss:0.0719111, Train: 1.0000, Val: 0.7740, Test: 0.7610\n",
      "Epoch: 816,train_loss:0.0857552, Train: 1.0000, Val: 0.7700, Test: 0.7670\n",
      "Epoch: 817,train_loss:0.0435906, Train: 1.0000, Val: 0.7760, Test: 0.7610\n",
      "Epoch: 818,train_loss:0.0393197, Train: 1.0000, Val: 0.7680, Test: 0.7570\n",
      "Epoch: 819,train_loss:0.0299643, Train: 1.0000, Val: 0.7600, Test: 0.7440\n",
      "Epoch: 820,train_loss:0.0321859, Train: 1.0000, Val: 0.7480, Test: 0.7370\n",
      "Epoch: 821,train_loss:0.0093842, Train: 1.0000, Val: 0.7340, Test: 0.7360\n",
      "Epoch: 822,train_loss:0.0139982, Train: 1.0000, Val: 0.7320, Test: 0.7260\n",
      "Epoch: 823,train_loss:0.0322634, Train: 1.0000, Val: 0.7280, Test: 0.7150\n",
      "Epoch: 824,train_loss:0.0128699, Train: 1.0000, Val: 0.7260, Test: 0.7110\n",
      "Epoch: 825,train_loss:0.0028084, Train: 1.0000, Val: 0.7300, Test: 0.7090\n",
      "Epoch: 826,train_loss:0.0647979, Train: 1.0000, Val: 0.7320, Test: 0.7280\n",
      "Epoch: 827,train_loss:0.0134811, Train: 1.0000, Val: 0.7460, Test: 0.7380\n",
      "Epoch: 828,train_loss:0.0052346, Train: 1.0000, Val: 0.7660, Test: 0.7450\n",
      "Epoch: 829,train_loss:0.0280802, Train: 1.0000, Val: 0.7600, Test: 0.7480\n",
      "Epoch: 830,train_loss:0.0310894, Train: 1.0000, Val: 0.7640, Test: 0.7490\n",
      "Epoch: 831,train_loss:0.0619461, Train: 1.0000, Val: 0.7600, Test: 0.7450\n",
      "Epoch: 832,train_loss:0.0575325, Train: 1.0000, Val: 0.7660, Test: 0.7400\n",
      "Epoch: 833,train_loss:0.0071719, Train: 1.0000, Val: 0.7760, Test: 0.7390\n",
      "Epoch: 834,train_loss:0.1217034, Train: 1.0000, Val: 0.7740, Test: 0.7420\n",
      "Epoch: 835,train_loss:0.0152077, Train: 1.0000, Val: 0.7640, Test: 0.7400\n",
      "Epoch: 836,train_loss:0.0296260, Train: 1.0000, Val: 0.7540, Test: 0.7330\n",
      "Epoch: 837,train_loss:0.0341365, Train: 1.0000, Val: 0.7340, Test: 0.7230\n",
      "Epoch: 838,train_loss:0.0333411, Train: 1.0000, Val: 0.7320, Test: 0.7090\n",
      "Epoch: 839,train_loss:0.0581685, Train: 1.0000, Val: 0.7180, Test: 0.6960\n",
      "Epoch: 840,train_loss:0.0655247, Train: 1.0000, Val: 0.7360, Test: 0.7250\n",
      "Epoch: 841,train_loss:0.0185322, Train: 1.0000, Val: 0.7540, Test: 0.7420\n",
      "Epoch: 842,train_loss:0.0023073, Train: 1.0000, Val: 0.7660, Test: 0.7530\n",
      "Epoch: 843,train_loss:0.0290173, Train: 1.0000, Val: 0.7700, Test: 0.7600\n",
      "Epoch: 844,train_loss:0.0050868, Train: 1.0000, Val: 0.7660, Test: 0.7600\n",
      "Epoch: 845,train_loss:0.0059110, Train: 1.0000, Val: 0.7660, Test: 0.7570\n",
      "Epoch: 846,train_loss:0.0385228, Train: 1.0000, Val: 0.7680, Test: 0.7570\n",
      "Epoch: 847,train_loss:0.0282891, Train: 1.0000, Val: 0.7680, Test: 0.7570\n",
      "Epoch: 848,train_loss:0.0280768, Train: 1.0000, Val: 0.7720, Test: 0.7520\n",
      "Epoch: 849,train_loss:0.0095905, Train: 1.0000, Val: 0.7720, Test: 0.7460\n",
      "Epoch: 850,train_loss:0.0077236, Train: 1.0000, Val: 0.7800, Test: 0.7480\n",
      "Epoch: 851,train_loss:0.0056426, Train: 1.0000, Val: 0.7760, Test: 0.7440\n",
      "Epoch: 852,train_loss:0.0384728, Train: 1.0000, Val: 0.7720, Test: 0.7380\n",
      "Epoch: 853,train_loss:0.0093505, Train: 1.0000, Val: 0.7740, Test: 0.7410\n",
      "Epoch: 854,train_loss:0.0213459, Train: 1.0000, Val: 0.7740, Test: 0.7400\n",
      "Epoch: 855,train_loss:0.0118065, Train: 1.0000, Val: 0.7740, Test: 0.7390\n",
      "Epoch: 856,train_loss:0.0111840, Train: 1.0000, Val: 0.7760, Test: 0.7410\n",
      "Epoch: 857,train_loss:0.0058165, Train: 1.0000, Val: 0.7720, Test: 0.7440\n",
      "Epoch: 858,train_loss:0.0213487, Train: 1.0000, Val: 0.7740, Test: 0.7420\n",
      "Epoch: 859,train_loss:0.0025555, Train: 1.0000, Val: 0.7720, Test: 0.7430\n",
      "Epoch: 860,train_loss:0.0155819, Train: 1.0000, Val: 0.7680, Test: 0.7430\n",
      "Epoch: 861,train_loss:0.0309229, Train: 1.0000, Val: 0.7620, Test: 0.7430\n",
      "Epoch: 862,train_loss:0.0095888, Train: 1.0000, Val: 0.7620, Test: 0.7480\n",
      "Epoch: 863,train_loss:0.0037823, Train: 1.0000, Val: 0.7640, Test: 0.7420\n",
      "Epoch: 864,train_loss:0.0252702, Train: 1.0000, Val: 0.7660, Test: 0.7390\n",
      "Epoch: 865,train_loss:0.0706125, Train: 1.0000, Val: 0.7700, Test: 0.7470\n",
      "Epoch: 866,train_loss:0.0105275, Train: 1.0000, Val: 0.7680, Test: 0.7370\n",
      "Epoch: 867,train_loss:0.0080953, Train: 1.0000, Val: 0.7680, Test: 0.7400\n",
      "Epoch: 868,train_loss:0.0020078, Train: 1.0000, Val: 0.7540, Test: 0.7350\n",
      "Epoch: 869,train_loss:0.0265768, Train: 1.0000, Val: 0.7540, Test: 0.7330\n",
      "Epoch: 870,train_loss:0.0127661, Train: 1.0000, Val: 0.7580, Test: 0.7360\n",
      "Epoch: 871,train_loss:0.0406748, Train: 1.0000, Val: 0.7720, Test: 0.7390\n",
      "Epoch: 872,train_loss:0.0236820, Train: 1.0000, Val: 0.7780, Test: 0.7460\n",
      "Epoch: 873,train_loss:0.0029365, Train: 1.0000, Val: 0.7760, Test: 0.7500\n",
      "Epoch: 874,train_loss:0.0132125, Train: 1.0000, Val: 0.7840, Test: 0.7520\n",
      "Epoch: 875,train_loss:0.0566408, Train: 1.0000, Val: 0.7920, Test: 0.7600\n",
      "Epoch: 876,train_loss:0.0374804, Train: 1.0000, Val: 0.7780, Test: 0.7570\n",
      "Epoch: 877,train_loss:0.0279683, Train: 1.0000, Val: 0.7760, Test: 0.7570\n",
      "Epoch: 878,train_loss:0.0302020, Train: 1.0000, Val: 0.7740, Test: 0.7570\n",
      "Epoch: 879,train_loss:0.0020284, Train: 1.0000, Val: 0.7700, Test: 0.7590\n",
      "Epoch: 880,train_loss:0.0229276, Train: 1.0000, Val: 0.7740, Test: 0.7540\n",
      "Epoch: 881,train_loss:0.0180720, Train: 1.0000, Val: 0.7820, Test: 0.7480\n",
      "Epoch: 882,train_loss:0.0374307, Train: 1.0000, Val: 0.7780, Test: 0.7410\n",
      "Epoch: 883,train_loss:0.0759599, Train: 1.0000, Val: 0.7680, Test: 0.7390\n",
      "Epoch: 884,train_loss:0.0536510, Train: 1.0000, Val: 0.7540, Test: 0.7380\n",
      "Epoch: 885,train_loss:0.0067717, Train: 1.0000, Val: 0.7460, Test: 0.7300\n",
      "Epoch: 886,train_loss:0.0143788, Train: 1.0000, Val: 0.7420, Test: 0.7260\n",
      "Epoch: 887,train_loss:0.0296155, Train: 1.0000, Val: 0.7400, Test: 0.7320\n",
      "Epoch: 888,train_loss:0.0150456, Train: 1.0000, Val: 0.7600, Test: 0.7320\n",
      "Epoch: 889,train_loss:0.0514605, Train: 1.0000, Val: 0.7560, Test: 0.7510\n",
      "Epoch: 890,train_loss:0.0326513, Train: 1.0000, Val: 0.7680, Test: 0.7520\n",
      "Epoch: 891,train_loss:0.0135154, Train: 1.0000, Val: 0.7580, Test: 0.7470\n",
      "Epoch: 892,train_loss:0.0254319, Train: 1.0000, Val: 0.7540, Test: 0.7370\n",
      "Epoch: 893,train_loss:0.0045996, Train: 1.0000, Val: 0.7480, Test: 0.7290\n",
      "Epoch: 894,train_loss:0.0060863, Train: 1.0000, Val: 0.7460, Test: 0.7310\n",
      "Epoch: 895,train_loss:0.1464602, Train: 1.0000, Val: 0.7640, Test: 0.7530\n",
      "Epoch: 896,train_loss:0.0063685, Train: 1.0000, Val: 0.7700, Test: 0.7510\n",
      "Epoch: 897,train_loss:0.0469896, Train: 1.0000, Val: 0.7720, Test: 0.7410\n",
      "Epoch: 898,train_loss:0.0139671, Train: 1.0000, Val: 0.7520, Test: 0.7370\n",
      "Epoch: 899,train_loss:0.0126189, Train: 1.0000, Val: 0.7320, Test: 0.7250\n",
      "Epoch: 900,train_loss:0.0083217, Train: 1.0000, Val: 0.7260, Test: 0.7140\n",
      "Epoch: 901,train_loss:0.0952595, Train: 1.0000, Val: 0.7600, Test: 0.7390\n",
      "Epoch: 902,train_loss:0.0397855, Train: 1.0000, Val: 0.7720, Test: 0.7510\n",
      "Epoch: 903,train_loss:0.0316343, Train: 1.0000, Val: 0.7580, Test: 0.7450\n",
      "Epoch: 904,train_loss:0.0421631, Train: 1.0000, Val: 0.7460, Test: 0.7410\n",
      "Epoch: 905,train_loss:0.0424727, Train: 1.0000, Val: 0.7420, Test: 0.7390\n",
      "Epoch: 906,train_loss:0.0546860, Train: 1.0000, Val: 0.7560, Test: 0.7380\n",
      "Epoch: 907,train_loss:0.0454989, Train: 1.0000, Val: 0.7600, Test: 0.7440\n",
      "Epoch: 908,train_loss:0.0026921, Train: 1.0000, Val: 0.7720, Test: 0.7460\n",
      "Epoch: 909,train_loss:0.0497259, Train: 1.0000, Val: 0.7600, Test: 0.7340\n",
      "Epoch: 910,train_loss:0.0081773, Train: 1.0000, Val: 0.7380, Test: 0.7170\n",
      "Epoch: 911,train_loss:0.0508203, Train: 1.0000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 912,train_loss:0.0160428, Train: 1.0000, Val: 0.7420, Test: 0.7300\n",
      "Epoch: 913,train_loss:0.0180837, Train: 1.0000, Val: 0.7320, Test: 0.7240\n",
      "Epoch: 914,train_loss:0.0320375, Train: 1.0000, Val: 0.7340, Test: 0.7330\n",
      "Epoch: 915,train_loss:0.0010446, Train: 1.0000, Val: 0.7380, Test: 0.7350\n",
      "Epoch: 916,train_loss:0.0628983, Train: 1.0000, Val: 0.7380, Test: 0.7440\n",
      "Epoch: 917,train_loss:0.0036046, Train: 1.0000, Val: 0.7440, Test: 0.7430\n",
      "Epoch: 918,train_loss:0.0665318, Train: 1.0000, Val: 0.7540, Test: 0.7550\n",
      "Epoch: 919,train_loss:0.0043341, Train: 1.0000, Val: 0.7800, Test: 0.7650\n",
      "Epoch: 920,train_loss:0.0124582, Train: 1.0000, Val: 0.7780, Test: 0.7560\n",
      "Epoch: 921,train_loss:0.0247023, Train: 1.0000, Val: 0.7720, Test: 0.7540\n",
      "Epoch: 922,train_loss:0.0053161, Train: 1.0000, Val: 0.7800, Test: 0.7520\n",
      "Epoch: 923,train_loss:0.0086330, Train: 1.0000, Val: 0.7740, Test: 0.7450\n",
      "Epoch: 924,train_loss:0.0205689, Train: 1.0000, Val: 0.7640, Test: 0.7440\n",
      "Epoch: 925,train_loss:0.0231596, Train: 1.0000, Val: 0.7640, Test: 0.7380\n",
      "Epoch: 926,train_loss:0.0129411, Train: 1.0000, Val: 0.7580, Test: 0.7350\n",
      "Epoch: 927,train_loss:0.0172774, Train: 1.0000, Val: 0.7580, Test: 0.7300\n",
      "Epoch: 928,train_loss:0.0086661, Train: 1.0000, Val: 0.7540, Test: 0.7440\n",
      "Epoch: 929,train_loss:0.0178599, Train: 1.0000, Val: 0.7560, Test: 0.7480\n",
      "Epoch: 930,train_loss:0.0092033, Train: 1.0000, Val: 0.7580, Test: 0.7500\n",
      "Epoch: 931,train_loss:0.0072566, Train: 1.0000, Val: 0.7520, Test: 0.7490\n",
      "Epoch: 932,train_loss:0.0254995, Train: 1.0000, Val: 0.7680, Test: 0.7560\n",
      "Epoch: 933,train_loss:0.0050210, Train: 1.0000, Val: 0.7620, Test: 0.7620\n",
      "Epoch: 934,train_loss:0.0479881, Train: 1.0000, Val: 0.7660, Test: 0.7630\n",
      "Epoch: 935,train_loss:0.1534002, Train: 1.0000, Val: 0.7800, Test: 0.7620\n",
      "Epoch: 936,train_loss:0.0196300, Train: 1.0000, Val: 0.7700, Test: 0.7570\n",
      "Epoch: 937,train_loss:0.0025056, Train: 1.0000, Val: 0.7660, Test: 0.7500\n",
      "Epoch: 938,train_loss:0.0027521, Train: 1.0000, Val: 0.7680, Test: 0.7390\n",
      "Epoch: 939,train_loss:0.0208861, Train: 1.0000, Val: 0.7600, Test: 0.7400\n",
      "Epoch: 940,train_loss:0.0283914, Train: 1.0000, Val: 0.7520, Test: 0.7380\n",
      "Epoch: 941,train_loss:0.0335908, Train: 1.0000, Val: 0.7520, Test: 0.7330\n",
      "Epoch: 942,train_loss:0.0076829, Train: 1.0000, Val: 0.7540, Test: 0.7350\n",
      "Epoch: 943,train_loss:0.0031922, Train: 1.0000, Val: 0.7580, Test: 0.7380\n",
      "Epoch: 944,train_loss:0.0270962, Train: 1.0000, Val: 0.7640, Test: 0.7390\n",
      "Epoch: 945,train_loss:0.0174345, Train: 1.0000, Val: 0.7720, Test: 0.7540\n",
      "Epoch: 946,train_loss:0.0067214, Train: 1.0000, Val: 0.7740, Test: 0.7570\n",
      "Epoch: 947,train_loss:0.0615012, Train: 1.0000, Val: 0.7720, Test: 0.7650\n",
      "Epoch: 948,train_loss:0.0198344, Train: 1.0000, Val: 0.7740, Test: 0.7640\n",
      "Epoch: 949,train_loss:0.0046292, Train: 1.0000, Val: 0.7680, Test: 0.7620\n",
      "Epoch: 950,train_loss:0.0492941, Train: 1.0000, Val: 0.7700, Test: 0.7630\n",
      "Epoch: 951,train_loss:0.0226623, Train: 1.0000, Val: 0.7680, Test: 0.7620\n",
      "Epoch: 952,train_loss:0.0102140, Train: 1.0000, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 953,train_loss:0.0325227, Train: 1.0000, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 954,train_loss:0.0205229, Train: 1.0000, Val: 0.7700, Test: 0.7610\n",
      "Epoch: 955,train_loss:0.0277300, Train: 1.0000, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 956,train_loss:0.0107845, Train: 1.0000, Val: 0.7680, Test: 0.7620\n",
      "Epoch: 957,train_loss:0.0032041, Train: 1.0000, Val: 0.7680, Test: 0.7630\n",
      "Epoch: 958,train_loss:0.0915403, Train: 1.0000, Val: 0.7680, Test: 0.7670\n",
      "Epoch: 959,train_loss:0.0027406, Train: 1.0000, Val: 0.7640, Test: 0.7710\n",
      "Epoch: 960,train_loss:0.0514142, Train: 1.0000, Val: 0.7720, Test: 0.7670\n",
      "Epoch: 961,train_loss:0.0281881, Train: 1.0000, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 962,train_loss:0.0317668, Train: 1.0000, Val: 0.7700, Test: 0.7600\n",
      "Epoch: 963,train_loss:0.0427000, Train: 1.0000, Val: 0.7800, Test: 0.7610\n",
      "Epoch: 964,train_loss:0.0093068, Train: 1.0000, Val: 0.7680, Test: 0.7600\n",
      "Epoch: 965,train_loss:0.0170985, Train: 1.0000, Val: 0.7660, Test: 0.7670\n",
      "Epoch: 966,train_loss:0.0448712, Train: 1.0000, Val: 0.7600, Test: 0.7590\n",
      "Epoch: 967,train_loss:0.0354480, Train: 1.0000, Val: 0.7620, Test: 0.7560\n",
      "Epoch: 968,train_loss:0.0430720, Train: 1.0000, Val: 0.7560, Test: 0.7470\n",
      "Epoch: 969,train_loss:0.0495958, Train: 1.0000, Val: 0.7540, Test: 0.7470\n",
      "Epoch: 970,train_loss:0.0184296, Train: 1.0000, Val: 0.7520, Test: 0.7450\n",
      "Epoch: 971,train_loss:0.0178077, Train: 1.0000, Val: 0.7480, Test: 0.7410\n",
      "Epoch: 972,train_loss:0.0366845, Train: 1.0000, Val: 0.7680, Test: 0.7550\n",
      "Epoch: 973,train_loss:0.0140977, Train: 1.0000, Val: 0.7800, Test: 0.7600\n",
      "Epoch: 974,train_loss:0.0031588, Train: 1.0000, Val: 0.7920, Test: 0.7680\n",
      "Epoch: 975,train_loss:0.0577883, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 976,train_loss:0.0330955, Train: 1.0000, Val: 0.7760, Test: 0.7660\n",
      "Epoch: 977,train_loss:0.0067468, Train: 1.0000, Val: 0.7800, Test: 0.7670\n",
      "Epoch: 978,train_loss:0.0370553, Train: 1.0000, Val: 0.7840, Test: 0.7670\n",
      "Epoch: 979,train_loss:0.0196464, Train: 1.0000, Val: 0.7500, Test: 0.7530\n",
      "Epoch: 980,train_loss:0.0057464, Train: 1.0000, Val: 0.7360, Test: 0.7430\n",
      "Epoch: 981,train_loss:0.0115674, Train: 1.0000, Val: 0.7300, Test: 0.7290\n",
      "Epoch: 982,train_loss:0.0582120, Train: 1.0000, Val: 0.7340, Test: 0.7320\n",
      "Epoch: 983,train_loss:0.0295843, Train: 1.0000, Val: 0.7740, Test: 0.7630\n",
      "Epoch: 984,train_loss:0.0138883, Train: 1.0000, Val: 0.7800, Test: 0.7690\n",
      "Epoch: 985,train_loss:0.0363399, Train: 0.9833, Val: 0.7780, Test: 0.7640\n",
      "Epoch: 986,train_loss:0.0636357, Train: 1.0000, Val: 0.7800, Test: 0.7630\n",
      "Epoch: 987,train_loss:0.0970167, Train: 1.0000, Val: 0.7740, Test: 0.7630\n",
      "Epoch: 988,train_loss:0.0059222, Train: 1.0000, Val: 0.7660, Test: 0.7530\n",
      "Epoch: 989,train_loss:0.0058284, Train: 1.0000, Val: 0.7380, Test: 0.7340\n",
      "Epoch: 990,train_loss:0.0245195, Train: 1.0000, Val: 0.7240, Test: 0.7270\n",
      "Epoch: 991,train_loss:0.0898590, Train: 1.0000, Val: 0.7480, Test: 0.7390\n",
      "Epoch: 992,train_loss:0.0333906, Train: 1.0000, Val: 0.7620, Test: 0.7530\n",
      "Epoch: 993,train_loss:0.0211673, Train: 1.0000, Val: 0.7600, Test: 0.7460\n",
      "Epoch: 994,train_loss:0.0583133, Train: 1.0000, Val: 0.7780, Test: 0.7480\n",
      "Epoch: 995,train_loss:0.0375771, Train: 1.0000, Val: 0.7540, Test: 0.7410\n",
      "Epoch: 996,train_loss:0.0440883, Train: 1.0000, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 997,train_loss:0.1526362, Train: 1.0000, Val: 0.7740, Test: 0.7530\n",
      "Epoch: 998,train_loss:0.0038554, Train: 1.0000, Val: 0.7640, Test: 0.7560\n",
      "Epoch: 999,train_loss:0.0265400, Train: 1.0000, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 1000,train_loss:0.0111538, Train: 1.0000, Val: 0.7700, Test: 0.7580\n",
      "CPU times: user 1min 8s, sys: 14.6 s, total: 1min 22s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "508035"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Pubmed'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 3  #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        # self.gatconv = AGNNConv(requires_grad=True)\n",
    "        self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_pubmed_geniepath,testacclist_pubmed_geniepath=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_pubmed_geniepath.append(loss)\n",
    "    testacclist_pubmed_geniepath.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# %matplotlib inline\n",
    "# f, ax = plt.subplots(1,2)\n",
    "\n",
    "# ax[0][0].plot(losslist_cora_geniepath,label=\"losslist_cora_geniepath\")\n",
    "# ax[0][1].plot(testacclist_cora_geniepath,label=\"testacclist_cora_geniepath\")\n",
    "# plt.legend(loc=0, ncol=1) \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000pubmed\n",
    "Epoch: 951,train_loss:0.0070723, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
    "Epoch: 952,train_loss:0.0485079, Train: 1.0000, Val: 0.7520, Test: 0.7610\n",
    "Epoch: 953,train_loss:0.0561940, Train: 1.0000, Val: 0.7480, Test: 0.7590\n",
    "Epoch: 954,train_loss:0.0304516, Train: 1.0000, Val: 0.7460, Test: 0.7610\n",
    "Epoch: 955,train_loss:0.0263838, Train: 1.0000, Val: 0.7520, Test: 0.7610\n",
    "Epoch: 956,train_loss:0.0356006, Train: 1.0000, Val: 0.7540, Test: 0.7590\n",
    "Epoch: 957,train_loss:0.0032032, Train: 1.0000, Val: 0.7500, Test: 0.7500\n",
    "Epoch: 958,train_loss:0.0043785, Train: 1.0000, Val: 0.7540, Test: 0.7430\n",
    "Epoch: 959,train_loss:0.0026555, Train: 1.0000, Val: 0.7520, Test: 0.7380\n",
    "Epoch: 960,train_loss:0.0370542, Train: 1.0000, Val: 0.7560, Test: 0.7400\n",
    "Epoch: 961,train_loss:0.0311677, Train: 1.0000, Val: 0.7600, Test: 0.7400\n",
    "Epoch: 962,train_loss:0.0098415, Train: 1.0000, Val: 0.7580, Test: 0.7370\n",
    "Epoch: 963,train_loss:0.0155770, Train: 1.0000, Val: 0.7520, Test: 0.7460\n",
    "Epoch: 964,train_loss:0.0127602, Train: 1.0000, Val: 0.7560, Test: 0.7610\n",
    "Epoch: 965,train_loss:0.0066608, Train: 1.0000, Val: 0.7560, Test: 0.7580\n",
    "Epoch: 966,train_loss:0.0628939, Train: 1.0000, Val: 0.7520, Test: 0.7420\n",
    "Epoch: 967,train_loss:0.0040972, Train: 1.0000, Val: 0.7540, Test: 0.7390\n",
    "Epoch: 968,train_loss:0.0079601, Train: 1.0000, Val: 0.7560, Test: 0.7360\n",
    "Epoch: 969,train_loss:0.0489217, Train: 1.0000, Val: 0.7500, Test: 0.7310\n",
    "Epoch: 970,train_loss:0.0439227, Train: 1.0000, Val: 0.7400, Test: 0.7280\n",
    "Epoch: 971,train_loss:0.0089715, Train: 1.0000, Val: 0.7340, Test: 0.7320\n",
    "Epoch: 972,train_loss:0.1212230, Train: 1.0000, Val: 0.7460, Test: 0.7510\n",
    "Epoch: 973,train_loss:0.0008306, Train: 1.0000, Val: 0.7440, Test: 0.7540\n",
    "Epoch: 974,train_loss:0.0242602, Train: 1.0000, Val: 0.7560, Test: 0.7620\n",
    "Epoch: 975,train_loss:0.0056071, Train: 1.0000, Val: 0.7580, Test: 0.7700\n",
    "Epoch: 976,train_loss:0.0649902, Train: 1.0000, Val: 0.7540, Test: 0.7660\n",
    "Epoch: 977,train_loss:0.0203555, Train: 1.0000, Val: 0.7460, Test: 0.7470\n",
    "Epoch: 978,train_loss:0.0176302, Train: 1.0000, Val: 0.7440, Test: 0.7430\n",
    "Epoch: 979,train_loss:0.0042343, Train: 1.0000, Val: 0.7420, Test: 0.7430\n",
    "Epoch: 980,train_loss:0.0205396, Train: 1.0000, Val: 0.7420, Test: 0.7410\n",
    "Epoch: 981,train_loss:0.0157316, Train: 1.0000, Val: 0.7500, Test: 0.7500\n",
    "Epoch: 982,train_loss:0.0279756, Train: 1.0000, Val: 0.7500, Test: 0.7530\n",
    "Epoch: 983,train_loss:0.0571950, Train: 1.0000, Val: 0.7540, Test: 0.7580\n",
    "Epoch: 984,train_loss:0.0120400, Train: 1.0000, Val: 0.7660, Test: 0.7600\n",
    "Epoch: 985,train_loss:0.0239784, Train: 1.0000, Val: 0.7600, Test: 0.7570\n",
    "Epoch: 986,train_loss:0.0036942, Train: 1.0000, Val: 0.7560, Test: 0.7400\n",
    "Epoch: 987,train_loss:0.0178741, Train: 1.0000, Val: 0.7540, Test: 0.7360\n",
    "Epoch: 988,train_loss:0.0436260, Train: 1.0000, Val: 0.7540, Test: 0.7440\n",
    "Epoch: 989,train_loss:0.0096762, Train: 1.0000, Val: 0.7560, Test: 0.7570\n",
    "Epoch: 990,train_loss:0.0511418, Train: 1.0000, Val: 0.7520, Test: 0.7560\n",
    "Epoch: 991,train_loss:0.0640492, Train: 1.0000, Val: 0.7540, Test: 0.7560\n",
    "Epoch: 992,train_loss:0.0821633, Train: 1.0000, Val: 0.7520, Test: 0.7720\n",
    "Epoch: 993,train_loss:0.0123788, Train: 1.0000, Val: 0.7520, Test: 0.7780\n",
    "Epoch: 994,train_loss:0.0405142, Train: 1.0000, Val: 0.7560, Test: 0.7730\n",
    "Epoch: 995,train_loss:0.0320284, Train: 1.0000, Val: 0.7520, Test: 0.7680\n",
    "Epoch: 996,train_loss:0.0058501, Train: 1.0000, Val: 0.7460, Test: 0.7550\n",
    "Epoch: 997,train_loss:0.0008803, Train: 1.0000, Val: 0.7440, Test: 0.7510\n",
    "Epoch: 998,train_loss:0.0061166, Train: 1.0000, Val: 0.7440, Test: 0.7430\n",
    "Epoch: 999,train_loss:0.0178923, Train: 1.0000, Val: 0.7440, Test: 0.7460\n",
    "Epoch: 1000,train_loss:0.0212040, Train: 1.0000, Val: 0.7420, Test: 0.7480\n",
    "CPU times: user 1min 10s, sys: 13.5 s, total: 1min 23s\n",
    "Wall time: 1min 20s\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sum([torch.numel(param) for param in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "f, ax = plt.subplots(1,2, figsize=(10,5), squeeze=False)\n",
    "\n",
    "ax[0][0].plot(losslist_pubmed_mymodel,label=\"losslist_pubmed_mymodel\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_pubmed_mymodel,label=\"testacclist_pubmed_mymodel\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "ax[0][0].plot(losslist_pubmed_geniepath,label=\"losslist_pubmed_geniepath\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_pubmed_geniepath,label=\"testacclist_pubmed_geniepath\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "# plt.savefig(\"gen_sgenpubmed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citeseer sgeniepath agnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.7923901, Train: 0.1667, Val: 0.1720, Test: 0.1820\n",
      "Epoch: 002,train_loss:1.7918785, Train: 0.1667, Val: 0.1720, Test: 0.1820\n",
      "Epoch: 003,train_loss:1.7913765, Train: 0.1667, Val: 0.1720, Test: 0.1820\n",
      "Epoch: 004,train_loss:1.7909145, Train: 0.1667, Val: 0.1720, Test: 0.1820\n",
      "Epoch: 005,train_loss:1.7905451, Train: 0.1667, Val: 0.1720, Test: 0.1820\n",
      "Epoch: 006,train_loss:1.7899067, Train: 0.2667, Val: 0.2500, Test: 0.1930\n",
      "Epoch: 007,train_loss:1.7898712, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 008,train_loss:1.7895527, Train: 0.1833, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 009,train_loss:1.7889564, Train: 0.2417, Val: 0.2320, Test: 0.1830\n",
      "Epoch: 010,train_loss:1.7885647, Train: 0.4000, Val: 0.3380, Test: 0.3560\n",
      "Epoch: 011,train_loss:1.7865410, Train: 0.1917, Val: 0.2120, Test: 0.2320\n",
      "Epoch: 012,train_loss:1.7863306, Train: 0.1750, Val: 0.2120, Test: 0.2320\n",
      "Epoch: 013,train_loss:1.7846472, Train: 0.1917, Val: 0.2120, Test: 0.2320\n",
      "Epoch: 014,train_loss:1.7832776, Train: 0.1917, Val: 0.2120, Test: 0.2320\n",
      "Epoch: 015,train_loss:1.7804207, Train: 0.2333, Val: 0.2160, Test: 0.2350\n",
      "Epoch: 016,train_loss:1.7765801, Train: 0.2667, Val: 0.2220, Test: 0.2490\n",
      "Epoch: 017,train_loss:1.7747333, Train: 0.3250, Val: 0.2300, Test: 0.2640\n",
      "Epoch: 018,train_loss:1.7693638, Train: 0.4167, Val: 0.2680, Test: 0.3040\n",
      "Epoch: 019,train_loss:1.7619681, Train: 0.5000, Val: 0.3080, Test: 0.3430\n",
      "Epoch: 020,train_loss:1.7547148, Train: 0.5667, Val: 0.3960, Test: 0.4240\n",
      "Epoch: 021,train_loss:1.7417393, Train: 0.6167, Val: 0.4540, Test: 0.4750\n",
      "Epoch: 022,train_loss:1.7278682, Train: 0.6333, Val: 0.4760, Test: 0.4900\n",
      "Epoch: 023,train_loss:1.7125467, Train: 0.6667, Val: 0.4860, Test: 0.5110\n",
      "Epoch: 024,train_loss:1.6931938, Train: 0.6917, Val: 0.5140, Test: 0.5280\n",
      "Epoch: 025,train_loss:1.6628375, Train: 0.6833, Val: 0.5120, Test: 0.5360\n",
      "Epoch: 026,train_loss:1.6326010, Train: 0.6583, Val: 0.5220, Test: 0.5390\n",
      "Epoch: 027,train_loss:1.5950330, Train: 0.6417, Val: 0.5120, Test: 0.5360\n",
      "Epoch: 028,train_loss:1.5578120, Train: 0.6333, Val: 0.5160, Test: 0.5540\n",
      "Epoch: 029,train_loss:1.5111390, Train: 0.6417, Val: 0.5140, Test: 0.5640\n",
      "Epoch: 030,train_loss:1.4557264, Train: 0.6500, Val: 0.5180, Test: 0.5620\n",
      "Epoch: 031,train_loss:1.4103844, Train: 0.6583, Val: 0.5200, Test: 0.5580\n",
      "Epoch: 032,train_loss:1.3579843, Train: 0.6667, Val: 0.5240, Test: 0.5670\n",
      "Epoch: 033,train_loss:1.2939996, Train: 0.6667, Val: 0.5240, Test: 0.5530\n",
      "Epoch: 034,train_loss:1.2449946, Train: 0.6917, Val: 0.5060, Test: 0.5350\n",
      "Epoch: 035,train_loss:1.1846793, Train: 0.7167, Val: 0.5100, Test: 0.5260\n",
      "Epoch: 036,train_loss:1.1448853, Train: 0.7583, Val: 0.5080, Test: 0.5220\n",
      "Epoch: 037,train_loss:1.1010977, Train: 0.7917, Val: 0.4920, Test: 0.4910\n",
      "Epoch: 038,train_loss:1.0303942, Train: 0.8500, Val: 0.4600, Test: 0.4690\n",
      "Epoch: 039,train_loss:0.9907594, Train: 0.8500, Val: 0.4480, Test: 0.4410\n",
      "Epoch: 040,train_loss:0.9688113, Train: 0.8750, Val: 0.4340, Test: 0.4380\n",
      "Epoch: 041,train_loss:0.8707730, Train: 0.9000, Val: 0.4440, Test: 0.4360\n",
      "Epoch: 042,train_loss:0.8334200, Train: 0.9083, Val: 0.4500, Test: 0.4430\n",
      "Epoch: 043,train_loss:0.8104703, Train: 0.9167, Val: 0.4660, Test: 0.4640\n",
      "Epoch: 044,train_loss:0.7811906, Train: 0.8917, Val: 0.4740, Test: 0.4730\n",
      "Epoch: 045,train_loss:0.7574151, Train: 0.9167, Val: 0.4980, Test: 0.4830\n",
      "Epoch: 046,train_loss:0.6802942, Train: 0.9417, Val: 0.4800, Test: 0.4700\n",
      "Epoch: 047,train_loss:0.6682805, Train: 0.9583, Val: 0.4680, Test: 0.4680\n",
      "Epoch: 048,train_loss:0.6152143, Train: 0.9750, Val: 0.4720, Test: 0.4600\n",
      "Epoch: 049,train_loss:0.6102125, Train: 0.9750, Val: 0.4800, Test: 0.4710\n",
      "Epoch: 050,train_loss:0.5167853, Train: 0.9750, Val: 0.5100, Test: 0.4770\n",
      "Epoch: 051,train_loss:0.4701481, Train: 0.9833, Val: 0.5140, Test: 0.4820\n",
      "Epoch: 052,train_loss:0.4537229, Train: 0.9833, Val: 0.5300, Test: 0.5040\n",
      "Epoch: 053,train_loss:0.4366974, Train: 0.9917, Val: 0.5380, Test: 0.5160\n",
      "Epoch: 054,train_loss:0.4556691, Train: 0.9833, Val: 0.5160, Test: 0.5140\n",
      "Epoch: 055,train_loss:0.4198907, Train: 0.9917, Val: 0.5120, Test: 0.5040\n",
      "Epoch: 056,train_loss:0.3307618, Train: 1.0000, Val: 0.5060, Test: 0.4970\n",
      "Epoch: 057,train_loss:0.3190744, Train: 0.9917, Val: 0.5000, Test: 0.4940\n",
      "Epoch: 058,train_loss:0.3087722, Train: 0.9833, Val: 0.5120, Test: 0.5010\n",
      "Epoch: 059,train_loss:0.2873846, Train: 0.9917, Val: 0.5480, Test: 0.5390\n",
      "Epoch: 060,train_loss:0.2911654, Train: 1.0000, Val: 0.5580, Test: 0.5450\n",
      "Epoch: 061,train_loss:0.2718556, Train: 1.0000, Val: 0.5400, Test: 0.5660\n",
      "Epoch: 062,train_loss:0.2709466, Train: 0.9917, Val: 0.5480, Test: 0.5480\n",
      "Epoch: 063,train_loss:0.2394471, Train: 0.9917, Val: 0.5520, Test: 0.5240\n",
      "Epoch: 064,train_loss:0.2760176, Train: 1.0000, Val: 0.5220, Test: 0.5190\n",
      "Epoch: 065,train_loss:0.2238847, Train: 1.0000, Val: 0.5180, Test: 0.5250\n",
      "Epoch: 066,train_loss:0.2794079, Train: 1.0000, Val: 0.5560, Test: 0.5300\n",
      "Epoch: 067,train_loss:0.2569848, Train: 1.0000, Val: 0.5700, Test: 0.5530\n",
      "Epoch: 068,train_loss:0.2435106, Train: 0.9917, Val: 0.5780, Test: 0.5780\n",
      "Epoch: 069,train_loss:0.2323628, Train: 0.9917, Val: 0.5760, Test: 0.5790\n",
      "Epoch: 070,train_loss:0.2106016, Train: 0.9917, Val: 0.5740, Test: 0.5780\n",
      "Epoch: 071,train_loss:0.1996724, Train: 0.9917, Val: 0.5720, Test: 0.5770\n",
      "Epoch: 072,train_loss:0.1965927, Train: 1.0000, Val: 0.5560, Test: 0.5610\n",
      "Epoch: 073,train_loss:0.1706630, Train: 0.9917, Val: 0.5460, Test: 0.5340\n",
      "Epoch: 074,train_loss:0.1851954, Train: 0.9917, Val: 0.5400, Test: 0.5280\n",
      "Epoch: 075,train_loss:0.1922594, Train: 0.9917, Val: 0.5540, Test: 0.5260\n",
      "Epoch: 076,train_loss:0.1721425, Train: 1.0000, Val: 0.5560, Test: 0.5460\n",
      "Epoch: 077,train_loss:0.1902880, Train: 1.0000, Val: 0.5780, Test: 0.5730\n",
      "Epoch: 078,train_loss:0.1509307, Train: 1.0000, Val: 0.5860, Test: 0.5740\n",
      "Epoch: 079,train_loss:0.1546157, Train: 0.9917, Val: 0.5620, Test: 0.5820\n",
      "Epoch: 080,train_loss:0.1998022, Train: 1.0000, Val: 0.5660, Test: 0.5790\n",
      "Epoch: 081,train_loss:0.2319196, Train: 1.0000, Val: 0.5840, Test: 0.5930\n",
      "Epoch: 082,train_loss:0.1758180, Train: 1.0000, Val: 0.6100, Test: 0.5980\n",
      "Epoch: 083,train_loss:0.1438472, Train: 1.0000, Val: 0.5800, Test: 0.5730\n",
      "Epoch: 084,train_loss:0.1651346, Train: 1.0000, Val: 0.5540, Test: 0.5490\n",
      "Epoch: 085,train_loss:0.1729514, Train: 1.0000, Val: 0.5600, Test: 0.5310\n",
      "Epoch: 086,train_loss:0.1893363, Train: 1.0000, Val: 0.5700, Test: 0.5440\n",
      "Epoch: 087,train_loss:0.1649825, Train: 1.0000, Val: 0.5640, Test: 0.5550\n",
      "Epoch: 088,train_loss:0.1379584, Train: 1.0000, Val: 0.5740, Test: 0.5640\n",
      "Epoch: 089,train_loss:0.1605475, Train: 1.0000, Val: 0.5780, Test: 0.5910\n",
      "Epoch: 090,train_loss:0.1748904, Train: 1.0000, Val: 0.5860, Test: 0.5900\n",
      "Epoch: 091,train_loss:0.1675231, Train: 1.0000, Val: 0.5800, Test: 0.5890\n",
      "Epoch: 092,train_loss:0.1390608, Train: 1.0000, Val: 0.5960, Test: 0.5890\n",
      "Epoch: 093,train_loss:0.1386642, Train: 1.0000, Val: 0.5940, Test: 0.5940\n",
      "Epoch: 094,train_loss:0.1394882, Train: 1.0000, Val: 0.5980, Test: 0.5870\n",
      "Epoch: 095,train_loss:0.1423764, Train: 1.0000, Val: 0.6000, Test: 0.5850\n",
      "Epoch: 096,train_loss:0.1672880, Train: 1.0000, Val: 0.5940, Test: 0.5820\n",
      "Epoch: 097,train_loss:0.1561550, Train: 1.0000, Val: 0.5820, Test: 0.5770\n",
      "Epoch: 098,train_loss:0.1285269, Train: 1.0000, Val: 0.5920, Test: 0.5900\n",
      "Epoch: 099,train_loss:0.1198410, Train: 1.0000, Val: 0.5940, Test: 0.6020\n",
      "Epoch: 100,train_loss:0.1308713, Train: 1.0000, Val: 0.6040, Test: 0.6080\n",
      "Epoch: 101,train_loss:0.1066188, Train: 1.0000, Val: 0.6160, Test: 0.6010\n",
      "Epoch: 102,train_loss:0.1225489, Train: 1.0000, Val: 0.6100, Test: 0.5960\n",
      "Epoch: 103,train_loss:0.1501517, Train: 1.0000, Val: 0.6160, Test: 0.5960\n",
      "Epoch: 104,train_loss:0.1364310, Train: 1.0000, Val: 0.6160, Test: 0.5930\n",
      "Epoch: 105,train_loss:0.1201986, Train: 1.0000, Val: 0.6060, Test: 0.5980\n",
      "Epoch: 106,train_loss:0.1362483, Train: 1.0000, Val: 0.5880, Test: 0.5910\n",
      "Epoch: 107,train_loss:0.1281485, Train: 1.0000, Val: 0.5920, Test: 0.5630\n",
      "Epoch: 108,train_loss:0.1372720, Train: 1.0000, Val: 0.5840, Test: 0.5620\n",
      "Epoch: 109,train_loss:0.1174704, Train: 1.0000, Val: 0.5900, Test: 0.5790\n",
      "Epoch: 110,train_loss:0.1228518, Train: 1.0000, Val: 0.6200, Test: 0.5950\n",
      "Epoch: 111,train_loss:0.1011971, Train: 1.0000, Val: 0.6220, Test: 0.6030\n",
      "Epoch: 112,train_loss:0.1258834, Train: 1.0000, Val: 0.6240, Test: 0.6140\n",
      "Epoch: 113,train_loss:0.1114485, Train: 1.0000, Val: 0.6200, Test: 0.6170\n",
      "Epoch: 114,train_loss:0.1068863, Train: 1.0000, Val: 0.6160, Test: 0.6130\n",
      "Epoch: 115,train_loss:0.1064422, Train: 1.0000, Val: 0.6160, Test: 0.6080\n",
      "Epoch: 116,train_loss:0.0983371, Train: 1.0000, Val: 0.6220, Test: 0.5980\n",
      "Epoch: 117,train_loss:0.0854096, Train: 1.0000, Val: 0.6080, Test: 0.5910\n",
      "Epoch: 118,train_loss:0.1106915, Train: 1.0000, Val: 0.6080, Test: 0.5910\n",
      "Epoch: 119,train_loss:0.1147021, Train: 1.0000, Val: 0.6260, Test: 0.6010\n",
      "Epoch: 120,train_loss:0.0925075, Train: 1.0000, Val: 0.6260, Test: 0.6100\n",
      "Epoch: 121,train_loss:0.0901309, Train: 1.0000, Val: 0.6280, Test: 0.6150\n",
      "Epoch: 122,train_loss:0.1081071, Train: 1.0000, Val: 0.6300, Test: 0.6150\n",
      "Epoch: 123,train_loss:0.0811244, Train: 1.0000, Val: 0.6320, Test: 0.6150\n",
      "Epoch: 124,train_loss:0.1050706, Train: 1.0000, Val: 0.6200, Test: 0.6210\n",
      "Epoch: 125,train_loss:0.1179604, Train: 1.0000, Val: 0.6000, Test: 0.6150\n",
      "Epoch: 126,train_loss:0.0952530, Train: 1.0000, Val: 0.6020, Test: 0.6060\n",
      "Epoch: 127,train_loss:0.0971396, Train: 1.0000, Val: 0.6080, Test: 0.6100\n",
      "Epoch: 128,train_loss:0.0843624, Train: 1.0000, Val: 0.6080, Test: 0.6200\n",
      "Epoch: 129,train_loss:0.0941745, Train: 1.0000, Val: 0.6200, Test: 0.6240\n",
      "Epoch: 130,train_loss:0.0887379, Train: 1.0000, Val: 0.6240, Test: 0.6250\n",
      "Epoch: 131,train_loss:0.0899641, Train: 1.0000, Val: 0.6260, Test: 0.6260\n",
      "Epoch: 132,train_loss:0.1114232, Train: 1.0000, Val: 0.6220, Test: 0.6280\n",
      "Epoch: 133,train_loss:0.0660720, Train: 1.0000, Val: 0.6200, Test: 0.6310\n",
      "Epoch: 134,train_loss:0.0776583, Train: 1.0000, Val: 0.6200, Test: 0.6260\n",
      "Epoch: 135,train_loss:0.0966591, Train: 1.0000, Val: 0.6140, Test: 0.6250\n",
      "Epoch: 136,train_loss:0.1012596, Train: 1.0000, Val: 0.6120, Test: 0.6200\n",
      "Epoch: 137,train_loss:0.0899031, Train: 1.0000, Val: 0.6100, Test: 0.6140\n",
      "Epoch: 138,train_loss:0.0855076, Train: 1.0000, Val: 0.6020, Test: 0.6100\n",
      "Epoch: 139,train_loss:0.0857341, Train: 1.0000, Val: 0.6020, Test: 0.6140\n",
      "Epoch: 140,train_loss:0.0863496, Train: 1.0000, Val: 0.6080, Test: 0.6130\n",
      "Epoch: 141,train_loss:0.0688101, Train: 1.0000, Val: 0.6040, Test: 0.6190\n",
      "Epoch: 142,train_loss:0.0648528, Train: 1.0000, Val: 0.6160, Test: 0.6170\n",
      "Epoch: 143,train_loss:0.0911360, Train: 1.0000, Val: 0.6200, Test: 0.6160\n",
      "Epoch: 144,train_loss:0.0802102, Train: 1.0000, Val: 0.6320, Test: 0.6160\n",
      "Epoch: 145,train_loss:0.0920592, Train: 1.0000, Val: 0.6360, Test: 0.6230\n",
      "Epoch: 146,train_loss:0.0643483, Train: 1.0000, Val: 0.6360, Test: 0.6330\n",
      "Epoch: 147,train_loss:0.0628834, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 148,train_loss:0.0736772, Train: 1.0000, Val: 0.6420, Test: 0.6420\n",
      "Epoch: 149,train_loss:0.0794332, Train: 1.0000, Val: 0.6360, Test: 0.6380\n",
      "Epoch: 150,train_loss:0.0711237, Train: 1.0000, Val: 0.6280, Test: 0.6330\n",
      "Epoch: 151,train_loss:0.0749302, Train: 1.0000, Val: 0.6220, Test: 0.6220\n",
      "Epoch: 152,train_loss:0.0562650, Train: 1.0000, Val: 0.6100, Test: 0.6050\n",
      "Epoch: 153,train_loss:0.0858063, Train: 1.0000, Val: 0.6100, Test: 0.6000\n",
      "Epoch: 154,train_loss:0.0801723, Train: 1.0000, Val: 0.6340, Test: 0.6140\n",
      "Epoch: 155,train_loss:0.0617166, Train: 1.0000, Val: 0.6300, Test: 0.6100\n",
      "Epoch: 156,train_loss:0.0608552, Train: 1.0000, Val: 0.6300, Test: 0.6250\n",
      "Epoch: 157,train_loss:0.0745640, Train: 1.0000, Val: 0.6280, Test: 0.6370\n",
      "Epoch: 158,train_loss:0.0587296, Train: 1.0000, Val: 0.6200, Test: 0.6360\n",
      "Epoch: 159,train_loss:0.0812168, Train: 1.0000, Val: 0.6200, Test: 0.6400\n",
      "Epoch: 160,train_loss:0.0906770, Train: 1.0000, Val: 0.6240, Test: 0.6410\n",
      "Epoch: 161,train_loss:0.0557468, Train: 1.0000, Val: 0.6340, Test: 0.6230\n",
      "Epoch: 162,train_loss:0.0890157, Train: 1.0000, Val: 0.6240, Test: 0.6060\n",
      "Epoch: 163,train_loss:0.0804895, Train: 1.0000, Val: 0.6320, Test: 0.6150\n",
      "Epoch: 164,train_loss:0.0659375, Train: 1.0000, Val: 0.6520, Test: 0.6300\n",
      "Epoch: 165,train_loss:0.0630701, Train: 1.0000, Val: 0.6400, Test: 0.6360\n",
      "Epoch: 166,train_loss:0.0811633, Train: 1.0000, Val: 0.6240, Test: 0.6430\n",
      "Epoch: 167,train_loss:0.0582083, Train: 1.0000, Val: 0.6200, Test: 0.6420\n",
      "Epoch: 168,train_loss:0.0560358, Train: 1.0000, Val: 0.6140, Test: 0.6320\n",
      "Epoch: 169,train_loss:0.0565186, Train: 1.0000, Val: 0.6160, Test: 0.6380\n",
      "Epoch: 170,train_loss:0.0810349, Train: 1.0000, Val: 0.6140, Test: 0.6240\n",
      "Epoch: 171,train_loss:0.0730214, Train: 1.0000, Val: 0.6320, Test: 0.6160\n",
      "Epoch: 172,train_loss:0.0755256, Train: 1.0000, Val: 0.6340, Test: 0.6200\n",
      "Epoch: 173,train_loss:0.0826990, Train: 1.0000, Val: 0.6460, Test: 0.6220\n",
      "Epoch: 174,train_loss:0.0525241, Train: 1.0000, Val: 0.6300, Test: 0.6280\n",
      "Epoch: 175,train_loss:0.0759374, Train: 1.0000, Val: 0.6340, Test: 0.6370\n",
      "Epoch: 176,train_loss:0.0742536, Train: 1.0000, Val: 0.6100, Test: 0.6280\n",
      "Epoch: 177,train_loss:0.0884333, Train: 1.0000, Val: 0.6160, Test: 0.6280\n",
      "Epoch: 178,train_loss:0.0731743, Train: 1.0000, Val: 0.6200, Test: 0.6370\n",
      "Epoch: 179,train_loss:0.0673662, Train: 1.0000, Val: 0.6300, Test: 0.6290\n",
      "Epoch: 180,train_loss:0.0661629, Train: 1.0000, Val: 0.6340, Test: 0.6150\n",
      "Epoch: 181,train_loss:0.0861946, Train: 1.0000, Val: 0.6440, Test: 0.6210\n",
      "Epoch: 182,train_loss:0.0595463, Train: 1.0000, Val: 0.6320, Test: 0.6200\n",
      "Epoch: 183,train_loss:0.0532596, Train: 1.0000, Val: 0.6240, Test: 0.6140\n",
      "Epoch: 184,train_loss:0.0653205, Train: 1.0000, Val: 0.6040, Test: 0.6070\n",
      "Epoch: 185,train_loss:0.0716513, Train: 1.0000, Val: 0.6040, Test: 0.6190\n",
      "Epoch: 186,train_loss:0.0564243, Train: 1.0000, Val: 0.6200, Test: 0.6260\n",
      "Epoch: 187,train_loss:0.0512041, Train: 1.0000, Val: 0.6160, Test: 0.6400\n",
      "Epoch: 188,train_loss:0.0779412, Train: 1.0000, Val: 0.6220, Test: 0.6390\n",
      "Epoch: 189,train_loss:0.0802555, Train: 1.0000, Val: 0.6300, Test: 0.6260\n",
      "Epoch: 190,train_loss:0.0560519, Train: 1.0000, Val: 0.6420, Test: 0.6210\n",
      "Epoch: 191,train_loss:0.0731215, Train: 1.0000, Val: 0.6400, Test: 0.6210\n",
      "Epoch: 192,train_loss:0.0649665, Train: 1.0000, Val: 0.6400, Test: 0.6300\n",
      "Epoch: 193,train_loss:0.0660868, Train: 1.0000, Val: 0.6240, Test: 0.6260\n",
      "Epoch: 194,train_loss:0.0435723, Train: 1.0000, Val: 0.6040, Test: 0.6250\n",
      "Epoch: 195,train_loss:0.0609543, Train: 1.0000, Val: 0.6020, Test: 0.6180\n",
      "Epoch: 196,train_loss:0.0869787, Train: 1.0000, Val: 0.6080, Test: 0.6160\n",
      "Epoch: 197,train_loss:0.0492919, Train: 1.0000, Val: 0.6240, Test: 0.6100\n",
      "Epoch: 198,train_loss:0.0525012, Train: 1.0000, Val: 0.6220, Test: 0.6140\n",
      "Epoch: 199,train_loss:0.0753875, Train: 1.0000, Val: 0.6160, Test: 0.6000\n",
      "Epoch: 200,train_loss:0.0829977, Train: 1.0000, Val: 0.6300, Test: 0.6340\n",
      "Epoch: 201,train_loss:0.0659347, Train: 1.0000, Val: 0.6320, Test: 0.6370\n",
      "Epoch: 202,train_loss:0.0527723, Train: 1.0000, Val: 0.6340, Test: 0.6420\n",
      "Epoch: 203,train_loss:0.0849504, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 204,train_loss:0.0634521, Train: 1.0000, Val: 0.6060, Test: 0.6310\n",
      "Epoch: 205,train_loss:0.0616055, Train: 1.0000, Val: 0.6200, Test: 0.6170\n",
      "Epoch: 206,train_loss:0.0721610, Train: 1.0000, Val: 0.6180, Test: 0.6200\n",
      "Epoch: 207,train_loss:0.0520844, Train: 1.0000, Val: 0.6220, Test: 0.6090\n",
      "Epoch: 208,train_loss:0.0697019, Train: 1.0000, Val: 0.6320, Test: 0.6190\n",
      "Epoch: 209,train_loss:0.0638341, Train: 1.0000, Val: 0.6340, Test: 0.6400\n",
      "Epoch: 210,train_loss:0.0593953, Train: 1.0000, Val: 0.6380, Test: 0.6420\n",
      "Epoch: 211,train_loss:0.0468777, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 212,train_loss:0.0636347, Train: 1.0000, Val: 0.6260, Test: 0.6420\n",
      "Epoch: 213,train_loss:0.0515334, Train: 1.0000, Val: 0.6240, Test: 0.6370\n",
      "Epoch: 214,train_loss:0.0557568, Train: 1.0000, Val: 0.6160, Test: 0.6200\n",
      "Epoch: 215,train_loss:0.0492944, Train: 1.0000, Val: 0.6220, Test: 0.6170\n",
      "Epoch: 216,train_loss:0.0650606, Train: 1.0000, Val: 0.6260, Test: 0.6150\n",
      "Epoch: 217,train_loss:0.0483628, Train: 1.0000, Val: 0.6240, Test: 0.6170\n",
      "Epoch: 218,train_loss:0.0514231, Train: 1.0000, Val: 0.6260, Test: 0.6270\n",
      "Epoch: 219,train_loss:0.0499268, Train: 1.0000, Val: 0.6160, Test: 0.6350\n",
      "Epoch: 220,train_loss:0.0455802, Train: 1.0000, Val: 0.6240, Test: 0.6390\n",
      "Epoch: 221,train_loss:0.0494924, Train: 1.0000, Val: 0.6240, Test: 0.6340\n",
      "Epoch: 222,train_loss:0.0638060, Train: 1.0000, Val: 0.6160, Test: 0.6300\n",
      "Epoch: 223,train_loss:0.0586640, Train: 1.0000, Val: 0.6200, Test: 0.6220\n",
      "Epoch: 224,train_loss:0.0479844, Train: 1.0000, Val: 0.6280, Test: 0.6140\n",
      "Epoch: 225,train_loss:0.0617836, Train: 1.0000, Val: 0.6240, Test: 0.6000\n",
      "Epoch: 226,train_loss:0.0582425, Train: 1.0000, Val: 0.6240, Test: 0.6200\n",
      "Epoch: 227,train_loss:0.0615080, Train: 1.0000, Val: 0.6440, Test: 0.6400\n",
      "Epoch: 228,train_loss:0.0724524, Train: 1.0000, Val: 0.6400, Test: 0.6510\n",
      "Epoch: 229,train_loss:0.0430520, Train: 1.0000, Val: 0.6400, Test: 0.6510\n",
      "Epoch: 230,train_loss:0.0848433, Train: 1.0000, Val: 0.6220, Test: 0.6430\n",
      "Epoch: 231,train_loss:0.0573583, Train: 1.0000, Val: 0.6120, Test: 0.6250\n",
      "Epoch: 232,train_loss:0.0479306, Train: 1.0000, Val: 0.5980, Test: 0.6170\n",
      "Epoch: 233,train_loss:0.0423716, Train: 1.0000, Val: 0.6100, Test: 0.6090\n",
      "Epoch: 234,train_loss:0.0531918, Train: 1.0000, Val: 0.6220, Test: 0.6170\n",
      "Epoch: 235,train_loss:0.0357002, Train: 1.0000, Val: 0.6360, Test: 0.6200\n",
      "Epoch: 236,train_loss:0.0458752, Train: 1.0000, Val: 0.6400, Test: 0.6190\n",
      "Epoch: 237,train_loss:0.0457655, Train: 1.0000, Val: 0.6380, Test: 0.6370\n",
      "Epoch: 238,train_loss:0.0640341, Train: 1.0000, Val: 0.6480, Test: 0.6370\n",
      "Epoch: 239,train_loss:0.0520425, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 240,train_loss:0.0491781, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 241,train_loss:0.0518463, Train: 1.0000, Val: 0.6260, Test: 0.6380\n",
      "Epoch: 242,train_loss:0.0548602, Train: 1.0000, Val: 0.6280, Test: 0.6430\n",
      "Epoch: 243,train_loss:0.0538897, Train: 1.0000, Val: 0.6340, Test: 0.6440\n",
      "Epoch: 244,train_loss:0.0518445, Train: 1.0000, Val: 0.6420, Test: 0.6490\n",
      "Epoch: 245,train_loss:0.0530233, Train: 1.0000, Val: 0.6480, Test: 0.6510\n",
      "Epoch: 246,train_loss:0.0454354, Train: 1.0000, Val: 0.6560, Test: 0.6480\n",
      "Epoch: 247,train_loss:0.0569557, Train: 1.0000, Val: 0.6460, Test: 0.6380\n",
      "Epoch: 248,train_loss:0.0471655, Train: 1.0000, Val: 0.6360, Test: 0.6300\n",
      "Epoch: 249,train_loss:0.0516899, Train: 1.0000, Val: 0.6220, Test: 0.6220\n",
      "Epoch: 250,train_loss:0.0378135, Train: 1.0000, Val: 0.6140, Test: 0.6130\n",
      "Epoch: 251,train_loss:0.0520439, Train: 1.0000, Val: 0.6260, Test: 0.6270\n",
      "Epoch: 252,train_loss:0.0530603, Train: 1.0000, Val: 0.6320, Test: 0.6390\n",
      "Epoch: 253,train_loss:0.0515695, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 254,train_loss:0.0620175, Train: 1.0000, Val: 0.6440, Test: 0.6410\n",
      "Epoch: 255,train_loss:0.0544375, Train: 1.0000, Val: 0.6400, Test: 0.6150\n",
      "Epoch: 256,train_loss:0.0453595, Train: 1.0000, Val: 0.6180, Test: 0.5930\n",
      "Epoch: 257,train_loss:0.0566609, Train: 1.0000, Val: 0.5880, Test: 0.5820\n",
      "Epoch: 258,train_loss:0.0598250, Train: 1.0000, Val: 0.5960, Test: 0.6030\n",
      "Epoch: 259,train_loss:0.0639469, Train: 1.0000, Val: 0.6240, Test: 0.6230\n",
      "Epoch: 260,train_loss:0.0450552, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 261,train_loss:0.0505267, Train: 1.0000, Val: 0.6340, Test: 0.6300\n",
      "Epoch: 262,train_loss:0.0463792, Train: 1.0000, Val: 0.6220, Test: 0.6260\n",
      "Epoch: 263,train_loss:0.0475813, Train: 1.0000, Val: 0.6280, Test: 0.6200\n",
      "Epoch: 264,train_loss:0.0293268, Train: 1.0000, Val: 0.6260, Test: 0.6290\n",
      "Epoch: 265,train_loss:0.0560742, Train: 1.0000, Val: 0.6340, Test: 0.6050\n",
      "Epoch: 266,train_loss:0.0660227, Train: 1.0000, Val: 0.6320, Test: 0.6040\n",
      "Epoch: 267,train_loss:0.0350962, Train: 1.0000, Val: 0.6420, Test: 0.6200\n",
      "Epoch: 268,train_loss:0.0648084, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 269,train_loss:0.0461227, Train: 1.0000, Val: 0.6380, Test: 0.6470\n",
      "Epoch: 270,train_loss:0.0462292, Train: 1.0000, Val: 0.6460, Test: 0.6500\n",
      "Epoch: 271,train_loss:0.0622781, Train: 1.0000, Val: 0.6480, Test: 0.6470\n",
      "Epoch: 272,train_loss:0.0452090, Train: 1.0000, Val: 0.6400, Test: 0.6360\n",
      "Epoch: 273,train_loss:0.0495423, Train: 1.0000, Val: 0.6400, Test: 0.6100\n",
      "Epoch: 274,train_loss:0.0590773, Train: 1.0000, Val: 0.6340, Test: 0.6000\n",
      "Epoch: 275,train_loss:0.0505171, Train: 1.0000, Val: 0.6100, Test: 0.6110\n",
      "Epoch: 276,train_loss:0.0641417, Train: 1.0000, Val: 0.6220, Test: 0.6300\n",
      "Epoch: 277,train_loss:0.0462398, Train: 1.0000, Val: 0.6340, Test: 0.6380\n",
      "Epoch: 278,train_loss:0.0466593, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 279,train_loss:0.0517455, Train: 1.0000, Val: 0.6440, Test: 0.6450\n",
      "Epoch: 280,train_loss:0.0420569, Train: 1.0000, Val: 0.6340, Test: 0.6400\n",
      "Epoch: 281,train_loss:0.0356446, Train: 1.0000, Val: 0.6260, Test: 0.6310\n",
      "Epoch: 282,train_loss:0.0498549, Train: 1.0000, Val: 0.6180, Test: 0.6120\n",
      "Epoch: 283,train_loss:0.0551785, Train: 1.0000, Val: 0.6280, Test: 0.6180\n",
      "Epoch: 284,train_loss:0.0476286, Train: 1.0000, Val: 0.6260, Test: 0.6200\n",
      "Epoch: 285,train_loss:0.0479247, Train: 1.0000, Val: 0.6380, Test: 0.6450\n",
      "Epoch: 286,train_loss:0.0492131, Train: 1.0000, Val: 0.6420, Test: 0.6490\n",
      "Epoch: 287,train_loss:0.0514647, Train: 1.0000, Val: 0.6420, Test: 0.6450\n",
      "Epoch: 288,train_loss:0.0441790, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 289,train_loss:0.0409855, Train: 1.0000, Val: 0.6380, Test: 0.6310\n",
      "Epoch: 290,train_loss:0.0410361, Train: 1.0000, Val: 0.6260, Test: 0.6210\n",
      "Epoch: 291,train_loss:0.0527350, Train: 1.0000, Val: 0.6060, Test: 0.6170\n",
      "Epoch: 292,train_loss:0.0347739, Train: 1.0000, Val: 0.6060, Test: 0.6230\n",
      "Epoch: 293,train_loss:0.0562003, Train: 1.0000, Val: 0.6400, Test: 0.6240\n",
      "Epoch: 294,train_loss:0.0396337, Train: 1.0000, Val: 0.6360, Test: 0.6300\n",
      "Epoch: 295,train_loss:0.0436133, Train: 1.0000, Val: 0.6500, Test: 0.6430\n",
      "Epoch: 296,train_loss:0.0464201, Train: 1.0000, Val: 0.6500, Test: 0.6520\n",
      "Epoch: 297,train_loss:0.0358162, Train: 1.0000, Val: 0.6440, Test: 0.6420\n",
      "Epoch: 298,train_loss:0.0452136, Train: 1.0000, Val: 0.6260, Test: 0.6370\n",
      "Epoch: 299,train_loss:0.0422343, Train: 1.0000, Val: 0.6280, Test: 0.6390\n",
      "Epoch: 300,train_loss:0.0472758, Train: 1.0000, Val: 0.6340, Test: 0.6330\n",
      "Epoch: 301,train_loss:0.0420449, Train: 1.0000, Val: 0.6280, Test: 0.6320\n",
      "Epoch: 302,train_loss:0.0391258, Train: 1.0000, Val: 0.6240, Test: 0.6080\n",
      "Epoch: 303,train_loss:0.0382015, Train: 1.0000, Val: 0.6300, Test: 0.6080\n",
      "Epoch: 304,train_loss:0.0380097, Train: 1.0000, Val: 0.6380, Test: 0.6260\n",
      "Epoch: 305,train_loss:0.0400365, Train: 1.0000, Val: 0.6380, Test: 0.6410\n",
      "Epoch: 306,train_loss:0.0475440, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 307,train_loss:0.0439908, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 308,train_loss:0.0470740, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 309,train_loss:0.0454417, Train: 1.0000, Val: 0.6320, Test: 0.6340\n",
      "Epoch: 310,train_loss:0.0398183, Train: 1.0000, Val: 0.6260, Test: 0.6270\n",
      "Epoch: 311,train_loss:0.0397171, Train: 1.0000, Val: 0.6260, Test: 0.6250\n",
      "Epoch: 312,train_loss:0.0329937, Train: 1.0000, Val: 0.6240, Test: 0.6250\n",
      "Epoch: 313,train_loss:0.0598260, Train: 1.0000, Val: 0.6300, Test: 0.6410\n",
      "Epoch: 314,train_loss:0.0497608, Train: 1.0000, Val: 0.6360, Test: 0.6480\n",
      "Epoch: 315,train_loss:0.0366685, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 316,train_loss:0.0380455, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 317,train_loss:0.0450942, Train: 1.0000, Val: 0.6280, Test: 0.6460\n",
      "Epoch: 318,train_loss:0.0454863, Train: 1.0000, Val: 0.6380, Test: 0.6340\n",
      "Epoch: 319,train_loss:0.0402563, Train: 1.0000, Val: 0.6220, Test: 0.6240\n",
      "Epoch: 320,train_loss:0.0490812, Train: 1.0000, Val: 0.6280, Test: 0.6240\n",
      "Epoch: 321,train_loss:0.0488933, Train: 1.0000, Val: 0.6200, Test: 0.6200\n",
      "Epoch: 322,train_loss:0.0383497, Train: 1.0000, Val: 0.6320, Test: 0.6400\n",
      "Epoch: 323,train_loss:0.0395464, Train: 1.0000, Val: 0.6340, Test: 0.6500\n",
      "Epoch: 324,train_loss:0.0479410, Train: 1.0000, Val: 0.6260, Test: 0.6390\n",
      "Epoch: 325,train_loss:0.0382277, Train: 1.0000, Val: 0.6200, Test: 0.6330\n",
      "Epoch: 326,train_loss:0.0371197, Train: 1.0000, Val: 0.6140, Test: 0.6290\n",
      "Epoch: 327,train_loss:0.0455933, Train: 1.0000, Val: 0.6080, Test: 0.6250\n",
      "Epoch: 328,train_loss:0.0435498, Train: 1.0000, Val: 0.6240, Test: 0.6260\n",
      "Epoch: 329,train_loss:0.0346804, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 330,train_loss:0.0436278, Train: 1.0000, Val: 0.6480, Test: 0.6470\n",
      "Epoch: 331,train_loss:0.0525907, Train: 1.0000, Val: 0.6440, Test: 0.6510\n",
      "Epoch: 332,train_loss:0.0558871, Train: 1.0000, Val: 0.6240, Test: 0.6420\n",
      "Epoch: 333,train_loss:0.0445226, Train: 1.0000, Val: 0.5940, Test: 0.6130\n",
      "Epoch: 334,train_loss:0.0502713, Train: 1.0000, Val: 0.5780, Test: 0.5960\n",
      "Epoch: 335,train_loss:0.0369400, Train: 1.0000, Val: 0.6040, Test: 0.6080\n",
      "Epoch: 336,train_loss:0.0420319, Train: 1.0000, Val: 0.6340, Test: 0.6350\n",
      "Epoch: 337,train_loss:0.0335278, Train: 1.0000, Val: 0.6480, Test: 0.6470\n",
      "Epoch: 338,train_loss:0.0447482, Train: 1.0000, Val: 0.6580, Test: 0.6560\n",
      "Epoch: 339,train_loss:0.0516480, Train: 1.0000, Val: 0.6340, Test: 0.6550\n",
      "Epoch: 340,train_loss:0.0368807, Train: 1.0000, Val: 0.6020, Test: 0.6280\n",
      "Epoch: 341,train_loss:0.0533668, Train: 1.0000, Val: 0.5900, Test: 0.6140\n",
      "Epoch: 342,train_loss:0.0531557, Train: 1.0000, Val: 0.6180, Test: 0.6200\n",
      "Epoch: 343,train_loss:0.0342721, Train: 1.0000, Val: 0.6380, Test: 0.6280\n",
      "Epoch: 344,train_loss:0.0483416, Train: 1.0000, Val: 0.6400, Test: 0.6360\n",
      "Epoch: 345,train_loss:0.0373824, Train: 1.0000, Val: 0.6400, Test: 0.6470\n",
      "Epoch: 346,train_loss:0.0391644, Train: 1.0000, Val: 0.6540, Test: 0.6560\n",
      "Epoch: 347,train_loss:0.0403429, Train: 1.0000, Val: 0.6360, Test: 0.6600\n",
      "Epoch: 348,train_loss:0.0413744, Train: 1.0000, Val: 0.6380, Test: 0.6560\n",
      "Epoch: 349,train_loss:0.0414751, Train: 1.0000, Val: 0.6320, Test: 0.6420\n",
      "Epoch: 350,train_loss:0.0459802, Train: 1.0000, Val: 0.6240, Test: 0.6250\n",
      "Epoch: 351,train_loss:0.0485452, Train: 1.0000, Val: 0.6220, Test: 0.6220\n",
      "Epoch: 352,train_loss:0.0354983, Train: 1.0000, Val: 0.6200, Test: 0.6150\n",
      "Epoch: 353,train_loss:0.0474666, Train: 1.0000, Val: 0.6260, Test: 0.6350\n",
      "Epoch: 354,train_loss:0.0386149, Train: 1.0000, Val: 0.6400, Test: 0.6420\n",
      "Epoch: 355,train_loss:0.0382050, Train: 1.0000, Val: 0.6460, Test: 0.6510\n",
      "Epoch: 356,train_loss:0.0364106, Train: 1.0000, Val: 0.6460, Test: 0.6540\n",
      "Epoch: 357,train_loss:0.0495032, Train: 1.0000, Val: 0.6320, Test: 0.6480\n",
      "Epoch: 358,train_loss:0.0393305, Train: 1.0000, Val: 0.6240, Test: 0.6340\n",
      "Epoch: 359,train_loss:0.0436982, Train: 1.0000, Val: 0.6200, Test: 0.6200\n",
      "Epoch: 360,train_loss:0.0410936, Train: 1.0000, Val: 0.6320, Test: 0.6190\n",
      "Epoch: 361,train_loss:0.0347203, Train: 1.0000, Val: 0.6400, Test: 0.6250\n",
      "Epoch: 362,train_loss:0.0565954, Train: 1.0000, Val: 0.6480, Test: 0.6500\n",
      "Epoch: 363,train_loss:0.0439490, Train: 1.0000, Val: 0.6520, Test: 0.6620\n",
      "Epoch: 364,train_loss:0.0469937, Train: 1.0000, Val: 0.6480, Test: 0.6540\n",
      "Epoch: 365,train_loss:0.0417832, Train: 1.0000, Val: 0.6480, Test: 0.6460\n",
      "Epoch: 366,train_loss:0.0368138, Train: 1.0000, Val: 0.6440, Test: 0.6420\n",
      "Epoch: 367,train_loss:0.0361060, Train: 1.0000, Val: 0.6280, Test: 0.6210\n",
      "Epoch: 368,train_loss:0.0413763, Train: 1.0000, Val: 0.6240, Test: 0.6160\n",
      "Epoch: 369,train_loss:0.0423205, Train: 1.0000, Val: 0.6280, Test: 0.6250\n",
      "Epoch: 370,train_loss:0.0319611, Train: 1.0000, Val: 0.6440, Test: 0.6420\n",
      "Epoch: 371,train_loss:0.0381272, Train: 1.0000, Val: 0.6420, Test: 0.6430\n",
      "Epoch: 372,train_loss:0.0339221, Train: 1.0000, Val: 0.6420, Test: 0.6470\n",
      "Epoch: 373,train_loss:0.0351246, Train: 1.0000, Val: 0.6300, Test: 0.6420\n",
      "Epoch: 374,train_loss:0.0382385, Train: 1.0000, Val: 0.6340, Test: 0.6420\n",
      "Epoch: 375,train_loss:0.0439226, Train: 1.0000, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 376,train_loss:0.0345578, Train: 1.0000, Val: 0.6360, Test: 0.6280\n",
      "Epoch: 377,train_loss:0.0389369, Train: 1.0000, Val: 0.6400, Test: 0.6210\n",
      "Epoch: 378,train_loss:0.0412040, Train: 1.0000, Val: 0.6380, Test: 0.6220\n",
      "Epoch: 379,train_loss:0.0341344, Train: 1.0000, Val: 0.6260, Test: 0.6220\n",
      "Epoch: 380,train_loss:0.0308266, Train: 1.0000, Val: 0.6380, Test: 0.6370\n",
      "Epoch: 381,train_loss:0.0413757, Train: 1.0000, Val: 0.6360, Test: 0.6430\n",
      "Epoch: 382,train_loss:0.0297246, Train: 1.0000, Val: 0.6400, Test: 0.6430\n",
      "Epoch: 383,train_loss:0.0388017, Train: 1.0000, Val: 0.6460, Test: 0.6490\n",
      "Epoch: 384,train_loss:0.0524907, Train: 1.0000, Val: 0.6520, Test: 0.6450\n",
      "Epoch: 385,train_loss:0.0413717, Train: 1.0000, Val: 0.6320, Test: 0.6270\n",
      "Epoch: 386,train_loss:0.0373460, Train: 1.0000, Val: 0.6260, Test: 0.6170\n",
      "Epoch: 387,train_loss:0.0443247, Train: 1.0000, Val: 0.6340, Test: 0.6230\n",
      "Epoch: 388,train_loss:0.0563841, Train: 1.0000, Val: 0.6300, Test: 0.6400\n",
      "Epoch: 389,train_loss:0.0421013, Train: 1.0000, Val: 0.6160, Test: 0.6200\n",
      "Epoch: 390,train_loss:0.0558209, Train: 1.0000, Val: 0.6220, Test: 0.6260\n",
      "Epoch: 391,train_loss:0.0440282, Train: 1.0000, Val: 0.6400, Test: 0.6480\n",
      "Epoch: 392,train_loss:0.0397138, Train: 1.0000, Val: 0.6400, Test: 0.6410\n",
      "Epoch: 393,train_loss:0.0361041, Train: 1.0000, Val: 0.6220, Test: 0.6190\n",
      "Epoch: 394,train_loss:0.0397047, Train: 1.0000, Val: 0.6180, Test: 0.6190\n",
      "Epoch: 395,train_loss:0.0370311, Train: 1.0000, Val: 0.6380, Test: 0.6280\n",
      "Epoch: 396,train_loss:0.0322641, Train: 1.0000, Val: 0.6240, Test: 0.6310\n",
      "Epoch: 397,train_loss:0.0383107, Train: 1.0000, Val: 0.6140, Test: 0.6300\n",
      "Epoch: 398,train_loss:0.0406300, Train: 1.0000, Val: 0.6120, Test: 0.6250\n",
      "Epoch: 399,train_loss:0.0385504, Train: 1.0000, Val: 0.6260, Test: 0.6370\n",
      "Epoch: 400,train_loss:0.0379693, Train: 1.0000, Val: 0.6340, Test: 0.6280\n",
      "Epoch: 401,train_loss:0.0334916, Train: 1.0000, Val: 0.6200, Test: 0.6040\n",
      "Epoch: 402,train_loss:0.0477649, Train: 1.0000, Val: 0.6220, Test: 0.6120\n",
      "Epoch: 403,train_loss:0.0447707, Train: 1.0000, Val: 0.6460, Test: 0.6380\n",
      "Epoch: 404,train_loss:0.0418943, Train: 1.0000, Val: 0.6340, Test: 0.6490\n",
      "Epoch: 405,train_loss:0.0500396, Train: 1.0000, Val: 0.6480, Test: 0.6480\n",
      "Epoch: 406,train_loss:0.0427244, Train: 1.0000, Val: 0.6420, Test: 0.6410\n",
      "Epoch: 407,train_loss:0.0350348, Train: 1.0000, Val: 0.6280, Test: 0.6370\n",
      "Epoch: 408,train_loss:0.0471479, Train: 1.0000, Val: 0.6140, Test: 0.6370\n",
      "Epoch: 409,train_loss:0.0280478, Train: 1.0000, Val: 0.6280, Test: 0.6310\n",
      "Epoch: 410,train_loss:0.0304241, Train: 1.0000, Val: 0.6200, Test: 0.6220\n",
      "Epoch: 411,train_loss:0.0326800, Train: 1.0000, Val: 0.6300, Test: 0.6210\n",
      "Epoch: 412,train_loss:0.0404570, Train: 1.0000, Val: 0.6320, Test: 0.6300\n",
      "Epoch: 413,train_loss:0.0329521, Train: 1.0000, Val: 0.6460, Test: 0.6410\n",
      "Epoch: 414,train_loss:0.0334510, Train: 1.0000, Val: 0.6500, Test: 0.6510\n",
      "Epoch: 415,train_loss:0.0302556, Train: 1.0000, Val: 0.6340, Test: 0.6480\n",
      "Epoch: 416,train_loss:0.0266377, Train: 1.0000, Val: 0.6220, Test: 0.6300\n",
      "Epoch: 417,train_loss:0.0463048, Train: 1.0000, Val: 0.6220, Test: 0.6410\n",
      "Epoch: 418,train_loss:0.0367461, Train: 1.0000, Val: 0.6340, Test: 0.6290\n",
      "Epoch: 419,train_loss:0.0317248, Train: 1.0000, Val: 0.6420, Test: 0.6400\n",
      "Epoch: 420,train_loss:0.0393867, Train: 1.0000, Val: 0.6440, Test: 0.6390\n",
      "Epoch: 421,train_loss:0.0503656, Train: 1.0000, Val: 0.6460, Test: 0.6400\n",
      "Epoch: 422,train_loss:0.0332727, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 423,train_loss:0.0301061, Train: 1.0000, Val: 0.6060, Test: 0.6270\n",
      "Epoch: 424,train_loss:0.0294765, Train: 1.0000, Val: 0.6120, Test: 0.6210\n",
      "Epoch: 425,train_loss:0.0415685, Train: 1.0000, Val: 0.6300, Test: 0.6320\n",
      "Epoch: 426,train_loss:0.0401543, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 427,train_loss:0.0402499, Train: 1.0000, Val: 0.6400, Test: 0.6270\n",
      "Epoch: 428,train_loss:0.0361735, Train: 1.0000, Val: 0.6080, Test: 0.6100\n",
      "Epoch: 429,train_loss:0.0361811, Train: 1.0000, Val: 0.6040, Test: 0.5970\n",
      "Epoch: 430,train_loss:0.0425937, Train: 1.0000, Val: 0.6100, Test: 0.5950\n",
      "Epoch: 431,train_loss:0.0475338, Train: 1.0000, Val: 0.6240, Test: 0.6260\n",
      "Epoch: 432,train_loss:0.0444603, Train: 1.0000, Val: 0.6160, Test: 0.6250\n",
      "Epoch: 433,train_loss:0.0443308, Train: 1.0000, Val: 0.6180, Test: 0.6310\n",
      "Epoch: 434,train_loss:0.0317858, Train: 1.0000, Val: 0.6200, Test: 0.6320\n",
      "Epoch: 435,train_loss:0.0386164, Train: 1.0000, Val: 0.6400, Test: 0.6410\n",
      "Epoch: 436,train_loss:0.0351751, Train: 1.0000, Val: 0.6520, Test: 0.6360\n",
      "Epoch: 437,train_loss:0.0344457, Train: 1.0000, Val: 0.6420, Test: 0.6300\n",
      "Epoch: 438,train_loss:0.0353885, Train: 1.0000, Val: 0.6400, Test: 0.6280\n",
      "Epoch: 439,train_loss:0.0374810, Train: 1.0000, Val: 0.6320, Test: 0.6260\n",
      "Epoch: 440,train_loss:0.0382667, Train: 1.0000, Val: 0.6200, Test: 0.6350\n",
      "Epoch: 441,train_loss:0.0315113, Train: 1.0000, Val: 0.6000, Test: 0.6250\n",
      "Epoch: 442,train_loss:0.0321583, Train: 1.0000, Val: 0.6040, Test: 0.6230\n",
      "Epoch: 443,train_loss:0.0420537, Train: 1.0000, Val: 0.6180, Test: 0.6330\n",
      "Epoch: 444,train_loss:0.0313832, Train: 1.0000, Val: 0.6480, Test: 0.6420\n",
      "Epoch: 445,train_loss:0.0311839, Train: 1.0000, Val: 0.6580, Test: 0.6520\n",
      "Epoch: 446,train_loss:0.0364304, Train: 1.0000, Val: 0.6320, Test: 0.6230\n",
      "Epoch: 447,train_loss:0.0401295, Train: 1.0000, Val: 0.6360, Test: 0.6250\n",
      "Epoch: 448,train_loss:0.0417697, Train: 1.0000, Val: 0.6380, Test: 0.6320\n",
      "Epoch: 449,train_loss:0.0256582, Train: 1.0000, Val: 0.6260, Test: 0.6160\n",
      "Epoch: 450,train_loss:0.0293721, Train: 1.0000, Val: 0.6000, Test: 0.6130\n",
      "Epoch: 451,train_loss:0.0399205, Train: 1.0000, Val: 0.6200, Test: 0.6270\n",
      "Epoch: 452,train_loss:0.0466707, Train: 1.0000, Val: 0.6320, Test: 0.6410\n",
      "Epoch: 453,train_loss:0.0386802, Train: 1.0000, Val: 0.6440, Test: 0.6460\n",
      "Epoch: 454,train_loss:0.0337645, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 455,train_loss:0.0365319, Train: 1.0000, Val: 0.6360, Test: 0.6260\n",
      "Epoch: 456,train_loss:0.0327101, Train: 1.0000, Val: 0.6340, Test: 0.6250\n",
      "Epoch: 457,train_loss:0.0398647, Train: 1.0000, Val: 0.6120, Test: 0.6020\n",
      "Epoch: 458,train_loss:0.0429728, Train: 1.0000, Val: 0.6040, Test: 0.6060\n",
      "Epoch: 459,train_loss:0.0277558, Train: 1.0000, Val: 0.6060, Test: 0.6200\n",
      "Epoch: 460,train_loss:0.0547427, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 461,train_loss:0.0321482, Train: 1.0000, Val: 0.6460, Test: 0.6510\n",
      "Epoch: 462,train_loss:0.0298987, Train: 1.0000, Val: 0.6540, Test: 0.6540\n",
      "Epoch: 463,train_loss:0.0345266, Train: 1.0000, Val: 0.6500, Test: 0.6430\n",
      "Epoch: 464,train_loss:0.0273179, Train: 1.0000, Val: 0.6320, Test: 0.6340\n",
      "Epoch: 465,train_loss:0.0361366, Train: 1.0000, Val: 0.6120, Test: 0.6040\n",
      "Epoch: 466,train_loss:0.0582503, Train: 1.0000, Val: 0.6240, Test: 0.6180\n",
      "Epoch: 467,train_loss:0.0361751, Train: 1.0000, Val: 0.6320, Test: 0.6450\n",
      "Epoch: 468,train_loss:0.0349480, Train: 1.0000, Val: 0.6260, Test: 0.6480\n",
      "Epoch: 469,train_loss:0.0273262, Train: 1.0000, Val: 0.6180, Test: 0.6470\n",
      "Epoch: 470,train_loss:0.0361164, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 471,train_loss:0.0275382, Train: 1.0000, Val: 0.6460, Test: 0.6430\n",
      "Epoch: 472,train_loss:0.0325552, Train: 1.0000, Val: 0.6280, Test: 0.6230\n",
      "Epoch: 473,train_loss:0.0316515, Train: 1.0000, Val: 0.6140, Test: 0.5920\n",
      "Epoch: 474,train_loss:0.0324725, Train: 1.0000, Val: 0.6080, Test: 0.6070\n",
      "Epoch: 475,train_loss:0.0346658, Train: 1.0000, Val: 0.6160, Test: 0.6280\n",
      "Epoch: 476,train_loss:0.0403316, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 477,train_loss:0.0341129, Train: 1.0000, Val: 0.6260, Test: 0.6530\n",
      "Epoch: 478,train_loss:0.0356810, Train: 1.0000, Val: 0.6260, Test: 0.6490\n",
      "Epoch: 479,train_loss:0.0328480, Train: 1.0000, Val: 0.6320, Test: 0.6480\n",
      "Epoch: 480,train_loss:0.0262362, Train: 1.0000, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 481,train_loss:0.0256926, Train: 1.0000, Val: 0.6340, Test: 0.6310\n",
      "Epoch: 482,train_loss:0.0273224, Train: 1.0000, Val: 0.6260, Test: 0.6230\n",
      "Epoch: 483,train_loss:0.0325049, Train: 1.0000, Val: 0.6320, Test: 0.6270\n",
      "Epoch: 484,train_loss:0.0298888, Train: 1.0000, Val: 0.6340, Test: 0.6380\n",
      "Epoch: 485,train_loss:0.0362455, Train: 1.0000, Val: 0.6460, Test: 0.6480\n",
      "Epoch: 486,train_loss:0.0387037, Train: 1.0000, Val: 0.6360, Test: 0.6460\n",
      "Epoch: 487,train_loss:0.0353415, Train: 1.0000, Val: 0.6260, Test: 0.6440\n",
      "Epoch: 488,train_loss:0.0359128, Train: 1.0000, Val: 0.6280, Test: 0.6430\n",
      "Epoch: 489,train_loss:0.0277009, Train: 1.0000, Val: 0.6340, Test: 0.6420\n",
      "Epoch: 490,train_loss:0.0402977, Train: 1.0000, Val: 0.6440, Test: 0.6400\n",
      "Epoch: 491,train_loss:0.0303058, Train: 1.0000, Val: 0.6480, Test: 0.6410\n",
      "Epoch: 492,train_loss:0.0325285, Train: 1.0000, Val: 0.6440, Test: 0.6410\n",
      "Epoch: 493,train_loss:0.0385585, Train: 1.0000, Val: 0.6340, Test: 0.6360\n",
      "Epoch: 494,train_loss:0.0393167, Train: 1.0000, Val: 0.6260, Test: 0.6330\n",
      "Epoch: 495,train_loss:0.0403063, Train: 1.0000, Val: 0.6360, Test: 0.6390\n",
      "Epoch: 496,train_loss:0.0418134, Train: 1.0000, Val: 0.6380, Test: 0.6350\n",
      "Epoch: 497,train_loss:0.0419022, Train: 1.0000, Val: 0.6440, Test: 0.6490\n",
      "Epoch: 498,train_loss:0.0283858, Train: 1.0000, Val: 0.6380, Test: 0.6510\n",
      "Epoch: 499,train_loss:0.0365484, Train: 1.0000, Val: 0.6480, Test: 0.6480\n",
      "Epoch: 500,train_loss:0.0322780, Train: 1.0000, Val: 0.6320, Test: 0.6410\n",
      "Epoch: 501,train_loss:0.0426747, Train: 1.0000, Val: 0.6100, Test: 0.6280\n",
      "Epoch: 502,train_loss:0.0431033, Train: 1.0000, Val: 0.6320, Test: 0.6340\n",
      "Epoch: 503,train_loss:0.0498814, Train: 1.0000, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 504,train_loss:0.0390625, Train: 1.0000, Val: 0.6480, Test: 0.6550\n",
      "Epoch: 505,train_loss:0.0432064, Train: 1.0000, Val: 0.6360, Test: 0.6520\n",
      "Epoch: 506,train_loss:0.0275784, Train: 1.0000, Val: 0.6240, Test: 0.6420\n",
      "Epoch: 507,train_loss:0.0322330, Train: 1.0000, Val: 0.6240, Test: 0.6270\n",
      "Epoch: 508,train_loss:0.0343796, Train: 1.0000, Val: 0.6220, Test: 0.6360\n",
      "Epoch: 509,train_loss:0.0499529, Train: 1.0000, Val: 0.6160, Test: 0.6310\n",
      "Epoch: 510,train_loss:0.0341502, Train: 1.0000, Val: 0.6380, Test: 0.6360\n",
      "Epoch: 511,train_loss:0.0418060, Train: 1.0000, Val: 0.6320, Test: 0.6470\n",
      "Epoch: 512,train_loss:0.0435044, Train: 1.0000, Val: 0.6320, Test: 0.6420\n",
      "Epoch: 513,train_loss:0.0383542, Train: 1.0000, Val: 0.6280, Test: 0.6240\n",
      "Epoch: 514,train_loss:0.0273059, Train: 1.0000, Val: 0.6140, Test: 0.6230\n",
      "Epoch: 515,train_loss:0.0385546, Train: 1.0000, Val: 0.6060, Test: 0.6190\n",
      "Epoch: 516,train_loss:0.0329481, Train: 1.0000, Val: 0.6100, Test: 0.6280\n",
      "Epoch: 517,train_loss:0.0374369, Train: 1.0000, Val: 0.6180, Test: 0.6270\n",
      "Epoch: 518,train_loss:0.0317628, Train: 1.0000, Val: 0.6180, Test: 0.6370\n",
      "Epoch: 519,train_loss:0.0382753, Train: 1.0000, Val: 0.6240, Test: 0.6330\n",
      "Epoch: 520,train_loss:0.0329831, Train: 1.0000, Val: 0.6280, Test: 0.6310\n",
      "Epoch: 521,train_loss:0.0291752, Train: 1.0000, Val: 0.6240, Test: 0.6260\n",
      "Epoch: 522,train_loss:0.0295914, Train: 1.0000, Val: 0.6240, Test: 0.6290\n",
      "Epoch: 523,train_loss:0.0449349, Train: 1.0000, Val: 0.6240, Test: 0.6330\n",
      "Epoch: 524,train_loss:0.0410337, Train: 1.0000, Val: 0.6280, Test: 0.6350\n",
      "Epoch: 525,train_loss:0.0278560, Train: 1.0000, Val: 0.6200, Test: 0.6360\n",
      "Epoch: 526,train_loss:0.0355928, Train: 1.0000, Val: 0.6120, Test: 0.6360\n",
      "Epoch: 527,train_loss:0.0388633, Train: 1.0000, Val: 0.6260, Test: 0.6390\n",
      "Epoch: 528,train_loss:0.0319703, Train: 1.0000, Val: 0.6280, Test: 0.6260\n",
      "Epoch: 529,train_loss:0.0398246, Train: 1.0000, Val: 0.6240, Test: 0.6310\n",
      "Epoch: 530,train_loss:0.0312478, Train: 1.0000, Val: 0.6260, Test: 0.6260\n",
      "Epoch: 531,train_loss:0.0367560, Train: 1.0000, Val: 0.6320, Test: 0.6410\n",
      "Epoch: 532,train_loss:0.0391833, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 533,train_loss:0.0361731, Train: 1.0000, Val: 0.6280, Test: 0.6400\n",
      "Epoch: 534,train_loss:0.0283834, Train: 1.0000, Val: 0.6220, Test: 0.6360\n",
      "Epoch: 535,train_loss:0.0403058, Train: 1.0000, Val: 0.6280, Test: 0.6280\n",
      "Epoch: 536,train_loss:0.0315122, Train: 1.0000, Val: 0.6300, Test: 0.6310\n",
      "Epoch: 537,train_loss:0.0337760, Train: 1.0000, Val: 0.6300, Test: 0.6270\n",
      "Epoch: 538,train_loss:0.0382442, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 539,train_loss:0.0354732, Train: 1.0000, Val: 0.6400, Test: 0.6500\n",
      "Epoch: 540,train_loss:0.0371740, Train: 1.0000, Val: 0.6400, Test: 0.6500\n",
      "Epoch: 541,train_loss:0.0337502, Train: 1.0000, Val: 0.6400, Test: 0.6340\n",
      "Epoch: 542,train_loss:0.0350991, Train: 1.0000, Val: 0.6240, Test: 0.6200\n",
      "Epoch: 543,train_loss:0.0243190, Train: 1.0000, Val: 0.6080, Test: 0.6050\n",
      "Epoch: 544,train_loss:0.0458901, Train: 1.0000, Val: 0.6220, Test: 0.6190\n",
      "Epoch: 545,train_loss:0.0259806, Train: 1.0000, Val: 0.6300, Test: 0.6350\n",
      "Epoch: 546,train_loss:0.0347873, Train: 1.0000, Val: 0.6360, Test: 0.6460\n",
      "Epoch: 547,train_loss:0.0343153, Train: 1.0000, Val: 0.6400, Test: 0.6490\n",
      "Epoch: 548,train_loss:0.0428580, Train: 1.0000, Val: 0.6280, Test: 0.6520\n",
      "Epoch: 549,train_loss:0.0403821, Train: 1.0000, Val: 0.6260, Test: 0.6400\n",
      "Epoch: 550,train_loss:0.0227137, Train: 1.0000, Val: 0.6140, Test: 0.6260\n",
      "Epoch: 551,train_loss:0.0323952, Train: 1.0000, Val: 0.6020, Test: 0.5920\n",
      "Epoch: 552,train_loss:0.0339319, Train: 1.0000, Val: 0.6140, Test: 0.6080\n",
      "Epoch: 553,train_loss:0.0430054, Train: 1.0000, Val: 0.6220, Test: 0.6290\n",
      "Epoch: 554,train_loss:0.0261286, Train: 1.0000, Val: 0.6260, Test: 0.6390\n",
      "Epoch: 555,train_loss:0.0337394, Train: 1.0000, Val: 0.6200, Test: 0.6370\n",
      "Epoch: 556,train_loss:0.0340019, Train: 1.0000, Val: 0.6240, Test: 0.6450\n",
      "Epoch: 557,train_loss:0.0400061, Train: 1.0000, Val: 0.6280, Test: 0.6480\n",
      "Epoch: 558,train_loss:0.0268753, Train: 1.0000, Val: 0.6340, Test: 0.6530\n",
      "Epoch: 559,train_loss:0.0379474, Train: 1.0000, Val: 0.6320, Test: 0.6460\n",
      "Epoch: 560,train_loss:0.0374834, Train: 1.0000, Val: 0.6100, Test: 0.6270\n",
      "Epoch: 561,train_loss:0.0295012, Train: 1.0000, Val: 0.6040, Test: 0.6100\n",
      "Epoch: 562,train_loss:0.0424219, Train: 1.0000, Val: 0.6100, Test: 0.6180\n",
      "Epoch: 563,train_loss:0.0291118, Train: 1.0000, Val: 0.6320, Test: 0.6310\n",
      "Epoch: 564,train_loss:0.0372235, Train: 1.0000, Val: 0.6340, Test: 0.6470\n",
      "Epoch: 565,train_loss:0.0308561, Train: 1.0000, Val: 0.6380, Test: 0.6490\n",
      "Epoch: 566,train_loss:0.0297678, Train: 1.0000, Val: 0.6320, Test: 0.6530\n",
      "Epoch: 567,train_loss:0.0303755, Train: 1.0000, Val: 0.6400, Test: 0.6470\n",
      "Epoch: 568,train_loss:0.0357304, Train: 1.0000, Val: 0.6200, Test: 0.6350\n",
      "Epoch: 569,train_loss:0.0359417, Train: 1.0000, Val: 0.6220, Test: 0.6090\n",
      "Epoch: 570,train_loss:0.0468487, Train: 1.0000, Val: 0.6260, Test: 0.6180\n",
      "Epoch: 571,train_loss:0.0380498, Train: 1.0000, Val: 0.6360, Test: 0.6380\n",
      "Epoch: 572,train_loss:0.0281587, Train: 1.0000, Val: 0.6500, Test: 0.6480\n",
      "Epoch: 573,train_loss:0.0362821, Train: 1.0000, Val: 0.6280, Test: 0.6490\n",
      "Epoch: 574,train_loss:0.0368283, Train: 1.0000, Val: 0.6300, Test: 0.6520\n",
      "Epoch: 575,train_loss:0.0281537, Train: 1.0000, Val: 0.6400, Test: 0.6400\n",
      "Epoch: 576,train_loss:0.0397308, Train: 1.0000, Val: 0.6360, Test: 0.6200\n",
      "Epoch: 577,train_loss:0.0390601, Train: 1.0000, Val: 0.6180, Test: 0.6050\n",
      "Epoch: 578,train_loss:0.0328997, Train: 1.0000, Val: 0.6300, Test: 0.6170\n",
      "Epoch: 579,train_loss:0.0249180, Train: 1.0000, Val: 0.6440, Test: 0.6360\n",
      "Epoch: 580,train_loss:0.0355427, Train: 1.0000, Val: 0.6500, Test: 0.6450\n",
      "Epoch: 581,train_loss:0.0293429, Train: 1.0000, Val: 0.6280, Test: 0.6490\n",
      "Epoch: 582,train_loss:0.0433680, Train: 1.0000, Val: 0.6280, Test: 0.6400\n",
      "Epoch: 583,train_loss:0.0294084, Train: 1.0000, Val: 0.6200, Test: 0.6320\n",
      "Epoch: 584,train_loss:0.0245197, Train: 1.0000, Val: 0.6160, Test: 0.6080\n",
      "Epoch: 585,train_loss:0.0356417, Train: 1.0000, Val: 0.6220, Test: 0.6130\n",
      "Epoch: 586,train_loss:0.0333948, Train: 1.0000, Val: 0.6360, Test: 0.6320\n",
      "Epoch: 587,train_loss:0.0349402, Train: 1.0000, Val: 0.6500, Test: 0.6520\n",
      "Epoch: 588,train_loss:0.0374150, Train: 1.0000, Val: 0.6460, Test: 0.6470\n",
      "Epoch: 589,train_loss:0.0401835, Train: 1.0000, Val: 0.6400, Test: 0.6380\n",
      "Epoch: 590,train_loss:0.0331926, Train: 1.0000, Val: 0.6380, Test: 0.6360\n",
      "Epoch: 591,train_loss:0.0274752, Train: 1.0000, Val: 0.6360, Test: 0.6390\n",
      "Epoch: 592,train_loss:0.0240527, Train: 1.0000, Val: 0.6340, Test: 0.6320\n",
      "Epoch: 593,train_loss:0.0386603, Train: 1.0000, Val: 0.6300, Test: 0.6330\n",
      "Epoch: 594,train_loss:0.0447268, Train: 1.0000, Val: 0.6400, Test: 0.6320\n",
      "Epoch: 595,train_loss:0.0349081, Train: 1.0000, Val: 0.6480, Test: 0.6460\n",
      "Epoch: 596,train_loss:0.0417070, Train: 1.0000, Val: 0.6420, Test: 0.6430\n",
      "Epoch: 597,train_loss:0.0472832, Train: 1.0000, Val: 0.6240, Test: 0.6460\n",
      "Epoch: 598,train_loss:0.0366436, Train: 1.0000, Val: 0.6200, Test: 0.6280\n",
      "Epoch: 599,train_loss:0.0353363, Train: 1.0000, Val: 0.6220, Test: 0.6260\n",
      "Epoch: 600,train_loss:0.0369835, Train: 1.0000, Val: 0.6280, Test: 0.6360\n",
      "Epoch: 601,train_loss:0.0281973, Train: 1.0000, Val: 0.6340, Test: 0.6270\n",
      "Epoch: 602,train_loss:0.0440013, Train: 1.0000, Val: 0.6220, Test: 0.6250\n",
      "Epoch: 603,train_loss:0.0386558, Train: 1.0000, Val: 0.6200, Test: 0.6270\n",
      "Epoch: 604,train_loss:0.0260130, Train: 1.0000, Val: 0.6220, Test: 0.6300\n",
      "Epoch: 605,train_loss:0.0269069, Train: 1.0000, Val: 0.6280, Test: 0.6380\n",
      "Epoch: 606,train_loss:0.0274977, Train: 1.0000, Val: 0.6280, Test: 0.6350\n",
      "Epoch: 607,train_loss:0.0396052, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 608,train_loss:0.0357237, Train: 1.0000, Val: 0.6300, Test: 0.6320\n",
      "Epoch: 609,train_loss:0.0311966, Train: 1.0000, Val: 0.6360, Test: 0.6350\n",
      "Epoch: 610,train_loss:0.0276416, Train: 1.0000, Val: 0.6300, Test: 0.6320\n",
      "Epoch: 611,train_loss:0.0375291, Train: 1.0000, Val: 0.6400, Test: 0.6320\n",
      "Epoch: 612,train_loss:0.0370560, Train: 1.0000, Val: 0.6260, Test: 0.6420\n",
      "Epoch: 613,train_loss:0.0268717, Train: 1.0000, Val: 0.6380, Test: 0.6440\n",
      "Epoch: 614,train_loss:0.0357200, Train: 1.0000, Val: 0.6260, Test: 0.6430\n",
      "Epoch: 615,train_loss:0.0322783, Train: 1.0000, Val: 0.6280, Test: 0.6380\n",
      "Epoch: 616,train_loss:0.0309169, Train: 1.0000, Val: 0.6280, Test: 0.6340\n",
      "Epoch: 617,train_loss:0.0271270, Train: 1.0000, Val: 0.6260, Test: 0.6280\n",
      "Epoch: 618,train_loss:0.0403208, Train: 1.0000, Val: 0.6300, Test: 0.6390\n",
      "Epoch: 619,train_loss:0.0314090, Train: 1.0000, Val: 0.6300, Test: 0.6380\n",
      "Epoch: 620,train_loss:0.0326480, Train: 1.0000, Val: 0.6340, Test: 0.6490\n",
      "Epoch: 621,train_loss:0.0337359, Train: 1.0000, Val: 0.6420, Test: 0.6520\n",
      "Epoch: 622,train_loss:0.0276202, Train: 1.0000, Val: 0.6500, Test: 0.6460\n",
      "Epoch: 623,train_loss:0.0312329, Train: 1.0000, Val: 0.6340, Test: 0.6390\n",
      "Epoch: 624,train_loss:0.0305518, Train: 1.0000, Val: 0.6340, Test: 0.6290\n",
      "Epoch: 625,train_loss:0.0370153, Train: 1.0000, Val: 0.6220, Test: 0.6230\n",
      "Epoch: 626,train_loss:0.0266684, Train: 1.0000, Val: 0.6100, Test: 0.6190\n",
      "Epoch: 627,train_loss:0.0351847, Train: 1.0000, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 628,train_loss:0.0389495, Train: 1.0000, Val: 0.6400, Test: 0.6510\n",
      "Epoch: 629,train_loss:0.0259471, Train: 1.0000, Val: 0.6460, Test: 0.6480\n",
      "Epoch: 630,train_loss:0.0344593, Train: 1.0000, Val: 0.6460, Test: 0.6470\n",
      "Epoch: 631,train_loss:0.0416418, Train: 1.0000, Val: 0.6300, Test: 0.6320\n",
      "Epoch: 632,train_loss:0.0377046, Train: 1.0000, Val: 0.6000, Test: 0.6210\n",
      "Epoch: 633,train_loss:0.0369974, Train: 1.0000, Val: 0.6020, Test: 0.6140\n",
      "Epoch: 634,train_loss:0.0387865, Train: 1.0000, Val: 0.6160, Test: 0.6280\n",
      "Epoch: 635,train_loss:0.0309541, Train: 1.0000, Val: 0.6480, Test: 0.6460\n",
      "Epoch: 636,train_loss:0.0412313, Train: 1.0000, Val: 0.6520, Test: 0.6470\n",
      "Epoch: 637,train_loss:0.0306680, Train: 1.0000, Val: 0.6380, Test: 0.6520\n",
      "Epoch: 638,train_loss:0.0319138, Train: 1.0000, Val: 0.6400, Test: 0.6460\n",
      "Epoch: 639,train_loss:0.0309934, Train: 1.0000, Val: 0.6220, Test: 0.6280\n",
      "Epoch: 640,train_loss:0.0307157, Train: 1.0000, Val: 0.6000, Test: 0.6210\n",
      "Epoch: 641,train_loss:0.0557909, Train: 1.0000, Val: 0.6180, Test: 0.6160\n",
      "Epoch: 642,train_loss:0.0403835, Train: 1.0000, Val: 0.6220, Test: 0.6330\n",
      "Epoch: 643,train_loss:0.0334915, Train: 1.0000, Val: 0.6380, Test: 0.6410\n",
      "Epoch: 644,train_loss:0.0277774, Train: 1.0000, Val: 0.6500, Test: 0.6450\n",
      "Epoch: 645,train_loss:0.0333909, Train: 1.0000, Val: 0.6320, Test: 0.6390\n",
      "Epoch: 646,train_loss:0.0303124, Train: 1.0000, Val: 0.6260, Test: 0.6230\n",
      "Epoch: 647,train_loss:0.0395976, Train: 1.0000, Val: 0.6020, Test: 0.6200\n",
      "Epoch: 648,train_loss:0.0323201, Train: 1.0000, Val: 0.6140, Test: 0.6290\n",
      "Epoch: 649,train_loss:0.0301947, Train: 1.0000, Val: 0.6200, Test: 0.6310\n",
      "Epoch: 650,train_loss:0.0326399, Train: 1.0000, Val: 0.6340, Test: 0.6410\n",
      "Epoch: 651,train_loss:0.0373555, Train: 1.0000, Val: 0.6360, Test: 0.6430\n",
      "Epoch: 652,train_loss:0.0288123, Train: 1.0000, Val: 0.6460, Test: 0.6500\n",
      "Epoch: 653,train_loss:0.0365819, Train: 1.0000, Val: 0.6340, Test: 0.6470\n",
      "Epoch: 654,train_loss:0.0370720, Train: 1.0000, Val: 0.6160, Test: 0.6160\n",
      "Epoch: 655,train_loss:0.0459760, Train: 1.0000, Val: 0.6160, Test: 0.6270\n",
      "Epoch: 656,train_loss:0.0344582, Train: 1.0000, Val: 0.6420, Test: 0.6330\n",
      "Epoch: 657,train_loss:0.0309883, Train: 1.0000, Val: 0.6240, Test: 0.6210\n",
      "Epoch: 658,train_loss:0.0333380, Train: 1.0000, Val: 0.6220, Test: 0.6220\n",
      "Epoch: 659,train_loss:0.0426883, Train: 1.0000, Val: 0.6520, Test: 0.6430\n",
      "Epoch: 660,train_loss:0.0254354, Train: 1.0000, Val: 0.6480, Test: 0.6440\n",
      "Epoch: 661,train_loss:0.0362515, Train: 1.0000, Val: 0.6420, Test: 0.6490\n",
      "Epoch: 662,train_loss:0.0298955, Train: 1.0000, Val: 0.6440, Test: 0.6400\n",
      "Epoch: 663,train_loss:0.0363273, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 664,train_loss:0.0302909, Train: 1.0000, Val: 0.6300, Test: 0.6320\n",
      "Epoch: 665,train_loss:0.0265710, Train: 1.0000, Val: 0.6200, Test: 0.6120\n",
      "Epoch: 666,train_loss:0.0383521, Train: 1.0000, Val: 0.6300, Test: 0.6380\n",
      "Epoch: 667,train_loss:0.0264091, Train: 1.0000, Val: 0.6380, Test: 0.6390\n",
      "Epoch: 668,train_loss:0.0315187, Train: 1.0000, Val: 0.6480, Test: 0.6450\n",
      "Epoch: 669,train_loss:0.0493081, Train: 1.0000, Val: 0.6440, Test: 0.6500\n",
      "Epoch: 670,train_loss:0.0298471, Train: 1.0000, Val: 0.6280, Test: 0.6360\n",
      "Epoch: 671,train_loss:0.0296033, Train: 1.0000, Val: 0.6060, Test: 0.6050\n",
      "Epoch: 672,train_loss:0.0341223, Train: 1.0000, Val: 0.6040, Test: 0.5990\n",
      "Epoch: 673,train_loss:0.0359424, Train: 1.0000, Val: 0.6240, Test: 0.6230\n",
      "Epoch: 674,train_loss:0.0341908, Train: 1.0000, Val: 0.6360, Test: 0.6340\n",
      "Epoch: 675,train_loss:0.0494169, Train: 1.0000, Val: 0.6540, Test: 0.6400\n",
      "Epoch: 676,train_loss:0.0263571, Train: 1.0000, Val: 0.6440, Test: 0.6360\n",
      "Epoch: 677,train_loss:0.0347881, Train: 1.0000, Val: 0.6460, Test: 0.6440\n",
      "Epoch: 678,train_loss:0.0213790, Train: 1.0000, Val: 0.6320, Test: 0.6410\n",
      "Epoch: 679,train_loss:0.0278626, Train: 1.0000, Val: 0.6320, Test: 0.6310\n",
      "Epoch: 680,train_loss:0.0296909, Train: 1.0000, Val: 0.6320, Test: 0.6190\n",
      "Epoch: 681,train_loss:0.0269971, Train: 1.0000, Val: 0.6220, Test: 0.6280\n",
      "Epoch: 682,train_loss:0.0255478, Train: 1.0000, Val: 0.6340, Test: 0.6410\n",
      "Epoch: 683,train_loss:0.0310536, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 684,train_loss:0.0305981, Train: 1.0000, Val: 0.6380, Test: 0.6500\n",
      "Epoch: 685,train_loss:0.0443142, Train: 1.0000, Val: 0.6460, Test: 0.6510\n",
      "Epoch: 686,train_loss:0.0340140, Train: 1.0000, Val: 0.6420, Test: 0.6500\n",
      "Epoch: 687,train_loss:0.0327487, Train: 1.0000, Val: 0.6280, Test: 0.6430\n",
      "Epoch: 688,train_loss:0.0332572, Train: 1.0000, Val: 0.6260, Test: 0.6150\n",
      "Epoch: 689,train_loss:0.0460859, Train: 1.0000, Val: 0.6360, Test: 0.6280\n",
      "Epoch: 690,train_loss:0.0424385, Train: 1.0000, Val: 0.6500, Test: 0.6410\n",
      "Epoch: 691,train_loss:0.0256726, Train: 1.0000, Val: 0.6340, Test: 0.6430\n",
      "Epoch: 692,train_loss:0.0504815, Train: 1.0000, Val: 0.6300, Test: 0.6420\n",
      "Epoch: 693,train_loss:0.0352292, Train: 1.0000, Val: 0.6160, Test: 0.6270\n",
      "Epoch: 694,train_loss:0.0452574, Train: 1.0000, Val: 0.6060, Test: 0.6090\n",
      "Epoch: 695,train_loss:0.0346449, Train: 1.0000, Val: 0.6020, Test: 0.5930\n",
      "Epoch: 696,train_loss:0.0420865, Train: 1.0000, Val: 0.6240, Test: 0.6250\n",
      "Epoch: 697,train_loss:0.0369321, Train: 1.0000, Val: 0.6480, Test: 0.6370\n",
      "Epoch: 698,train_loss:0.0308468, Train: 1.0000, Val: 0.6500, Test: 0.6480\n",
      "Epoch: 699,train_loss:0.0526035, Train: 1.0000, Val: 0.6240, Test: 0.6350\n",
      "Epoch: 700,train_loss:0.0246355, Train: 1.0000, Val: 0.5920, Test: 0.6000\n",
      "Epoch: 701,train_loss:0.0391545, Train: 1.0000, Val: 0.5880, Test: 0.5840\n",
      "Epoch: 702,train_loss:0.0356645, Train: 1.0000, Val: 0.6140, Test: 0.6210\n",
      "Epoch: 703,train_loss:0.0282381, Train: 1.0000, Val: 0.6500, Test: 0.6420\n",
      "Epoch: 704,train_loss:0.0411881, Train: 1.0000, Val: 0.6460, Test: 0.6490\n",
      "Epoch: 705,train_loss:0.0359007, Train: 1.0000, Val: 0.6460, Test: 0.6480\n",
      "Epoch: 706,train_loss:0.0230271, Train: 1.0000, Val: 0.6480, Test: 0.6460\n",
      "Epoch: 707,train_loss:0.0289804, Train: 1.0000, Val: 0.6340, Test: 0.6440\n",
      "Epoch: 708,train_loss:0.0390907, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 709,train_loss:0.0307205, Train: 1.0000, Val: 0.6380, Test: 0.6400\n",
      "Epoch: 710,train_loss:0.0248632, Train: 1.0000, Val: 0.6380, Test: 0.6410\n",
      "Epoch: 711,train_loss:0.0220184, Train: 1.0000, Val: 0.6460, Test: 0.6460\n",
      "Epoch: 712,train_loss:0.0397697, Train: 1.0000, Val: 0.6440, Test: 0.6490\n",
      "Epoch: 713,train_loss:0.0452485, Train: 1.0000, Val: 0.6500, Test: 0.6490\n",
      "Epoch: 714,train_loss:0.0290771, Train: 1.0000, Val: 0.6440, Test: 0.6460\n",
      "Epoch: 715,train_loss:0.0282027, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 716,train_loss:0.0287591, Train: 1.0000, Val: 0.6300, Test: 0.6330\n",
      "Epoch: 717,train_loss:0.0292691, Train: 1.0000, Val: 0.6340, Test: 0.6410\n",
      "Epoch: 718,train_loss:0.0313725, Train: 1.0000, Val: 0.6340, Test: 0.6440\n",
      "Epoch: 719,train_loss:0.0304422, Train: 1.0000, Val: 0.6460, Test: 0.6390\n",
      "Epoch: 720,train_loss:0.0255253, Train: 1.0000, Val: 0.6460, Test: 0.6450\n",
      "Epoch: 721,train_loss:0.0359317, Train: 1.0000, Val: 0.6400, Test: 0.6410\n",
      "Epoch: 722,train_loss:0.0260563, Train: 1.0000, Val: 0.6340, Test: 0.6450\n",
      "Epoch: 723,train_loss:0.0275464, Train: 1.0000, Val: 0.6200, Test: 0.6260\n",
      "Epoch: 724,train_loss:0.0312444, Train: 1.0000, Val: 0.6100, Test: 0.6160\n",
      "Epoch: 725,train_loss:0.0304628, Train: 1.0000, Val: 0.6160, Test: 0.6220\n",
      "Epoch: 726,train_loss:0.0297846, Train: 1.0000, Val: 0.6340, Test: 0.6460\n",
      "Epoch: 727,train_loss:0.0288676, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 728,train_loss:0.0394699, Train: 1.0000, Val: 0.6500, Test: 0.6420\n",
      "Epoch: 729,train_loss:0.0288786, Train: 1.0000, Val: 0.6440, Test: 0.6320\n",
      "Epoch: 730,train_loss:0.0342092, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 731,train_loss:0.0294618, Train: 1.0000, Val: 0.6360, Test: 0.6290\n",
      "Epoch: 732,train_loss:0.0336576, Train: 1.0000, Val: 0.6360, Test: 0.6250\n",
      "Epoch: 733,train_loss:0.0295846, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 734,train_loss:0.0219417, Train: 1.0000, Val: 0.6320, Test: 0.6380\n",
      "Epoch: 735,train_loss:0.0465610, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 736,train_loss:0.0349733, Train: 1.0000, Val: 0.6520, Test: 0.6370\n",
      "Epoch: 737,train_loss:0.0264686, Train: 1.0000, Val: 0.6460, Test: 0.6370\n",
      "Epoch: 738,train_loss:0.0280759, Train: 1.0000, Val: 0.6380, Test: 0.6300\n",
      "Epoch: 739,train_loss:0.0343735, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 740,train_loss:0.0400260, Train: 1.0000, Val: 0.6300, Test: 0.6380\n",
      "Epoch: 741,train_loss:0.0385377, Train: 1.0000, Val: 0.6380, Test: 0.6360\n",
      "Epoch: 742,train_loss:0.0279730, Train: 1.0000, Val: 0.6360, Test: 0.6340\n",
      "Epoch: 743,train_loss:0.0242370, Train: 1.0000, Val: 0.6440, Test: 0.6340\n",
      "Epoch: 744,train_loss:0.0325117, Train: 1.0000, Val: 0.6500, Test: 0.6320\n",
      "Epoch: 745,train_loss:0.0364093, Train: 1.0000, Val: 0.6320, Test: 0.6340\n",
      "Epoch: 746,train_loss:0.0334932, Train: 1.0000, Val: 0.6260, Test: 0.6300\n",
      "Epoch: 747,train_loss:0.0453208, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 748,train_loss:0.0259680, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 749,train_loss:0.0398018, Train: 1.0000, Val: 0.6360, Test: 0.6460\n",
      "Epoch: 750,train_loss:0.0319875, Train: 1.0000, Val: 0.6440, Test: 0.6360\n",
      "Epoch: 751,train_loss:0.0256810, Train: 1.0000, Val: 0.6140, Test: 0.6100\n",
      "Epoch: 752,train_loss:0.0475737, Train: 1.0000, Val: 0.6220, Test: 0.6150\n",
      "Epoch: 753,train_loss:0.0315746, Train: 1.0000, Val: 0.6460, Test: 0.6370\n",
      "Epoch: 754,train_loss:0.0249850, Train: 1.0000, Val: 0.6340, Test: 0.6490\n",
      "Epoch: 755,train_loss:0.0407005, Train: 1.0000, Val: 0.6240, Test: 0.6400\n",
      "Epoch: 756,train_loss:0.0493800, Train: 1.0000, Val: 0.6300, Test: 0.6500\n",
      "Epoch: 757,train_loss:0.0256784, Train: 1.0000, Val: 0.6420, Test: 0.6400\n",
      "Epoch: 758,train_loss:0.0292559, Train: 1.0000, Val: 0.6260, Test: 0.6310\n",
      "Epoch: 759,train_loss:0.0282862, Train: 1.0000, Val: 0.6040, Test: 0.6080\n",
      "Epoch: 760,train_loss:0.0303881, Train: 1.0000, Val: 0.6220, Test: 0.6160\n",
      "Epoch: 761,train_loss:0.0278701, Train: 1.0000, Val: 0.6380, Test: 0.6330\n",
      "Epoch: 762,train_loss:0.0379098, Train: 1.0000, Val: 0.6360, Test: 0.6360\n",
      "Epoch: 763,train_loss:0.0208216, Train: 1.0000, Val: 0.6140, Test: 0.6350\n",
      "Epoch: 764,train_loss:0.0387690, Train: 1.0000, Val: 0.6240, Test: 0.6330\n",
      "Epoch: 765,train_loss:0.0312634, Train: 1.0000, Val: 0.6260, Test: 0.6320\n",
      "Epoch: 766,train_loss:0.0272856, Train: 1.0000, Val: 0.6500, Test: 0.6310\n",
      "Epoch: 767,train_loss:0.0309563, Train: 1.0000, Val: 0.6440, Test: 0.6390\n",
      "Epoch: 768,train_loss:0.0392927, Train: 1.0000, Val: 0.6400, Test: 0.6260\n",
      "Epoch: 769,train_loss:0.0343839, Train: 1.0000, Val: 0.6420, Test: 0.6230\n",
      "Epoch: 770,train_loss:0.0284650, Train: 1.0000, Val: 0.6360, Test: 0.6270\n",
      "Epoch: 771,train_loss:0.0307288, Train: 1.0000, Val: 0.6420, Test: 0.6330\n",
      "Epoch: 772,train_loss:0.0218065, Train: 1.0000, Val: 0.6160, Test: 0.6400\n",
      "Epoch: 773,train_loss:0.0317595, Train: 1.0000, Val: 0.6140, Test: 0.6390\n",
      "Epoch: 774,train_loss:0.0371768, Train: 1.0000, Val: 0.6420, Test: 0.6330\n",
      "Epoch: 775,train_loss:0.0243455, Train: 1.0000, Val: 0.6300, Test: 0.6270\n",
      "Epoch: 776,train_loss:0.0344151, Train: 1.0000, Val: 0.6160, Test: 0.6250\n",
      "Epoch: 777,train_loss:0.0244447, Train: 1.0000, Val: 0.6240, Test: 0.6230\n",
      "Epoch: 778,train_loss:0.0481764, Train: 1.0000, Val: 0.6600, Test: 0.6390\n",
      "Epoch: 779,train_loss:0.0301082, Train: 1.0000, Val: 0.6520, Test: 0.6420\n",
      "Epoch: 780,train_loss:0.0270323, Train: 1.0000, Val: 0.6440, Test: 0.6430\n",
      "Epoch: 781,train_loss:0.0323448, Train: 1.0000, Val: 0.6600, Test: 0.6480\n",
      "Epoch: 782,train_loss:0.0324523, Train: 1.0000, Val: 0.6500, Test: 0.6420\n",
      "Epoch: 783,train_loss:0.0247332, Train: 1.0000, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 784,train_loss:0.0304756, Train: 1.0000, Val: 0.6340, Test: 0.6290\n",
      "Epoch: 785,train_loss:0.0295527, Train: 1.0000, Val: 0.6400, Test: 0.6350\n",
      "Epoch: 786,train_loss:0.0240552, Train: 1.0000, Val: 0.6580, Test: 0.6400\n",
      "Epoch: 787,train_loss:0.0306485, Train: 1.0000, Val: 0.6560, Test: 0.6470\n",
      "Epoch: 788,train_loss:0.0259437, Train: 1.0000, Val: 0.6460, Test: 0.6410\n",
      "Epoch: 789,train_loss:0.0295555, Train: 1.0000, Val: 0.6420, Test: 0.6390\n",
      "Epoch: 790,train_loss:0.0346593, Train: 1.0000, Val: 0.6380, Test: 0.6270\n",
      "Epoch: 791,train_loss:0.0364362, Train: 1.0000, Val: 0.6140, Test: 0.6110\n",
      "Epoch: 792,train_loss:0.0252582, Train: 1.0000, Val: 0.5960, Test: 0.5940\n",
      "Epoch: 793,train_loss:0.0369422, Train: 1.0000, Val: 0.6200, Test: 0.6240\n",
      "Epoch: 794,train_loss:0.0346176, Train: 1.0000, Val: 0.6540, Test: 0.6460\n",
      "Epoch: 795,train_loss:0.0336755, Train: 1.0000, Val: 0.6480, Test: 0.6520\n",
      "Epoch: 796,train_loss:0.0389029, Train: 1.0000, Val: 0.6320, Test: 0.6500\n",
      "Epoch: 797,train_loss:0.0363189, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 798,train_loss:0.0327234, Train: 1.0000, Val: 0.6300, Test: 0.6230\n",
      "Epoch: 799,train_loss:0.0252341, Train: 1.0000, Val: 0.6000, Test: 0.6090\n",
      "Epoch: 800,train_loss:0.0393222, Train: 1.0000, Val: 0.6320, Test: 0.6200\n",
      "Epoch: 801,train_loss:0.0284941, Train: 1.0000, Val: 0.6460, Test: 0.6430\n",
      "Epoch: 802,train_loss:0.0369221, Train: 1.0000, Val: 0.6500, Test: 0.6460\n",
      "Epoch: 803,train_loss:0.0282955, Train: 1.0000, Val: 0.6460, Test: 0.6520\n",
      "Epoch: 804,train_loss:0.0322331, Train: 1.0000, Val: 0.6360, Test: 0.6430\n",
      "Epoch: 805,train_loss:0.0394248, Train: 1.0000, Val: 0.6260, Test: 0.6260\n",
      "Epoch: 806,train_loss:0.0408020, Train: 1.0000, Val: 0.6180, Test: 0.6150\n",
      "Epoch: 807,train_loss:0.0251496, Train: 1.0000, Val: 0.6120, Test: 0.6240\n",
      "Epoch: 808,train_loss:0.0258957, Train: 1.0000, Val: 0.6200, Test: 0.6350\n",
      "Epoch: 809,train_loss:0.0269432, Train: 1.0000, Val: 0.6260, Test: 0.6420\n",
      "Epoch: 810,train_loss:0.0333867, Train: 1.0000, Val: 0.6400, Test: 0.6510\n",
      "Epoch: 811,train_loss:0.0288047, Train: 1.0000, Val: 0.6420, Test: 0.6430\n",
      "Epoch: 812,train_loss:0.0405928, Train: 1.0000, Val: 0.6600, Test: 0.6540\n",
      "Epoch: 813,train_loss:0.0273880, Train: 1.0000, Val: 0.6400, Test: 0.6470\n",
      "Epoch: 814,train_loss:0.0557000, Train: 1.0000, Val: 0.6340, Test: 0.6390\n",
      "Epoch: 815,train_loss:0.0321878, Train: 1.0000, Val: 0.6160, Test: 0.6220\n",
      "Epoch: 816,train_loss:0.0314809, Train: 1.0000, Val: 0.6060, Test: 0.6090\n",
      "Epoch: 817,train_loss:0.0309207, Train: 1.0000, Val: 0.6200, Test: 0.6140\n",
      "Epoch: 818,train_loss:0.0306964, Train: 1.0000, Val: 0.6400, Test: 0.6320\n",
      "Epoch: 819,train_loss:0.0287206, Train: 1.0000, Val: 0.6480, Test: 0.6470\n",
      "Epoch: 820,train_loss:0.0386566, Train: 1.0000, Val: 0.6480, Test: 0.6520\n",
      "Epoch: 821,train_loss:0.0258363, Train: 1.0000, Val: 0.6340, Test: 0.6510\n",
      "Epoch: 822,train_loss:0.0291211, Train: 1.0000, Val: 0.6440, Test: 0.6490\n",
      "Epoch: 823,train_loss:0.0259374, Train: 1.0000, Val: 0.6340, Test: 0.6350\n",
      "Epoch: 824,train_loss:0.0297531, Train: 1.0000, Val: 0.6280, Test: 0.6300\n",
      "Epoch: 825,train_loss:0.0337750, Train: 1.0000, Val: 0.6220, Test: 0.6250\n",
      "Epoch: 826,train_loss:0.0280825, Train: 1.0000, Val: 0.6180, Test: 0.6300\n",
      "Epoch: 827,train_loss:0.0248583, Train: 1.0000, Val: 0.6280, Test: 0.6420\n",
      "Epoch: 828,train_loss:0.0226116, Train: 1.0000, Val: 0.6420, Test: 0.6510\n",
      "Epoch: 829,train_loss:0.0279105, Train: 1.0000, Val: 0.6480, Test: 0.6530\n",
      "Epoch: 830,train_loss:0.0305680, Train: 1.0000, Val: 0.6460, Test: 0.6430\n",
      "Epoch: 831,train_loss:0.0256423, Train: 1.0000, Val: 0.6340, Test: 0.6380\n",
      "Epoch: 832,train_loss:0.0214723, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 833,train_loss:0.0420396, Train: 1.0000, Val: 0.6560, Test: 0.6440\n",
      "Epoch: 834,train_loss:0.0212889, Train: 1.0000, Val: 0.6580, Test: 0.6550\n",
      "Epoch: 835,train_loss:0.0350110, Train: 1.0000, Val: 0.6580, Test: 0.6510\n",
      "Epoch: 836,train_loss:0.0301766, Train: 1.0000, Val: 0.6300, Test: 0.6440\n",
      "Epoch: 837,train_loss:0.0219243, Train: 1.0000, Val: 0.6140, Test: 0.6230\n",
      "Epoch: 838,train_loss:0.0307769, Train: 1.0000, Val: 0.6200, Test: 0.6190\n",
      "Epoch: 839,train_loss:0.0305535, Train: 1.0000, Val: 0.6320, Test: 0.6370\n",
      "Epoch: 840,train_loss:0.0304922, Train: 1.0000, Val: 0.6520, Test: 0.6500\n",
      "Epoch: 841,train_loss:0.0365920, Train: 1.0000, Val: 0.6560, Test: 0.6570\n",
      "Epoch: 842,train_loss:0.0295918, Train: 1.0000, Val: 0.6560, Test: 0.6520\n",
      "Epoch: 843,train_loss:0.0364833, Train: 1.0000, Val: 0.6480, Test: 0.6420\n",
      "Epoch: 844,train_loss:0.0307382, Train: 1.0000, Val: 0.6240, Test: 0.6370\n",
      "Epoch: 845,train_loss:0.0257520, Train: 1.0000, Val: 0.6180, Test: 0.6240\n",
      "Epoch: 846,train_loss:0.0396643, Train: 1.0000, Val: 0.6220, Test: 0.6290\n",
      "Epoch: 847,train_loss:0.0324349, Train: 1.0000, Val: 0.6460, Test: 0.6360\n",
      "Epoch: 848,train_loss:0.0433351, Train: 1.0000, Val: 0.6080, Test: 0.6400\n",
      "Epoch: 849,train_loss:0.0443936, Train: 1.0000, Val: 0.6000, Test: 0.6440\n",
      "Epoch: 850,train_loss:0.0364907, Train: 1.0000, Val: 0.6180, Test: 0.6430\n",
      "Epoch: 851,train_loss:0.0391051, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 852,train_loss:0.0244772, Train: 1.0000, Val: 0.6360, Test: 0.6410\n",
      "Epoch: 853,train_loss:0.0323974, Train: 1.0000, Val: 0.6440, Test: 0.6390\n",
      "Epoch: 854,train_loss:0.0404751, Train: 1.0000, Val: 0.6220, Test: 0.6380\n",
      "Epoch: 855,train_loss:0.0332075, Train: 1.0000, Val: 0.6280, Test: 0.6400\n",
      "Epoch: 856,train_loss:0.0271137, Train: 1.0000, Val: 0.6320, Test: 0.6380\n",
      "Epoch: 857,train_loss:0.0267022, Train: 1.0000, Val: 0.6300, Test: 0.6430\n",
      "Epoch: 858,train_loss:0.0275318, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 859,train_loss:0.0250360, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 860,train_loss:0.0298154, Train: 1.0000, Val: 0.6420, Test: 0.6450\n",
      "Epoch: 861,train_loss:0.0292126, Train: 1.0000, Val: 0.6440, Test: 0.6450\n",
      "Epoch: 862,train_loss:0.0383843, Train: 1.0000, Val: 0.6440, Test: 0.6400\n",
      "Epoch: 863,train_loss:0.0337592, Train: 1.0000, Val: 0.6300, Test: 0.6310\n",
      "Epoch: 864,train_loss:0.0301334, Train: 1.0000, Val: 0.6180, Test: 0.6250\n",
      "Epoch: 865,train_loss:0.0437938, Train: 1.0000, Val: 0.6240, Test: 0.6270\n",
      "Epoch: 866,train_loss:0.0241322, Train: 1.0000, Val: 0.6400, Test: 0.6280\n",
      "Epoch: 867,train_loss:0.0402651, Train: 1.0000, Val: 0.6340, Test: 0.6400\n",
      "Epoch: 868,train_loss:0.0261394, Train: 1.0000, Val: 0.6380, Test: 0.6420\n",
      "Epoch: 869,train_loss:0.0278028, Train: 1.0000, Val: 0.6420, Test: 0.6430\n",
      "Epoch: 870,train_loss:0.0303326, Train: 1.0000, Val: 0.6420, Test: 0.6470\n",
      "Epoch: 871,train_loss:0.0393625, Train: 1.0000, Val: 0.6340, Test: 0.6350\n",
      "Epoch: 872,train_loss:0.0285472, Train: 1.0000, Val: 0.5980, Test: 0.6130\n",
      "Epoch: 873,train_loss:0.0361464, Train: 1.0000, Val: 0.6100, Test: 0.6090\n",
      "Epoch: 874,train_loss:0.0438722, Train: 1.0000, Val: 0.6380, Test: 0.6330\n",
      "Epoch: 875,train_loss:0.0292806, Train: 1.0000, Val: 0.6460, Test: 0.6500\n",
      "Epoch: 876,train_loss:0.0254911, Train: 1.0000, Val: 0.6360, Test: 0.6420\n",
      "Epoch: 877,train_loss:0.0314191, Train: 1.0000, Val: 0.6360, Test: 0.6380\n",
      "Epoch: 878,train_loss:0.0298246, Train: 1.0000, Val: 0.6340, Test: 0.6460\n",
      "Epoch: 879,train_loss:0.0332161, Train: 1.0000, Val: 0.6440, Test: 0.6510\n",
      "Epoch: 880,train_loss:0.0223554, Train: 1.0000, Val: 0.6360, Test: 0.6380\n",
      "Epoch: 881,train_loss:0.0281354, Train: 1.0000, Val: 0.6120, Test: 0.6220\n",
      "Epoch: 882,train_loss:0.0484290, Train: 1.0000, Val: 0.6420, Test: 0.6620\n",
      "Epoch: 883,train_loss:0.0243103, Train: 1.0000, Val: 0.6440, Test: 0.6600\n",
      "Epoch: 884,train_loss:0.0262256, Train: 1.0000, Val: 0.6440, Test: 0.6640\n",
      "Epoch: 885,train_loss:0.0359756, Train: 1.0000, Val: 0.6420, Test: 0.6700\n",
      "Epoch: 886,train_loss:0.0284563, Train: 1.0000, Val: 0.6380, Test: 0.6590\n",
      "Epoch: 887,train_loss:0.0257322, Train: 1.0000, Val: 0.6460, Test: 0.6380\n",
      "Epoch: 888,train_loss:0.0262486, Train: 1.0000, Val: 0.6220, Test: 0.6210\n",
      "Epoch: 889,train_loss:0.0395971, Train: 1.0000, Val: 0.6060, Test: 0.5970\n",
      "Epoch: 890,train_loss:0.0434675, Train: 1.0000, Val: 0.6360, Test: 0.6310\n",
      "Epoch: 891,train_loss:0.0214981, Train: 1.0000, Val: 0.6480, Test: 0.6480\n",
      "Epoch: 892,train_loss:0.0289110, Train: 1.0000, Val: 0.6460, Test: 0.6540\n",
      "Epoch: 893,train_loss:0.0321877, Train: 1.0000, Val: 0.6480, Test: 0.6600\n",
      "Epoch: 894,train_loss:0.0335691, Train: 1.0000, Val: 0.6380, Test: 0.6380\n",
      "Epoch: 895,train_loss:0.0343390, Train: 1.0000, Val: 0.6100, Test: 0.6190\n",
      "Epoch: 896,train_loss:0.0309742, Train: 1.0000, Val: 0.5940, Test: 0.6020\n",
      "Epoch: 897,train_loss:0.0310970, Train: 1.0000, Val: 0.6020, Test: 0.5890\n",
      "Epoch: 898,train_loss:0.0256671, Train: 1.0000, Val: 0.6120, Test: 0.6090\n",
      "Epoch: 899,train_loss:0.0351587, Train: 1.0000, Val: 0.6400, Test: 0.6260\n",
      "Epoch: 900,train_loss:0.0461589, Train: 1.0000, Val: 0.6420, Test: 0.6580\n",
      "Epoch: 901,train_loss:0.0195453, Train: 1.0000, Val: 0.6280, Test: 0.6400\n",
      "Epoch: 902,train_loss:0.0321075, Train: 1.0000, Val: 0.6260, Test: 0.6410\n",
      "Epoch: 903,train_loss:0.0350259, Train: 1.0000, Val: 0.6320, Test: 0.6450\n",
      "Epoch: 904,train_loss:0.0246582, Train: 1.0000, Val: 0.6260, Test: 0.6230\n",
      "Epoch: 905,train_loss:0.0305452, Train: 1.0000, Val: 0.6040, Test: 0.5830\n",
      "Epoch: 906,train_loss:0.0327532, Train: 1.0000, Val: 0.6180, Test: 0.5990\n",
      "Epoch: 907,train_loss:0.0289561, Train: 1.0000, Val: 0.6160, Test: 0.6160\n",
      "Epoch: 908,train_loss:0.0253544, Train: 1.0000, Val: 0.6320, Test: 0.6350\n",
      "Epoch: 909,train_loss:0.0306494, Train: 1.0000, Val: 0.6400, Test: 0.6380\n",
      "Epoch: 910,train_loss:0.0319903, Train: 1.0000, Val: 0.6360, Test: 0.6530\n",
      "Epoch: 911,train_loss:0.0673096, Train: 1.0000, Val: 0.6440, Test: 0.6440\n",
      "Epoch: 912,train_loss:0.0264827, Train: 1.0000, Val: 0.6240, Test: 0.6130\n",
      "Epoch: 913,train_loss:0.0242506, Train: 1.0000, Val: 0.5820, Test: 0.5730\n",
      "Epoch: 914,train_loss:0.0304707, Train: 1.0000, Val: 0.6000, Test: 0.5840\n",
      "Epoch: 915,train_loss:0.0457455, Train: 1.0000, Val: 0.6520, Test: 0.6240\n",
      "Epoch: 916,train_loss:0.0274755, Train: 1.0000, Val: 0.6260, Test: 0.6380\n",
      "Epoch: 917,train_loss:0.0251450, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 918,train_loss:0.0312034, Train: 1.0000, Val: 0.6340, Test: 0.6340\n",
      "Epoch: 919,train_loss:0.0302204, Train: 1.0000, Val: 0.6480, Test: 0.6430\n",
      "Epoch: 920,train_loss:0.0307757, Train: 1.0000, Val: 0.6460, Test: 0.6320\n",
      "Epoch: 921,train_loss:0.0211493, Train: 1.0000, Val: 0.6360, Test: 0.6170\n",
      "Epoch: 922,train_loss:0.0351847, Train: 1.0000, Val: 0.6120, Test: 0.5980\n",
      "Epoch: 923,train_loss:0.0275045, Train: 1.0000, Val: 0.6200, Test: 0.6040\n",
      "Epoch: 924,train_loss:0.0346387, Train: 1.0000, Val: 0.6260, Test: 0.6170\n",
      "Epoch: 925,train_loss:0.0281105, Train: 1.0000, Val: 0.6120, Test: 0.6250\n",
      "Epoch: 926,train_loss:0.0257605, Train: 1.0000, Val: 0.6240, Test: 0.6320\n",
      "Epoch: 927,train_loss:0.0334852, Train: 1.0000, Val: 0.6380, Test: 0.6460\n",
      "Epoch: 928,train_loss:0.0300797, Train: 1.0000, Val: 0.6420, Test: 0.6490\n",
      "Epoch: 929,train_loss:0.0271681, Train: 1.0000, Val: 0.6500, Test: 0.6350\n",
      "Epoch: 930,train_loss:0.0262638, Train: 1.0000, Val: 0.6120, Test: 0.6100\n",
      "Epoch: 931,train_loss:0.0352863, Train: 1.0000, Val: 0.6080, Test: 0.5990\n",
      "Epoch: 932,train_loss:0.0327113, Train: 1.0000, Val: 0.6080, Test: 0.6140\n",
      "Epoch: 933,train_loss:0.0270238, Train: 1.0000, Val: 0.6080, Test: 0.6210\n",
      "Epoch: 934,train_loss:0.0342342, Train: 1.0000, Val: 0.6260, Test: 0.6330\n",
      "Epoch: 935,train_loss:0.0357425, Train: 1.0000, Val: 0.6260, Test: 0.6460\n",
      "Epoch: 936,train_loss:0.0320738, Train: 1.0000, Val: 0.6400, Test: 0.6390\n",
      "Epoch: 937,train_loss:0.0286648, Train: 1.0000, Val: 0.6520, Test: 0.6390\n",
      "Epoch: 938,train_loss:0.0239822, Train: 1.0000, Val: 0.6400, Test: 0.6330\n",
      "Epoch: 939,train_loss:0.0252248, Train: 1.0000, Val: 0.6240, Test: 0.6220\n",
      "Epoch: 940,train_loss:0.0301200, Train: 1.0000, Val: 0.6160, Test: 0.6180\n",
      "Epoch: 941,train_loss:0.0366084, Train: 1.0000, Val: 0.6320, Test: 0.6290\n",
      "Epoch: 942,train_loss:0.0285853, Train: 1.0000, Val: 0.6220, Test: 0.6380\n",
      "Epoch: 943,train_loss:0.0337762, Train: 1.0000, Val: 0.6300, Test: 0.6440\n",
      "Epoch: 944,train_loss:0.0290739, Train: 1.0000, Val: 0.6300, Test: 0.6470\n",
      "Epoch: 945,train_loss:0.0267653, Train: 1.0000, Val: 0.6360, Test: 0.6400\n",
      "Epoch: 946,train_loss:0.0321086, Train: 1.0000, Val: 0.6440, Test: 0.6310\n",
      "Epoch: 947,train_loss:0.0244281, Train: 1.0000, Val: 0.6300, Test: 0.6230\n",
      "Epoch: 948,train_loss:0.0293124, Train: 1.0000, Val: 0.6280, Test: 0.6150\n",
      "Epoch: 949,train_loss:0.0325904, Train: 1.0000, Val: 0.6260, Test: 0.6200\n",
      "Epoch: 950,train_loss:0.0282407, Train: 1.0000, Val: 0.6360, Test: 0.6370\n",
      "Epoch: 951,train_loss:0.0279367, Train: 1.0000, Val: 0.6320, Test: 0.6510\n",
      "Epoch: 952,train_loss:0.0251441, Train: 1.0000, Val: 0.6340, Test: 0.6540\n",
      "Epoch: 953,train_loss:0.0155611, Train: 1.0000, Val: 0.6320, Test: 0.6520\n",
      "Epoch: 954,train_loss:0.0227721, Train: 1.0000, Val: 0.6340, Test: 0.6530\n",
      "Epoch: 955,train_loss:0.0258394, Train: 1.0000, Val: 0.6320, Test: 0.6410\n",
      "Epoch: 956,train_loss:0.0274292, Train: 1.0000, Val: 0.6300, Test: 0.6360\n",
      "Epoch: 957,train_loss:0.0315722, Train: 1.0000, Val: 0.6140, Test: 0.6220\n",
      "Epoch: 958,train_loss:0.0295301, Train: 1.0000, Val: 0.6080, Test: 0.6010\n",
      "Epoch: 959,train_loss:0.0290995, Train: 1.0000, Val: 0.6080, Test: 0.6050\n",
      "Epoch: 960,train_loss:0.0241750, Train: 1.0000, Val: 0.6220, Test: 0.6250\n",
      "Epoch: 961,train_loss:0.0332047, Train: 1.0000, Val: 0.6360, Test: 0.6450\n",
      "Epoch: 962,train_loss:0.0250494, Train: 1.0000, Val: 0.6300, Test: 0.6470\n",
      "Epoch: 963,train_loss:0.0338102, Train: 1.0000, Val: 0.6300, Test: 0.6530\n",
      "Epoch: 964,train_loss:0.0401501, Train: 1.0000, Val: 0.6380, Test: 0.6440\n",
      "Epoch: 965,train_loss:0.0336575, Train: 1.0000, Val: 0.6200, Test: 0.6170\n",
      "Epoch: 966,train_loss:0.0382918, Train: 1.0000, Val: 0.6100, Test: 0.5920\n",
      "Epoch: 967,train_loss:0.0341981, Train: 1.0000, Val: 0.6220, Test: 0.5900\n",
      "Epoch: 968,train_loss:0.0405168, Train: 1.0000, Val: 0.6380, Test: 0.6320\n",
      "Epoch: 969,train_loss:0.0389302, Train: 1.0000, Val: 0.6340, Test: 0.6540\n",
      "Epoch: 970,train_loss:0.0357916, Train: 1.0000, Val: 0.6400, Test: 0.6420\n",
      "Epoch: 971,train_loss:0.0383253, Train: 1.0000, Val: 0.6400, Test: 0.6390\n",
      "Epoch: 972,train_loss:0.0353862, Train: 1.0000, Val: 0.6260, Test: 0.6360\n",
      "Epoch: 973,train_loss:0.0300164, Train: 1.0000, Val: 0.6200, Test: 0.6010\n",
      "Epoch: 974,train_loss:0.0394803, Train: 1.0000, Val: 0.6060, Test: 0.5970\n",
      "Epoch: 975,train_loss:0.0430131, Train: 1.0000, Val: 0.6320, Test: 0.6010\n",
      "Epoch: 976,train_loss:0.0301785, Train: 1.0000, Val: 0.6280, Test: 0.6350\n",
      "Epoch: 977,train_loss:0.0294275, Train: 1.0000, Val: 0.6360, Test: 0.6480\n",
      "Epoch: 978,train_loss:0.0317948, Train: 1.0000, Val: 0.6320, Test: 0.6390\n",
      "Epoch: 979,train_loss:0.0249980, Train: 1.0000, Val: 0.6280, Test: 0.6380\n",
      "Epoch: 980,train_loss:0.0334657, Train: 1.0000, Val: 0.6460, Test: 0.6410\n",
      "Epoch: 981,train_loss:0.0214131, Train: 1.0000, Val: 0.6480, Test: 0.6430\n",
      "Epoch: 982,train_loss:0.0245764, Train: 1.0000, Val: 0.6520, Test: 0.6270\n",
      "Epoch: 983,train_loss:0.0276720, Train: 1.0000, Val: 0.6460, Test: 0.6210\n",
      "Epoch: 984,train_loss:0.0285437, Train: 1.0000, Val: 0.6440, Test: 0.6350\n",
      "Epoch: 985,train_loss:0.0263419, Train: 1.0000, Val: 0.6440, Test: 0.6360\n",
      "Epoch: 986,train_loss:0.0285565, Train: 1.0000, Val: 0.6380, Test: 0.6410\n",
      "Epoch: 987,train_loss:0.0213431, Train: 1.0000, Val: 0.6400, Test: 0.6470\n",
      "Epoch: 988,train_loss:0.0207646, Train: 1.0000, Val: 0.6400, Test: 0.6450\n",
      "Epoch: 989,train_loss:0.0386657, Train: 1.0000, Val: 0.6460, Test: 0.6450\n",
      "Epoch: 990,train_loss:0.0358827, Train: 1.0000, Val: 0.6440, Test: 0.6380\n",
      "Epoch: 991,train_loss:0.0207064, Train: 1.0000, Val: 0.6540, Test: 0.6380\n",
      "Epoch: 992,train_loss:0.0243925, Train: 1.0000, Val: 0.6480, Test: 0.6320\n",
      "Epoch: 993,train_loss:0.0384864, Train: 1.0000, Val: 0.6360, Test: 0.6330\n",
      "Epoch: 994,train_loss:0.0324533, Train: 1.0000, Val: 0.6420, Test: 0.6370\n",
      "Epoch: 995,train_loss:0.0377605, Train: 1.0000, Val: 0.6500, Test: 0.6320\n",
      "Epoch: 996,train_loss:0.0254981, Train: 1.0000, Val: 0.6500, Test: 0.6380\n",
      "Epoch: 997,train_loss:0.0250709, Train: 1.0000, Val: 0.6500, Test: 0.6330\n",
      "Epoch: 998,train_loss:0.0263189, Train: 1.0000, Val: 0.6460, Test: 0.6340\n",
      "Epoch: 999,train_loss:0.0267739, Train: 1.0000, Val: 0.6500, Test: 0.6350\n",
      "Epoch: 1000,train_loss:0.0298322, Train: 1.0000, Val: 0.6460, Test: 0.6320\n",
      "CPU times: user 18.8 s, sys: 908 ms, total: 19.7 s\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "605959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Citeseer'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 1 #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "        # self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_Citeseer_mymodel,testacclist_Citeseer_mymodel=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_Citeseer_mymodel.append(loss)\n",
    "    testacclist_Citeseer_mymodel.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# # %matplotlib inline\n",
    "\n",
    "# plt.plot(losslist)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000Citeseer\n",
    "Epoch: 987,train_loss:0.0229825, Train: 1.0000, Val: 0.6380, Test: 0.6320\n",
    "Epoch: 988,train_loss:0.0355166, Train: 1.0000, Val: 0.6320, Test: 0.6320\n",
    "Epoch: 989,train_loss:0.0285329, Train: 1.0000, Val: 0.6320, Test: 0.6420\n",
    "Epoch: 990,train_loss:0.0338571, Train: 1.0000, Val: 0.6440, Test: 0.6490\n",
    "Epoch: 991,train_loss:0.0253069, Train: 1.0000, Val: 0.6320, Test: 0.6440\n",
    "Epoch: 992,train_loss:0.0278655, Train: 1.0000, Val: 0.6220, Test: 0.6400\n",
    "Epoch: 993,train_loss:0.0319123, Train: 1.0000, Val: 0.6240, Test: 0.6400\n",
    "Epoch: 994,train_loss:0.0279624, Train: 1.0000, Val: 0.6280, Test: 0.6350\n",
    "Epoch: 995,train_loss:0.0284591, Train: 1.0000, Val: 0.6340, Test: 0.6490\n",
    "Epoch: 996,train_loss:0.0282186, Train: 1.0000, Val: 0.6400, Test: 0.6490\n",
    "Epoch: 997,train_loss:0.0270203, Train: 1.0000, Val: 0.6480, Test: 0.6440\n",
    "Epoch: 998,train_loss:0.0306090, Train: 1.0000, Val: 0.6420, Test: 0.6420\n",
    "Epoch: 999,train_loss:0.0247412, Train: 1.0000, Val: 0.6400, Test: 0.6460\n",
    "Epoch: 1000,train_loss:0.0237779, Train: 1.0000, Val: 0.6480, Test: 0.6500\n",
    "CPU times: user 18.1 s, sys: 1.05 s, total: 19.1 s\n",
    "Wall time: 16.7 s\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "sum([torch.numel(param) for param in model.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citeseer geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001,train_loss:1.7927133, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 002,train_loss:1.7921691, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 003,train_loss:1.7916572, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 004,train_loss:1.7917211, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 005,train_loss:1.7910351, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 006,train_loss:1.7907796, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 007,train_loss:1.7900548, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 008,train_loss:1.7899803, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 009,train_loss:1.7895981, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 010,train_loss:1.7884063, Train: 0.1667, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 011,train_loss:1.7877656, Train: 0.1833, Val: 0.2320, Test: 0.1810\n",
      "Epoch: 012,train_loss:1.7866204, Train: 0.3333, Val: 0.3500, Test: 0.3550\n",
      "Epoch: 013,train_loss:1.7851262, Train: 0.2000, Val: 0.2200, Test: 0.2340\n",
      "Epoch: 014,train_loss:1.7818829, Train: 0.1833, Val: 0.2140, Test: 0.2310\n",
      "Epoch: 015,train_loss:1.7795157, Train: 0.1667, Val: 0.2120, Test: 0.2310\n",
      "Epoch: 016,train_loss:1.7748398, Train: 0.2083, Val: 0.2140, Test: 0.2320\n",
      "Epoch: 017,train_loss:1.7702941, Train: 0.3250, Val: 0.2200, Test: 0.2550\n",
      "Epoch: 018,train_loss:1.7542933, Train: 0.3750, Val: 0.2300, Test: 0.2600\n",
      "Epoch: 019,train_loss:1.7337402, Train: 0.4333, Val: 0.2520, Test: 0.2740\n",
      "Epoch: 020,train_loss:1.6885738, Train: 0.4250, Val: 0.2940, Test: 0.2880\n",
      "Epoch: 021,train_loss:1.6504035, Train: 0.3583, Val: 0.3180, Test: 0.2940\n",
      "Epoch: 022,train_loss:1.5751832, Train: 0.4917, Val: 0.4340, Test: 0.3820\n",
      "Epoch: 023,train_loss:1.5445390, Train: 0.4333, Val: 0.4180, Test: 0.3770\n",
      "Epoch: 024,train_loss:1.4262319, Train: 0.4083, Val: 0.4080, Test: 0.3540\n",
      "Epoch: 025,train_loss:1.4008745, Train: 0.3583, Val: 0.3920, Test: 0.3490\n",
      "Epoch: 026,train_loss:1.3184887, Train: 0.3417, Val: 0.3980, Test: 0.3640\n",
      "Epoch: 027,train_loss:1.2816089, Train: 0.3417, Val: 0.4040, Test: 0.3600\n",
      "Epoch: 028,train_loss:1.2751577, Train: 0.3333, Val: 0.4040, Test: 0.3620\n",
      "Epoch: 029,train_loss:1.2082245, Train: 0.3417, Val: 0.4160, Test: 0.3860\n",
      "Epoch: 030,train_loss:1.2207552, Train: 0.3417, Val: 0.4160, Test: 0.3910\n",
      "Epoch: 031,train_loss:1.2096789, Train: 0.4000, Val: 0.4040, Test: 0.3980\n",
      "Epoch: 032,train_loss:1.1748340, Train: 0.4333, Val: 0.3780, Test: 0.3830\n",
      "Epoch: 033,train_loss:1.2018945, Train: 0.4500, Val: 0.3760, Test: 0.3780\n",
      "Epoch: 034,train_loss:1.1537827, Train: 0.5167, Val: 0.3420, Test: 0.3660\n",
      "Epoch: 035,train_loss:1.1685423, Train: 0.4667, Val: 0.2800, Test: 0.3160\n",
      "Epoch: 036,train_loss:1.1824280, Train: 0.5417, Val: 0.2720, Test: 0.3040\n",
      "Epoch: 037,train_loss:1.1136612, Train: 0.6333, Val: 0.2340, Test: 0.2320\n",
      "Epoch: 038,train_loss:1.1039951, Train: 0.6500, Val: 0.1980, Test: 0.1940\n",
      "Epoch: 039,train_loss:1.1696169, Train: 0.5667, Val: 0.2240, Test: 0.2180\n",
      "Epoch: 040,train_loss:1.1300604, Train: 0.7500, Val: 0.2540, Test: 0.2460\n",
      "Epoch: 041,train_loss:1.0155430, Train: 0.7500, Val: 0.2780, Test: 0.3040\n",
      "Epoch: 042,train_loss:1.0323176, Train: 0.7417, Val: 0.2820, Test: 0.3250\n",
      "Epoch: 043,train_loss:1.0371841, Train: 0.7667, Val: 0.2720, Test: 0.3010\n",
      "Epoch: 044,train_loss:1.0242794, Train: 0.7667, Val: 0.2820, Test: 0.2890\n",
      "Epoch: 045,train_loss:0.9774342, Train: 0.6917, Val: 0.3000, Test: 0.2700\n",
      "Epoch: 046,train_loss:1.0200576, Train: 0.7167, Val: 0.2800, Test: 0.2700\n",
      "Epoch: 047,train_loss:0.9250700, Train: 0.8667, Val: 0.2820, Test: 0.3000\n",
      "Epoch: 048,train_loss:0.8500770, Train: 0.8500, Val: 0.2900, Test: 0.3250\n",
      "Epoch: 049,train_loss:0.8803694, Train: 0.8667, Val: 0.3020, Test: 0.3310\n",
      "Epoch: 050,train_loss:0.9138365, Train: 0.8333, Val: 0.3120, Test: 0.3460\n",
      "Epoch: 051,train_loss:0.8935760, Train: 0.8250, Val: 0.3360, Test: 0.3380\n",
      "Epoch: 052,train_loss:0.7237944, Train: 0.8250, Val: 0.3300, Test: 0.3300\n",
      "Epoch: 053,train_loss:0.7670432, Train: 0.8417, Val: 0.3140, Test: 0.3100\n",
      "Epoch: 054,train_loss:0.6669244, Train: 0.8417, Val: 0.3000, Test: 0.3000\n",
      "Epoch: 055,train_loss:0.7230339, Train: 0.8417, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 056,train_loss:0.6818072, Train: 0.8667, Val: 0.3260, Test: 0.3390\n",
      "Epoch: 057,train_loss:0.7287008, Train: 0.9167, Val: 0.3400, Test: 0.3630\n",
      "Epoch: 058,train_loss:0.6681983, Train: 0.9250, Val: 0.3500, Test: 0.3710\n",
      "Epoch: 059,train_loss:0.5453216, Train: 0.8917, Val: 0.3660, Test: 0.3770\n",
      "Epoch: 060,train_loss:0.5811829, Train: 0.8917, Val: 0.3560, Test: 0.3870\n",
      "Epoch: 061,train_loss:0.5383161, Train: 0.9667, Val: 0.3940, Test: 0.4170\n",
      "Epoch: 062,train_loss:0.5695066, Train: 0.9667, Val: 0.4200, Test: 0.4240\n",
      "Epoch: 063,train_loss:0.6220044, Train: 0.8833, Val: 0.4440, Test: 0.4370\n",
      "Epoch: 064,train_loss:0.5196243, Train: 0.8417, Val: 0.4280, Test: 0.4120\n",
      "Epoch: 065,train_loss:0.5938279, Train: 0.8417, Val: 0.4100, Test: 0.3970\n",
      "Epoch: 066,train_loss:0.5643886, Train: 0.8417, Val: 0.3980, Test: 0.3910\n",
      "Epoch: 067,train_loss:0.5435193, Train: 0.8417, Val: 0.4100, Test: 0.4140\n",
      "Epoch: 068,train_loss:0.5507028, Train: 0.8833, Val: 0.4240, Test: 0.4330\n",
      "Epoch: 069,train_loss:0.4742001, Train: 0.9750, Val: 0.4340, Test: 0.4480\n",
      "Epoch: 070,train_loss:0.5007576, Train: 1.0000, Val: 0.4340, Test: 0.4490\n",
      "Epoch: 071,train_loss:0.4281090, Train: 0.9833, Val: 0.4180, Test: 0.4390\n",
      "Epoch: 072,train_loss:0.4807994, Train: 0.9833, Val: 0.4240, Test: 0.4450\n",
      "Epoch: 073,train_loss:0.4458742, Train: 1.0000, Val: 0.4200, Test: 0.4460\n",
      "Epoch: 074,train_loss:0.4561541, Train: 1.0000, Val: 0.4280, Test: 0.4500\n",
      "Epoch: 075,train_loss:0.3087075, Train: 1.0000, Val: 0.4320, Test: 0.4500\n",
      "Epoch: 076,train_loss:0.4365693, Train: 0.9917, Val: 0.4580, Test: 0.4520\n",
      "Epoch: 077,train_loss:0.3755437, Train: 0.9917, Val: 0.4560, Test: 0.4510\n",
      "Epoch: 078,train_loss:0.3389084, Train: 0.9917, Val: 0.4540, Test: 0.4490\n",
      "Epoch: 079,train_loss:0.2696371, Train: 0.9833, Val: 0.4600, Test: 0.4530\n",
      "Epoch: 080,train_loss:0.3594512, Train: 0.9917, Val: 0.4700, Test: 0.4620\n",
      "Epoch: 081,train_loss:0.2418727, Train: 1.0000, Val: 0.4720, Test: 0.4670\n",
      "Epoch: 082,train_loss:0.2560569, Train: 1.0000, Val: 0.4480, Test: 0.4810\n",
      "Epoch: 083,train_loss:0.2707188, Train: 1.0000, Val: 0.4580, Test: 0.4810\n",
      "Epoch: 084,train_loss:0.2353798, Train: 1.0000, Val: 0.4720, Test: 0.4930\n",
      "Epoch: 085,train_loss:0.2628438, Train: 1.0000, Val: 0.4960, Test: 0.4990\n",
      "Epoch: 086,train_loss:0.3458022, Train: 1.0000, Val: 0.5040, Test: 0.5030\n",
      "Epoch: 087,train_loss:0.2962243, Train: 1.0000, Val: 0.5120, Test: 0.4990\n",
      "Epoch: 088,train_loss:0.2716269, Train: 1.0000, Val: 0.5060, Test: 0.5000\n",
      "Epoch: 089,train_loss:0.3391359, Train: 1.0000, Val: 0.5220, Test: 0.5170\n",
      "Epoch: 090,train_loss:0.3612809, Train: 1.0000, Val: 0.5300, Test: 0.5340\n",
      "Epoch: 091,train_loss:0.2741358, Train: 1.0000, Val: 0.5360, Test: 0.5410\n",
      "Epoch: 092,train_loss:0.2350576, Train: 0.9917, Val: 0.5160, Test: 0.5250\n",
      "Epoch: 093,train_loss:0.3302879, Train: 1.0000, Val: 0.4980, Test: 0.4940\n",
      "Epoch: 094,train_loss:0.2756824, Train: 1.0000, Val: 0.4920, Test: 0.4840\n",
      "Epoch: 095,train_loss:0.2465192, Train: 1.0000, Val: 0.4960, Test: 0.4850\n",
      "Epoch: 096,train_loss:0.2498487, Train: 1.0000, Val: 0.5080, Test: 0.5030\n",
      "Epoch: 097,train_loss:0.2648579, Train: 1.0000, Val: 0.5100, Test: 0.5180\n",
      "Epoch: 098,train_loss:0.2459051, Train: 1.0000, Val: 0.5100, Test: 0.5300\n",
      "Epoch: 099,train_loss:0.2948327, Train: 1.0000, Val: 0.5320, Test: 0.5380\n",
      "Epoch: 100,train_loss:0.2461535, Train: 1.0000, Val: 0.5540, Test: 0.5460\n",
      "Epoch: 101,train_loss:0.2179041, Train: 1.0000, Val: 0.5640, Test: 0.5570\n",
      "Epoch: 102,train_loss:0.1903320, Train: 1.0000, Val: 0.5640, Test: 0.5500\n",
      "Epoch: 103,train_loss:0.2097499, Train: 1.0000, Val: 0.5540, Test: 0.5350\n",
      "Epoch: 104,train_loss:0.1579492, Train: 1.0000, Val: 0.5400, Test: 0.5240\n",
      "Epoch: 105,train_loss:0.2658756, Train: 1.0000, Val: 0.5540, Test: 0.5320\n",
      "Epoch: 106,train_loss:0.3155292, Train: 1.0000, Val: 0.5540, Test: 0.5370\n",
      "Epoch: 107,train_loss:0.1719133, Train: 1.0000, Val: 0.5540, Test: 0.5550\n",
      "Epoch: 108,train_loss:0.2404465, Train: 1.0000, Val: 0.5460, Test: 0.5580\n",
      "Epoch: 109,train_loss:0.2363480, Train: 1.0000, Val: 0.5300, Test: 0.5540\n",
      "Epoch: 110,train_loss:0.1961896, Train: 1.0000, Val: 0.5300, Test: 0.5500\n",
      "Epoch: 111,train_loss:0.2702452, Train: 1.0000, Val: 0.5320, Test: 0.5250\n",
      "Epoch: 112,train_loss:0.2274314, Train: 1.0000, Val: 0.5260, Test: 0.5130\n",
      "Epoch: 113,train_loss:0.2152848, Train: 1.0000, Val: 0.5060, Test: 0.5010\n",
      "Epoch: 114,train_loss:0.1683430, Train: 1.0000, Val: 0.5000, Test: 0.5080\n",
      "Epoch: 115,train_loss:0.2283439, Train: 1.0000, Val: 0.4960, Test: 0.4990\n",
      "Epoch: 116,train_loss:0.2320293, Train: 1.0000, Val: 0.5180, Test: 0.5120\n",
      "Epoch: 117,train_loss:0.2112320, Train: 1.0000, Val: 0.5360, Test: 0.5400\n",
      "Epoch: 118,train_loss:0.2624001, Train: 1.0000, Val: 0.5460, Test: 0.5760\n",
      "Epoch: 119,train_loss:0.3311508, Train: 1.0000, Val: 0.5480, Test: 0.5980\n",
      "Epoch: 120,train_loss:0.2342710, Train: 1.0000, Val: 0.5420, Test: 0.5890\n",
      "Epoch: 121,train_loss:0.2885820, Train: 1.0000, Val: 0.5500, Test: 0.5970\n",
      "Epoch: 122,train_loss:0.2467103, Train: 1.0000, Val: 0.5680, Test: 0.6000\n",
      "Epoch: 123,train_loss:0.2400096, Train: 1.0000, Val: 0.5740, Test: 0.5950\n",
      "Epoch: 124,train_loss:0.2034680, Train: 1.0000, Val: 0.5880, Test: 0.5710\n",
      "Epoch: 125,train_loss:0.3056903, Train: 1.0000, Val: 0.5700, Test: 0.5540\n",
      "Epoch: 126,train_loss:0.2653596, Train: 1.0000, Val: 0.5460, Test: 0.5360\n",
      "Epoch: 127,train_loss:0.3355521, Train: 1.0000, Val: 0.5540, Test: 0.5350\n",
      "Epoch: 128,train_loss:0.2310401, Train: 1.0000, Val: 0.5260, Test: 0.5310\n",
      "Epoch: 129,train_loss:0.2902635, Train: 1.0000, Val: 0.5140, Test: 0.5300\n",
      "Epoch: 130,train_loss:0.2557118, Train: 1.0000, Val: 0.5100, Test: 0.5330\n",
      "Epoch: 131,train_loss:0.3192376, Train: 1.0000, Val: 0.5240, Test: 0.5530\n",
      "Epoch: 132,train_loss:0.2039403, Train: 1.0000, Val: 0.5560, Test: 0.5980\n",
      "Epoch: 133,train_loss:0.3041121, Train: 1.0000, Val: 0.5740, Test: 0.6290\n",
      "Epoch: 134,train_loss:0.2714920, Train: 1.0000, Val: 0.5920, Test: 0.6380\n",
      "Epoch: 135,train_loss:0.2620173, Train: 1.0000, Val: 0.5920, Test: 0.6350\n",
      "Epoch: 136,train_loss:0.2031368, Train: 1.0000, Val: 0.5840, Test: 0.6200\n",
      "Epoch: 137,train_loss:0.1670150, Train: 1.0000, Val: 0.5660, Test: 0.5920\n",
      "Epoch: 138,train_loss:0.2132880, Train: 1.0000, Val: 0.5680, Test: 0.5820\n",
      "Epoch: 139,train_loss:0.1916513, Train: 1.0000, Val: 0.5620, Test: 0.5740\n",
      "Epoch: 140,train_loss:0.2788376, Train: 1.0000, Val: 0.5520, Test: 0.5850\n",
      "Epoch: 141,train_loss:0.2603809, Train: 1.0000, Val: 0.5580, Test: 0.5870\n",
      "Epoch: 142,train_loss:0.2052792, Train: 1.0000, Val: 0.5620, Test: 0.5980\n",
      "Epoch: 143,train_loss:0.2498177, Train: 1.0000, Val: 0.5580, Test: 0.6030\n",
      "Epoch: 144,train_loss:0.2088529, Train: 1.0000, Val: 0.5480, Test: 0.5900\n",
      "Epoch: 145,train_loss:0.2228255, Train: 1.0000, Val: 0.5420, Test: 0.5740\n",
      "Epoch: 146,train_loss:0.2024207, Train: 1.0000, Val: 0.5480, Test: 0.5690\n",
      "Epoch: 147,train_loss:0.2168089, Train: 1.0000, Val: 0.5500, Test: 0.5670\n",
      "Epoch: 148,train_loss:0.2745804, Train: 1.0000, Val: 0.5540, Test: 0.5500\n",
      "Epoch: 149,train_loss:0.2332966, Train: 1.0000, Val: 0.5440, Test: 0.5420\n",
      "Epoch: 150,train_loss:0.2120056, Train: 1.0000, Val: 0.5600, Test: 0.5670\n",
      "Epoch: 151,train_loss:0.1774446, Train: 1.0000, Val: 0.5760, Test: 0.5740\n",
      "Epoch: 152,train_loss:0.2607084, Train: 1.0000, Val: 0.5860, Test: 0.5940\n",
      "Epoch: 153,train_loss:0.2302496, Train: 1.0000, Val: 0.5920, Test: 0.6050\n",
      "Epoch: 154,train_loss:0.1658411, Train: 1.0000, Val: 0.6000, Test: 0.6140\n",
      "Epoch: 155,train_loss:0.1411290, Train: 1.0000, Val: 0.5960, Test: 0.6180\n",
      "Epoch: 156,train_loss:0.2857306, Train: 1.0000, Val: 0.5960, Test: 0.6170\n",
      "Epoch: 157,train_loss:0.2346028, Train: 1.0000, Val: 0.5820, Test: 0.6160\n",
      "Epoch: 158,train_loss:0.2188208, Train: 1.0000, Val: 0.5640, Test: 0.5930\n",
      "Epoch: 159,train_loss:0.2224660, Train: 1.0000, Val: 0.5500, Test: 0.5580\n",
      "Epoch: 160,train_loss:0.1638148, Train: 1.0000, Val: 0.5520, Test: 0.5680\n",
      "Epoch: 161,train_loss:0.2047497, Train: 1.0000, Val: 0.5700, Test: 0.5740\n",
      "Epoch: 162,train_loss:0.1494958, Train: 1.0000, Val: 0.5800, Test: 0.5870\n",
      "Epoch: 163,train_loss:0.2658278, Train: 1.0000, Val: 0.5860, Test: 0.6000\n",
      "Epoch: 164,train_loss:0.2575077, Train: 1.0000, Val: 0.5960, Test: 0.6120\n",
      "Epoch: 165,train_loss:0.2248193, Train: 1.0000, Val: 0.6120, Test: 0.6220\n",
      "Epoch: 166,train_loss:0.3152224, Train: 1.0000, Val: 0.6120, Test: 0.6190\n",
      "Epoch: 167,train_loss:0.2197767, Train: 1.0000, Val: 0.6000, Test: 0.6210\n",
      "Epoch: 168,train_loss:0.1317642, Train: 1.0000, Val: 0.5880, Test: 0.6200\n",
      "Epoch: 169,train_loss:0.1578145, Train: 1.0000, Val: 0.5800, Test: 0.6150\n",
      "Epoch: 170,train_loss:0.1895332, Train: 1.0000, Val: 0.5900, Test: 0.6030\n",
      "Epoch: 171,train_loss:0.2066173, Train: 1.0000, Val: 0.5920, Test: 0.5950\n",
      "Epoch: 172,train_loss:0.1765899, Train: 1.0000, Val: 0.5920, Test: 0.5920\n",
      "Epoch: 173,train_loss:0.2124021, Train: 1.0000, Val: 0.6000, Test: 0.5920\n",
      "Epoch: 174,train_loss:0.1828273, Train: 1.0000, Val: 0.6080, Test: 0.6010\n",
      "Epoch: 175,train_loss:0.3655290, Train: 1.0000, Val: 0.5840, Test: 0.5860\n",
      "Epoch: 176,train_loss:0.2831729, Train: 1.0000, Val: 0.5520, Test: 0.5720\n",
      "Epoch: 177,train_loss:0.2365849, Train: 1.0000, Val: 0.5540, Test: 0.5630\n",
      "Epoch: 178,train_loss:0.2480038, Train: 1.0000, Val: 0.5940, Test: 0.5930\n",
      "Epoch: 179,train_loss:0.2137307, Train: 1.0000, Val: 0.6200, Test: 0.6200\n",
      "Epoch: 180,train_loss:0.2109957, Train: 1.0000, Val: 0.6080, Test: 0.6120\n",
      "Epoch: 181,train_loss:0.2396639, Train: 1.0000, Val: 0.6300, Test: 0.6230\n",
      "Epoch: 182,train_loss:0.2670378, Train: 1.0000, Val: 0.6340, Test: 0.6180\n",
      "Epoch: 183,train_loss:0.1474058, Train: 1.0000, Val: 0.5980, Test: 0.6240\n",
      "Epoch: 184,train_loss:0.3143787, Train: 1.0000, Val: 0.5840, Test: 0.6120\n",
      "Epoch: 185,train_loss:0.2785591, Train: 1.0000, Val: 0.5940, Test: 0.6170\n",
      "Epoch: 186,train_loss:0.1795253, Train: 1.0000, Val: 0.6020, Test: 0.6240\n",
      "Epoch: 187,train_loss:0.2199506, Train: 1.0000, Val: 0.5960, Test: 0.6280\n",
      "Epoch: 188,train_loss:0.1338200, Train: 1.0000, Val: 0.5940, Test: 0.6270\n",
      "Epoch: 189,train_loss:0.2512271, Train: 1.0000, Val: 0.5900, Test: 0.6120\n",
      "Epoch: 190,train_loss:0.2149597, Train: 1.0000, Val: 0.5920, Test: 0.6140\n",
      "Epoch: 191,train_loss:0.1893966, Train: 1.0000, Val: 0.6180, Test: 0.6170\n",
      "Epoch: 192,train_loss:0.2933045, Train: 1.0000, Val: 0.6160, Test: 0.6070\n",
      "Epoch: 193,train_loss:0.2882972, Train: 1.0000, Val: 0.6120, Test: 0.6130\n",
      "Epoch: 194,train_loss:0.2794665, Train: 1.0000, Val: 0.6120, Test: 0.6170\n",
      "Epoch: 195,train_loss:0.2501159, Train: 1.0000, Val: 0.6460, Test: 0.6380\n",
      "Epoch: 196,train_loss:0.2152833, Train: 1.0000, Val: 0.6120, Test: 0.6300\n",
      "Epoch: 197,train_loss:0.1725490, Train: 1.0000, Val: 0.5960, Test: 0.6290\n",
      "Epoch: 198,train_loss:0.2051028, Train: 1.0000, Val: 0.6020, Test: 0.6260\n",
      "Epoch: 199,train_loss:0.1846916, Train: 1.0000, Val: 0.6160, Test: 0.6090\n",
      "Epoch: 200,train_loss:0.2389893, Train: 1.0000, Val: 0.6220, Test: 0.6190\n",
      "Epoch: 201,train_loss:0.2174059, Train: 1.0000, Val: 0.6080, Test: 0.6070\n",
      "Epoch: 202,train_loss:0.1884540, Train: 1.0000, Val: 0.5860, Test: 0.5940\n",
      "Epoch: 203,train_loss:0.2493214, Train: 1.0000, Val: 0.5840, Test: 0.6020\n",
      "Epoch: 204,train_loss:0.2160984, Train: 1.0000, Val: 0.5800, Test: 0.6020\n",
      "Epoch: 205,train_loss:0.3115547, Train: 1.0000, Val: 0.5780, Test: 0.6060\n",
      "Epoch: 206,train_loss:0.2242055, Train: 1.0000, Val: 0.5920, Test: 0.6150\n",
      "Epoch: 207,train_loss:0.2332431, Train: 1.0000, Val: 0.6120, Test: 0.6170\n",
      "Epoch: 208,train_loss:0.2330464, Train: 1.0000, Val: 0.6120, Test: 0.6180\n",
      "Epoch: 209,train_loss:0.1626777, Train: 1.0000, Val: 0.6000, Test: 0.6020\n",
      "Epoch: 210,train_loss:0.2869445, Train: 1.0000, Val: 0.6100, Test: 0.6200\n",
      "Epoch: 211,train_loss:0.2666358, Train: 1.0000, Val: 0.6360, Test: 0.6420\n",
      "Epoch: 212,train_loss:0.1634713, Train: 1.0000, Val: 0.6360, Test: 0.6430\n",
      "Epoch: 213,train_loss:0.2055722, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 214,train_loss:0.2647254, Train: 1.0000, Val: 0.6180, Test: 0.6300\n",
      "Epoch: 215,train_loss:0.1931924, Train: 1.0000, Val: 0.6180, Test: 0.6300\n",
      "Epoch: 216,train_loss:0.2208718, Train: 1.0000, Val: 0.6120, Test: 0.6300\n",
      "Epoch: 217,train_loss:0.2419955, Train: 1.0000, Val: 0.6020, Test: 0.6100\n",
      "Epoch: 218,train_loss:0.2324596, Train: 1.0000, Val: 0.6020, Test: 0.6090\n",
      "Epoch: 219,train_loss:0.2372220, Train: 1.0000, Val: 0.6020, Test: 0.6070\n",
      "Epoch: 220,train_loss:0.2194104, Train: 1.0000, Val: 0.6000, Test: 0.6060\n",
      "Epoch: 221,train_loss:0.1570607, Train: 1.0000, Val: 0.6240, Test: 0.6160\n",
      "Epoch: 222,train_loss:0.1493073, Train: 1.0000, Val: 0.6280, Test: 0.6330\n",
      "Epoch: 223,train_loss:0.2366254, Train: 1.0000, Val: 0.6160, Test: 0.6370\n",
      "Epoch: 224,train_loss:0.1241746, Train: 1.0000, Val: 0.6240, Test: 0.6400\n",
      "Epoch: 225,train_loss:0.2598427, Train: 1.0000, Val: 0.6440, Test: 0.6360\n",
      "Epoch: 226,train_loss:0.2034324, Train: 1.0000, Val: 0.6360, Test: 0.6290\n",
      "Epoch: 227,train_loss:0.2008178, Train: 1.0000, Val: 0.6200, Test: 0.6220\n",
      "Epoch: 228,train_loss:0.2396109, Train: 1.0000, Val: 0.6060, Test: 0.6130\n",
      "Epoch: 229,train_loss:0.2866188, Train: 1.0000, Val: 0.6060, Test: 0.6180\n",
      "Epoch: 230,train_loss:0.2082526, Train: 1.0000, Val: 0.6080, Test: 0.6090\n",
      "Epoch: 231,train_loss:0.2069004, Train: 1.0000, Val: 0.6080, Test: 0.6140\n",
      "Epoch: 232,train_loss:0.2535607, Train: 1.0000, Val: 0.6000, Test: 0.6200\n",
      "Epoch: 233,train_loss:0.2034485, Train: 1.0000, Val: 0.6080, Test: 0.6210\n",
      "Epoch: 234,train_loss:0.1958260, Train: 1.0000, Val: 0.6060, Test: 0.6250\n",
      "Epoch: 235,train_loss:0.2527888, Train: 1.0000, Val: 0.5980, Test: 0.6090\n",
      "Epoch: 236,train_loss:0.1456285, Train: 1.0000, Val: 0.6020, Test: 0.6030\n",
      "Epoch: 237,train_loss:0.2275845, Train: 1.0000, Val: 0.6160, Test: 0.6110\n",
      "Epoch: 238,train_loss:0.2495384, Train: 1.0000, Val: 0.6260, Test: 0.6240\n",
      "Epoch: 239,train_loss:0.3066869, Train: 1.0000, Val: 0.6300, Test: 0.6210\n",
      "Epoch: 240,train_loss:0.2237329, Train: 1.0000, Val: 0.6300, Test: 0.6180\n",
      "Epoch: 241,train_loss:0.2318958, Train: 1.0000, Val: 0.6340, Test: 0.6170\n",
      "Epoch: 242,train_loss:0.2159088, Train: 1.0000, Val: 0.6160, Test: 0.5970\n",
      "Epoch: 243,train_loss:0.3412752, Train: 1.0000, Val: 0.6180, Test: 0.6000\n",
      "Epoch: 244,train_loss:0.2299345, Train: 1.0000, Val: 0.6160, Test: 0.6000\n",
      "Epoch: 245,train_loss:0.1373201, Train: 1.0000, Val: 0.6060, Test: 0.6040\n",
      "Epoch: 246,train_loss:0.1940312, Train: 1.0000, Val: 0.5960, Test: 0.5980\n",
      "Epoch: 247,train_loss:0.1725469, Train: 1.0000, Val: 0.5960, Test: 0.6050\n",
      "Epoch: 248,train_loss:0.1744813, Train: 1.0000, Val: 0.6060, Test: 0.6150\n",
      "Epoch: 249,train_loss:0.1433833, Train: 1.0000, Val: 0.6280, Test: 0.6270\n",
      "Epoch: 250,train_loss:0.1354713, Train: 1.0000, Val: 0.6380, Test: 0.6220\n",
      "Epoch: 251,train_loss:0.1931415, Train: 1.0000, Val: 0.6340, Test: 0.6270\n",
      "Epoch: 252,train_loss:0.2131862, Train: 1.0000, Val: 0.6360, Test: 0.6290\n",
      "Epoch: 253,train_loss:0.2474120, Train: 1.0000, Val: 0.6420, Test: 0.6220\n",
      "Epoch: 254,train_loss:0.1473834, Train: 1.0000, Val: 0.6380, Test: 0.6240\n",
      "Epoch: 255,train_loss:0.2798789, Train: 1.0000, Val: 0.6160, Test: 0.6190\n",
      "Epoch: 256,train_loss:0.2015528, Train: 1.0000, Val: 0.6100, Test: 0.6080\n",
      "Epoch: 257,train_loss:0.2360172, Train: 1.0000, Val: 0.6020, Test: 0.6180\n",
      "Epoch: 258,train_loss:0.2688351, Train: 1.0000, Val: 0.6020, Test: 0.6170\n",
      "Epoch: 259,train_loss:0.2156038, Train: 1.0000, Val: 0.6280, Test: 0.6170\n",
      "Epoch: 260,train_loss:0.2597595, Train: 1.0000, Val: 0.6300, Test: 0.6180\n",
      "Epoch: 261,train_loss:0.1264666, Train: 1.0000, Val: 0.6360, Test: 0.6200\n",
      "Epoch: 262,train_loss:0.1854220, Train: 1.0000, Val: 0.6320, Test: 0.6240\n",
      "Epoch: 263,train_loss:0.1924414, Train: 1.0000, Val: 0.6300, Test: 0.6270\n",
      "Epoch: 264,train_loss:0.1263159, Train: 1.0000, Val: 0.6240, Test: 0.6290\n",
      "Epoch: 265,train_loss:0.2135376, Train: 1.0000, Val: 0.6280, Test: 0.6270\n",
      "Epoch: 266,train_loss:0.2554112, Train: 1.0000, Val: 0.6260, Test: 0.6330\n",
      "Epoch: 267,train_loss:0.2908717, Train: 1.0000, Val: 0.6240, Test: 0.6270\n",
      "Epoch: 268,train_loss:0.2049449, Train: 1.0000, Val: 0.6260, Test: 0.6280\n",
      "Epoch: 269,train_loss:0.2831407, Train: 1.0000, Val: 0.6120, Test: 0.6190\n",
      "Epoch: 270,train_loss:0.3274670, Train: 1.0000, Val: 0.6060, Test: 0.6090\n",
      "Epoch: 271,train_loss:0.1988091, Train: 1.0000, Val: 0.6180, Test: 0.6160\n",
      "Epoch: 272,train_loss:0.3051030, Train: 1.0000, Val: 0.6360, Test: 0.6290\n",
      "Epoch: 273,train_loss:0.1956651, Train: 1.0000, Val: 0.6420, Test: 0.6430\n",
      "Epoch: 274,train_loss:0.1841382, Train: 1.0000, Val: 0.6280, Test: 0.6410\n",
      "Epoch: 275,train_loss:0.2344045, Train: 1.0000, Val: 0.6240, Test: 0.6350\n",
      "Epoch: 276,train_loss:0.1979128, Train: 1.0000, Val: 0.6140, Test: 0.6370\n",
      "Epoch: 277,train_loss:0.2732416, Train: 1.0000, Val: 0.6140, Test: 0.6360\n",
      "Epoch: 278,train_loss:0.2983435, Train: 1.0000, Val: 0.6140, Test: 0.6340\n",
      "Epoch: 279,train_loss:0.0981855, Train: 1.0000, Val: 0.6140, Test: 0.6330\n",
      "Epoch: 280,train_loss:0.2315066, Train: 1.0000, Val: 0.6080, Test: 0.6260\n",
      "Epoch: 281,train_loss:0.2061817, Train: 1.0000, Val: 0.6100, Test: 0.6240\n",
      "Epoch: 282,train_loss:0.2289205, Train: 1.0000, Val: 0.6140, Test: 0.6210\n",
      "Epoch: 283,train_loss:0.1795760, Train: 1.0000, Val: 0.6160, Test: 0.6240\n",
      "Epoch: 284,train_loss:0.2227619, Train: 1.0000, Val: 0.6300, Test: 0.6260\n",
      "Epoch: 285,train_loss:0.2503903, Train: 1.0000, Val: 0.6260, Test: 0.6270\n",
      "Epoch: 286,train_loss:0.2323718, Train: 1.0000, Val: 0.6240, Test: 0.6270\n",
      "Epoch: 287,train_loss:0.2397239, Train: 1.0000, Val: 0.6280, Test: 0.6300\n",
      "Epoch: 288,train_loss:0.1748628, Train: 1.0000, Val: 0.6300, Test: 0.6340\n",
      "Epoch: 289,train_loss:0.1846944, Train: 1.0000, Val: 0.6260, Test: 0.6350\n",
      "Epoch: 290,train_loss:0.2746504, Train: 1.0000, Val: 0.6300, Test: 0.6370\n",
      "Epoch: 291,train_loss:0.1715931, Train: 1.0000, Val: 0.6200, Test: 0.6310\n",
      "Epoch: 292,train_loss:0.2728925, Train: 1.0000, Val: 0.6160, Test: 0.6280\n",
      "Epoch: 293,train_loss:0.1432121, Train: 1.0000, Val: 0.6020, Test: 0.6250\n",
      "Epoch: 294,train_loss:0.2716119, Train: 1.0000, Val: 0.6080, Test: 0.6270\n",
      "Epoch: 295,train_loss:0.2805228, Train: 1.0000, Val: 0.6160, Test: 0.6320\n",
      "Epoch: 296,train_loss:0.2306876, Train: 1.0000, Val: 0.6220, Test: 0.6390\n",
      "Epoch: 297,train_loss:0.2563351, Train: 1.0000, Val: 0.6260, Test: 0.6370\n",
      "Epoch: 298,train_loss:0.2213272, Train: 1.0000, Val: 0.6260, Test: 0.6480\n",
      "Epoch: 299,train_loss:0.1544824, Train: 1.0000, Val: 0.6280, Test: 0.6420\n",
      "Epoch: 300,train_loss:0.2752160, Train: 1.0000, Val: 0.6080, Test: 0.6350\n",
      "Epoch: 301,train_loss:0.1805201, Train: 1.0000, Val: 0.5820, Test: 0.6020\n",
      "Epoch: 302,train_loss:0.2642187, Train: 1.0000, Val: 0.5680, Test: 0.5800\n",
      "Epoch: 303,train_loss:0.2912631, Train: 1.0000, Val: 0.5500, Test: 0.5760\n",
      "Epoch: 304,train_loss:0.2949858, Train: 1.0000, Val: 0.5840, Test: 0.5960\n",
      "Epoch: 305,train_loss:0.2521069, Train: 1.0000, Val: 0.5960, Test: 0.6130\n",
      "Epoch: 306,train_loss:0.2574101, Train: 1.0000, Val: 0.6180, Test: 0.6200\n",
      "Epoch: 307,train_loss:0.1889615, Train: 1.0000, Val: 0.6160, Test: 0.6100\n",
      "Epoch: 308,train_loss:0.1652840, Train: 1.0000, Val: 0.6340, Test: 0.6450\n",
      "Epoch: 309,train_loss:0.1923097, Train: 1.0000, Val: 0.6380, Test: 0.6360\n",
      "Epoch: 310,train_loss:0.2297795, Train: 1.0000, Val: 0.6160, Test: 0.6320\n",
      "Epoch: 311,train_loss:0.1823022, Train: 1.0000, Val: 0.6000, Test: 0.5990\n",
      "Epoch: 312,train_loss:0.2331432, Train: 1.0000, Val: 0.6080, Test: 0.5980\n",
      "Epoch: 313,train_loss:0.1735491, Train: 1.0000, Val: 0.6120, Test: 0.6120\n",
      "Epoch: 314,train_loss:0.2388561, Train: 1.0000, Val: 0.6060, Test: 0.6020\n",
      "Epoch: 315,train_loss:0.2499138, Train: 1.0000, Val: 0.6140, Test: 0.6110\n",
      "Epoch: 316,train_loss:0.2506653, Train: 1.0000, Val: 0.6180, Test: 0.6190\n",
      "Epoch: 317,train_loss:0.2771961, Train: 1.0000, Val: 0.6160, Test: 0.6300\n",
      "Epoch: 318,train_loss:0.1941175, Train: 1.0000, Val: 0.6000, Test: 0.6330\n",
      "Epoch: 319,train_loss:0.2430250, Train: 1.0000, Val: 0.6060, Test: 0.6390\n",
      "Epoch: 320,train_loss:0.2398702, Train: 1.0000, Val: 0.6260, Test: 0.6370\n",
      "Epoch: 321,train_loss:0.2214899, Train: 1.0000, Val: 0.6240, Test: 0.6340\n",
      "Epoch: 322,train_loss:0.3112814, Train: 1.0000, Val: 0.6300, Test: 0.6280\n",
      "Epoch: 323,train_loss:0.2043206, Train: 1.0000, Val: 0.6160, Test: 0.6230\n",
      "Epoch: 324,train_loss:0.1827834, Train: 1.0000, Val: 0.6140, Test: 0.6060\n",
      "Epoch: 325,train_loss:0.1823908, Train: 1.0000, Val: 0.6240, Test: 0.6130\n",
      "Epoch: 326,train_loss:0.2918942, Train: 1.0000, Val: 0.6060, Test: 0.6150\n",
      "Epoch: 327,train_loss:0.1300777, Train: 1.0000, Val: 0.6040, Test: 0.6080\n",
      "Epoch: 328,train_loss:0.1830491, Train: 1.0000, Val: 0.5900, Test: 0.5930\n",
      "Epoch: 329,train_loss:0.4157948, Train: 1.0000, Val: 0.6040, Test: 0.6230\n",
      "Epoch: 330,train_loss:0.1988674, Train: 1.0000, Val: 0.6260, Test: 0.6360\n",
      "Epoch: 331,train_loss:0.3127078, Train: 1.0000, Val: 0.6140, Test: 0.6220\n",
      "Epoch: 332,train_loss:0.2420708, Train: 1.0000, Val: 0.5940, Test: 0.5950\n",
      "Epoch: 333,train_loss:0.2371890, Train: 1.0000, Val: 0.5860, Test: 0.5940\n",
      "Epoch: 334,train_loss:0.2192958, Train: 1.0000, Val: 0.5920, Test: 0.6040\n",
      "Epoch: 335,train_loss:0.2757303, Train: 1.0000, Val: 0.6160, Test: 0.6230\n",
      "Epoch: 336,train_loss:0.1346653, Train: 1.0000, Val: 0.6240, Test: 0.6390\n",
      "Epoch: 337,train_loss:0.1697831, Train: 1.0000, Val: 0.6180, Test: 0.6380\n",
      "Epoch: 338,train_loss:0.1960944, Train: 1.0000, Val: 0.6120, Test: 0.6350\n",
      "Epoch: 339,train_loss:0.1729407, Train: 1.0000, Val: 0.6180, Test: 0.6430\n",
      "Epoch: 340,train_loss:0.2843502, Train: 1.0000, Val: 0.6140, Test: 0.6340\n",
      "Epoch: 341,train_loss:0.2430371, Train: 1.0000, Val: 0.6120, Test: 0.6290\n",
      "Epoch: 342,train_loss:0.1643142, Train: 1.0000, Val: 0.6100, Test: 0.6260\n",
      "Epoch: 343,train_loss:0.1963759, Train: 1.0000, Val: 0.6060, Test: 0.6260\n",
      "Epoch: 344,train_loss:0.2142158, Train: 1.0000, Val: 0.6100, Test: 0.6310\n",
      "Epoch: 345,train_loss:0.1272113, Train: 1.0000, Val: 0.6100, Test: 0.6370\n",
      "Epoch: 346,train_loss:0.1933397, Train: 1.0000, Val: 0.5980, Test: 0.6290\n",
      "Epoch: 347,train_loss:0.1462039, Train: 1.0000, Val: 0.5920, Test: 0.6240\n",
      "Epoch: 348,train_loss:0.2504128, Train: 1.0000, Val: 0.6000, Test: 0.6250\n",
      "Epoch: 349,train_loss:0.2794496, Train: 1.0000, Val: 0.6100, Test: 0.6280\n",
      "Epoch: 350,train_loss:0.2351206, Train: 1.0000, Val: 0.6160, Test: 0.6030\n",
      "Epoch: 351,train_loss:0.1472391, Train: 1.0000, Val: 0.6100, Test: 0.5820\n",
      "Epoch: 352,train_loss:0.2518788, Train: 1.0000, Val: 0.6160, Test: 0.5890\n",
      "Epoch: 353,train_loss:0.1407760, Train: 1.0000, Val: 0.6160, Test: 0.6020\n",
      "Epoch: 354,train_loss:0.1325001, Train: 1.0000, Val: 0.6200, Test: 0.6280\n",
      "Epoch: 355,train_loss:0.2408654, Train: 1.0000, Val: 0.6240, Test: 0.6310\n",
      "Epoch: 356,train_loss:0.2300251, Train: 1.0000, Val: 0.6060, Test: 0.6280\n",
      "Epoch: 357,train_loss:0.2708110, Train: 1.0000, Val: 0.5860, Test: 0.6010\n",
      "Epoch: 358,train_loss:0.2129281, Train: 1.0000, Val: 0.5900, Test: 0.6110\n",
      "Epoch: 359,train_loss:0.1426860, Train: 1.0000, Val: 0.5940, Test: 0.6140\n",
      "Epoch: 360,train_loss:0.2181796, Train: 1.0000, Val: 0.6240, Test: 0.6110\n",
      "Epoch: 361,train_loss:0.2279816, Train: 1.0000, Val: 0.6260, Test: 0.6110\n",
      "Epoch: 362,train_loss:0.1573482, Train: 1.0000, Val: 0.6140, Test: 0.5920\n",
      "Epoch: 363,train_loss:0.1677746, Train: 1.0000, Val: 0.5980, Test: 0.5690\n",
      "Epoch: 364,train_loss:0.1736222, Train: 1.0000, Val: 0.5880, Test: 0.5710\n",
      "Epoch: 365,train_loss:0.2384073, Train: 1.0000, Val: 0.5980, Test: 0.5840\n",
      "Epoch: 366,train_loss:0.2142694, Train: 1.0000, Val: 0.6100, Test: 0.6060\n",
      "Epoch: 367,train_loss:0.2498443, Train: 1.0000, Val: 0.6200, Test: 0.6370\n",
      "Epoch: 368,train_loss:0.2261525, Train: 1.0000, Val: 0.6080, Test: 0.6330\n",
      "Epoch: 369,train_loss:0.2677248, Train: 1.0000, Val: 0.6040, Test: 0.6280\n",
      "Epoch: 370,train_loss:0.2053945, Train: 1.0000, Val: 0.6040, Test: 0.6220\n",
      "Epoch: 371,train_loss:0.1790213, Train: 1.0000, Val: 0.6060, Test: 0.6270\n",
      "Epoch: 372,train_loss:0.2004917, Train: 1.0000, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 373,train_loss:0.2680746, Train: 1.0000, Val: 0.6300, Test: 0.6330\n",
      "Epoch: 374,train_loss:0.2487769, Train: 1.0000, Val: 0.6200, Test: 0.6190\n",
      "Epoch: 375,train_loss:0.3156240, Train: 1.0000, Val: 0.6220, Test: 0.6250\n",
      "Epoch: 376,train_loss:0.1770124, Train: 1.0000, Val: 0.6100, Test: 0.6280\n",
      "Epoch: 377,train_loss:0.1715364, Train: 1.0000, Val: 0.6080, Test: 0.6250\n",
      "Epoch: 378,train_loss:0.2237935, Train: 1.0000, Val: 0.6080, Test: 0.6250\n",
      "Epoch: 379,train_loss:0.1672359, Train: 1.0000, Val: 0.6200, Test: 0.6290\n",
      "Epoch: 380,train_loss:0.1881169, Train: 1.0000, Val: 0.6060, Test: 0.6340\n",
      "Epoch: 381,train_loss:0.1327730, Train: 1.0000, Val: 0.6140, Test: 0.6240\n",
      "Epoch: 382,train_loss:0.2717494, Train: 1.0000, Val: 0.6060, Test: 0.6070\n",
      "Epoch: 383,train_loss:0.1778731, Train: 1.0000, Val: 0.6000, Test: 0.6230\n",
      "Epoch: 384,train_loss:0.2680689, Train: 1.0000, Val: 0.6040, Test: 0.6260\n",
      "Epoch: 385,train_loss:0.2126601, Train: 1.0000, Val: 0.5980, Test: 0.6380\n",
      "Epoch: 386,train_loss:0.2176005, Train: 1.0000, Val: 0.5880, Test: 0.6400\n",
      "Epoch: 387,train_loss:0.2127522, Train: 1.0000, Val: 0.6060, Test: 0.6490\n",
      "Epoch: 388,train_loss:0.2464774, Train: 1.0000, Val: 0.6160, Test: 0.6450\n",
      "Epoch: 389,train_loss:0.3384428, Train: 1.0000, Val: 0.6140, Test: 0.6220\n",
      "Epoch: 390,train_loss:0.2075079, Train: 1.0000, Val: 0.5980, Test: 0.5910\n",
      "Epoch: 391,train_loss:0.1861614, Train: 1.0000, Val: 0.6000, Test: 0.6010\n",
      "Epoch: 392,train_loss:0.2209058, Train: 1.0000, Val: 0.6140, Test: 0.6240\n",
      "Epoch: 393,train_loss:0.1846300, Train: 1.0000, Val: 0.6280, Test: 0.6320\n",
      "Epoch: 394,train_loss:0.2404896, Train: 1.0000, Val: 0.6260, Test: 0.6390\n",
      "Epoch: 395,train_loss:0.2038194, Train: 1.0000, Val: 0.5920, Test: 0.6270\n",
      "Epoch: 396,train_loss:0.2606187, Train: 1.0000, Val: 0.6000, Test: 0.6320\n",
      "Epoch: 397,train_loss:0.1651383, Train: 1.0000, Val: 0.6140, Test: 0.6380\n",
      "Epoch: 398,train_loss:0.2380062, Train: 1.0000, Val: 0.6200, Test: 0.6360\n",
      "Epoch: 399,train_loss:0.2268061, Train: 1.0000, Val: 0.6400, Test: 0.6240\n",
      "Epoch: 400,train_loss:0.1858572, Train: 1.0000, Val: 0.6120, Test: 0.5960\n",
      "Epoch: 401,train_loss:0.2462036, Train: 1.0000, Val: 0.5760, Test: 0.5660\n",
      "Epoch: 402,train_loss:0.2459321, Train: 1.0000, Val: 0.5740, Test: 0.5470\n",
      "Epoch: 403,train_loss:0.2367407, Train: 1.0000, Val: 0.5600, Test: 0.5440\n",
      "Epoch: 404,train_loss:0.2089888, Train: 1.0000, Val: 0.5740, Test: 0.5620\n",
      "Epoch: 405,train_loss:0.2433598, Train: 1.0000, Val: 0.5860, Test: 0.5900\n",
      "Epoch: 406,train_loss:0.2076005, Train: 1.0000, Val: 0.5860, Test: 0.6220\n",
      "Epoch: 407,train_loss:0.1773192, Train: 1.0000, Val: 0.5860, Test: 0.6320\n",
      "Epoch: 408,train_loss:0.2383618, Train: 1.0000, Val: 0.6080, Test: 0.6460\n",
      "Epoch: 409,train_loss:0.1195490, Train: 1.0000, Val: 0.6180, Test: 0.6580\n",
      "Epoch: 410,train_loss:0.1934328, Train: 1.0000, Val: 0.6500, Test: 0.6650\n",
      "Epoch: 411,train_loss:0.2468715, Train: 1.0000, Val: 0.6240, Test: 0.6200\n",
      "Epoch: 412,train_loss:0.1872132, Train: 1.0000, Val: 0.5860, Test: 0.5840\n",
      "Epoch: 413,train_loss:0.1424500, Train: 1.0000, Val: 0.5800, Test: 0.5790\n",
      "Epoch: 414,train_loss:0.2199274, Train: 1.0000, Val: 0.6020, Test: 0.6040\n",
      "Epoch: 415,train_loss:0.2172906, Train: 1.0000, Val: 0.6500, Test: 0.6350\n",
      "Epoch: 416,train_loss:0.2068601, Train: 1.0000, Val: 0.6260, Test: 0.6480\n",
      "Epoch: 417,train_loss:0.1573119, Train: 1.0000, Val: 0.6080, Test: 0.6420\n",
      "Epoch: 418,train_loss:0.2188349, Train: 1.0000, Val: 0.6120, Test: 0.6400\n",
      "Epoch: 419,train_loss:0.1337720, Train: 1.0000, Val: 0.6220, Test: 0.6360\n",
      "Epoch: 420,train_loss:0.2760022, Train: 1.0000, Val: 0.6360, Test: 0.6350\n",
      "Epoch: 421,train_loss:0.2510520, Train: 1.0000, Val: 0.5980, Test: 0.5890\n",
      "Epoch: 422,train_loss:0.2210900, Train: 1.0000, Val: 0.5620, Test: 0.5530\n",
      "Epoch: 423,train_loss:0.2748858, Train: 1.0000, Val: 0.5320, Test: 0.5370\n",
      "Epoch: 424,train_loss:0.2455041, Train: 1.0000, Val: 0.5740, Test: 0.5880\n",
      "Epoch: 425,train_loss:0.0797895, Train: 1.0000, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 426,train_loss:0.2605307, Train: 1.0000, Val: 0.6200, Test: 0.6350\n",
      "Epoch: 427,train_loss:0.2298748, Train: 1.0000, Val: 0.5940, Test: 0.6140\n",
      "Epoch: 428,train_loss:0.2575613, Train: 1.0000, Val: 0.6200, Test: 0.6370\n",
      "Epoch: 429,train_loss:0.3330758, Train: 1.0000, Val: 0.6220, Test: 0.6410\n",
      "Epoch: 430,train_loss:0.1387509, Train: 1.0000, Val: 0.6340, Test: 0.6370\n",
      "Epoch: 431,train_loss:0.2409735, Train: 1.0000, Val: 0.6080, Test: 0.6240\n",
      "Epoch: 432,train_loss:0.2551707, Train: 1.0000, Val: 0.5900, Test: 0.5990\n",
      "Epoch: 433,train_loss:0.1377026, Train: 1.0000, Val: 0.5700, Test: 0.5880\n",
      "Epoch: 434,train_loss:0.2127653, Train: 1.0000, Val: 0.5700, Test: 0.5890\n",
      "Epoch: 435,train_loss:0.1928335, Train: 1.0000, Val: 0.5700, Test: 0.5890\n",
      "Epoch: 436,train_loss:0.1939339, Train: 1.0000, Val: 0.5880, Test: 0.5990\n",
      "Epoch: 437,train_loss:0.1284712, Train: 1.0000, Val: 0.6100, Test: 0.6090\n",
      "Epoch: 438,train_loss:0.3213625, Train: 1.0000, Val: 0.6100, Test: 0.6060\n",
      "Epoch: 439,train_loss:0.1873484, Train: 1.0000, Val: 0.6320, Test: 0.6280\n",
      "Epoch: 440,train_loss:0.2360404, Train: 1.0000, Val: 0.6160, Test: 0.6420\n",
      "Epoch: 441,train_loss:0.2050147, Train: 1.0000, Val: 0.6040, Test: 0.6280\n",
      "Epoch: 442,train_loss:0.2599336, Train: 1.0000, Val: 0.5800, Test: 0.6180\n",
      "Epoch: 443,train_loss:0.2512789, Train: 1.0000, Val: 0.5760, Test: 0.6110\n",
      "Epoch: 444,train_loss:0.2211240, Train: 1.0000, Val: 0.6180, Test: 0.6190\n",
      "Epoch: 445,train_loss:0.3161947, Train: 1.0000, Val: 0.6100, Test: 0.6000\n",
      "Epoch: 446,train_loss:0.1912386, Train: 1.0000, Val: 0.5900, Test: 0.5840\n",
      "Epoch: 447,train_loss:0.2505662, Train: 1.0000, Val: 0.6140, Test: 0.6080\n",
      "Epoch: 448,train_loss:0.1678723, Train: 1.0000, Val: 0.6340, Test: 0.6140\n",
      "Epoch: 449,train_loss:0.1786026, Train: 1.0000, Val: 0.6080, Test: 0.6250\n",
      "Epoch: 450,train_loss:0.2076975, Train: 1.0000, Val: 0.6000, Test: 0.6160\n",
      "Epoch: 451,train_loss:0.2187742, Train: 1.0000, Val: 0.5300, Test: 0.5750\n",
      "Epoch: 452,train_loss:0.1801742, Train: 1.0000, Val: 0.5040, Test: 0.5630\n",
      "Epoch: 453,train_loss:0.1621393, Train: 1.0000, Val: 0.5220, Test: 0.5750\n",
      "Epoch: 454,train_loss:0.2412827, Train: 1.0000, Val: 0.5800, Test: 0.6370\n",
      "Epoch: 455,train_loss:0.2990942, Train: 1.0000, Val: 0.6120, Test: 0.6190\n",
      "Epoch: 456,train_loss:0.1585700, Train: 1.0000, Val: 0.5820, Test: 0.5790\n",
      "Epoch: 457,train_loss:0.1244893, Train: 1.0000, Val: 0.5700, Test: 0.5490\n",
      "Epoch: 458,train_loss:0.2068152, Train: 1.0000, Val: 0.5600, Test: 0.5630\n",
      "Epoch: 459,train_loss:0.3208522, Train: 1.0000, Val: 0.5880, Test: 0.5850\n",
      "Epoch: 460,train_loss:0.2442605, Train: 1.0000, Val: 0.5500, Test: 0.5500\n",
      "Epoch: 461,train_loss:0.2019862, Train: 1.0000, Val: 0.5320, Test: 0.5320\n",
      "Epoch: 462,train_loss:0.2459998, Train: 1.0000, Val: 0.5340, Test: 0.5290\n",
      "Epoch: 463,train_loss:0.2376439, Train: 1.0000, Val: 0.5420, Test: 0.5530\n",
      "Epoch: 464,train_loss:0.2905425, Train: 1.0000, Val: 0.5680, Test: 0.6130\n",
      "Epoch: 465,train_loss:0.1649169, Train: 1.0000, Val: 0.5880, Test: 0.6220\n",
      "Epoch: 466,train_loss:0.3147808, Train: 1.0000, Val: 0.6060, Test: 0.6210\n",
      "Epoch: 467,train_loss:0.1906858, Train: 1.0000, Val: 0.5780, Test: 0.5880\n",
      "Epoch: 468,train_loss:0.2039058, Train: 1.0000, Val: 0.5740, Test: 0.5800\n",
      "Epoch: 469,train_loss:0.2259281, Train: 1.0000, Val: 0.5720, Test: 0.5870\n",
      "Epoch: 470,train_loss:0.1961018, Train: 1.0000, Val: 0.5700, Test: 0.5830\n",
      "Epoch: 471,train_loss:0.1323851, Train: 1.0000, Val: 0.5500, Test: 0.5700\n",
      "Epoch: 472,train_loss:0.2437264, Train: 1.0000, Val: 0.5380, Test: 0.5820\n",
      "Epoch: 473,train_loss:0.1600930, Train: 1.0000, Val: 0.5300, Test: 0.5870\n",
      "Epoch: 474,train_loss:0.2792555, Train: 1.0000, Val: 0.5640, Test: 0.6130\n",
      "Epoch: 475,train_loss:0.1782196, Train: 1.0000, Val: 0.5820, Test: 0.6320\n",
      "Epoch: 476,train_loss:0.2644486, Train: 1.0000, Val: 0.5840, Test: 0.6200\n",
      "Epoch: 477,train_loss:0.2426722, Train: 1.0000, Val: 0.5780, Test: 0.5780\n",
      "Epoch: 478,train_loss:0.2384286, Train: 1.0000, Val: 0.5420, Test: 0.5360\n",
      "Epoch: 479,train_loss:0.2158981, Train: 1.0000, Val: 0.5360, Test: 0.5270\n",
      "Epoch: 480,train_loss:0.1463165, Train: 1.0000, Val: 0.5400, Test: 0.5340\n",
      "Epoch: 481,train_loss:0.3100711, Train: 1.0000, Val: 0.5520, Test: 0.5520\n",
      "Epoch: 482,train_loss:0.0799729, Train: 1.0000, Val: 0.5680, Test: 0.5810\n",
      "Epoch: 483,train_loss:0.2812516, Train: 1.0000, Val: 0.5740, Test: 0.5960\n",
      "Epoch: 484,train_loss:0.2091368, Train: 1.0000, Val: 0.5900, Test: 0.6130\n",
      "Epoch: 485,train_loss:0.2329380, Train: 1.0000, Val: 0.5860, Test: 0.6180\n",
      "Epoch: 486,train_loss:0.2579354, Train: 1.0000, Val: 0.5760, Test: 0.6140\n",
      "Epoch: 487,train_loss:0.2274778, Train: 1.0000, Val: 0.5740, Test: 0.6190\n",
      "Epoch: 488,train_loss:0.2379320, Train: 1.0000, Val: 0.5860, Test: 0.6230\n",
      "Epoch: 489,train_loss:0.2544526, Train: 1.0000, Val: 0.5920, Test: 0.6290\n",
      "Epoch: 490,train_loss:0.1645302, Train: 1.0000, Val: 0.5740, Test: 0.6020\n",
      "Epoch: 491,train_loss:0.1798778, Train: 1.0000, Val: 0.5620, Test: 0.5730\n",
      "Epoch: 492,train_loss:0.1890390, Train: 1.0000, Val: 0.5580, Test: 0.5580\n",
      "Epoch: 493,train_loss:0.1210929, Train: 1.0000, Val: 0.5660, Test: 0.5640\n",
      "Epoch: 494,train_loss:0.2886413, Train: 1.0000, Val: 0.5700, Test: 0.5990\n",
      "Epoch: 495,train_loss:0.3367858, Train: 1.0000, Val: 0.5760, Test: 0.6140\n",
      "Epoch: 496,train_loss:0.1949126, Train: 1.0000, Val: 0.5920, Test: 0.6370\n",
      "Epoch: 497,train_loss:0.1555644, Train: 1.0000, Val: 0.5920, Test: 0.6320\n",
      "Epoch: 498,train_loss:0.2034312, Train: 1.0000, Val: 0.5920, Test: 0.6330\n",
      "Epoch: 499,train_loss:0.2424406, Train: 1.0000, Val: 0.5900, Test: 0.6320\n",
      "Epoch: 500,train_loss:0.2433101, Train: 1.0000, Val: 0.5960, Test: 0.6320\n",
      "Epoch: 501,train_loss:0.2491949, Train: 1.0000, Val: 0.5960, Test: 0.6250\n",
      "Epoch: 502,train_loss:0.2413538, Train: 1.0000, Val: 0.5780, Test: 0.6150\n",
      "Epoch: 503,train_loss:0.1927037, Train: 1.0000, Val: 0.5760, Test: 0.5940\n",
      "Epoch: 504,train_loss:0.1899962, Train: 1.0000, Val: 0.5640, Test: 0.5620\n",
      "Epoch: 505,train_loss:0.1877283, Train: 1.0000, Val: 0.5700, Test: 0.5650\n",
      "Epoch: 506,train_loss:0.3153160, Train: 1.0000, Val: 0.5920, Test: 0.6190\n",
      "Epoch: 507,train_loss:0.2182144, Train: 1.0000, Val: 0.5920, Test: 0.6250\n",
      "Epoch: 508,train_loss:0.2397992, Train: 1.0000, Val: 0.5720, Test: 0.6120\n",
      "Epoch: 509,train_loss:0.1180178, Train: 1.0000, Val: 0.5500, Test: 0.5750\n",
      "Epoch: 510,train_loss:0.3023018, Train: 1.0000, Val: 0.5580, Test: 0.5820\n",
      "Epoch: 511,train_loss:0.2359850, Train: 1.0000, Val: 0.5880, Test: 0.6160\n",
      "Epoch: 512,train_loss:0.2343605, Train: 1.0000, Val: 0.5940, Test: 0.6310\n",
      "Epoch: 513,train_loss:0.2111573, Train: 1.0000, Val: 0.6160, Test: 0.6190\n",
      "Epoch: 514,train_loss:0.2477643, Train: 1.0000, Val: 0.6020, Test: 0.5790\n",
      "Epoch: 515,train_loss:0.2434976, Train: 1.0000, Val: 0.5820, Test: 0.5640\n",
      "Epoch: 516,train_loss:0.2654514, Train: 1.0000, Val: 0.6020, Test: 0.5790\n",
      "Epoch: 517,train_loss:0.1964833, Train: 1.0000, Val: 0.5860, Test: 0.5930\n",
      "Epoch: 518,train_loss:0.2405289, Train: 1.0000, Val: 0.5960, Test: 0.6120\n",
      "Epoch: 519,train_loss:0.1687749, Train: 1.0000, Val: 0.6040, Test: 0.6100\n",
      "Epoch: 520,train_loss:0.1920880, Train: 1.0000, Val: 0.5900, Test: 0.6120\n",
      "Epoch: 521,train_loss:0.2032359, Train: 1.0000, Val: 0.5900, Test: 0.6040\n",
      "Epoch: 522,train_loss:0.1491331, Train: 1.0000, Val: 0.5880, Test: 0.6260\n",
      "Epoch: 523,train_loss:0.1905321, Train: 1.0000, Val: 0.5940, Test: 0.6170\n",
      "Epoch: 524,train_loss:0.1855848, Train: 1.0000, Val: 0.5960, Test: 0.5930\n",
      "Epoch: 525,train_loss:0.1102367, Train: 1.0000, Val: 0.6040, Test: 0.5920\n",
      "Epoch: 526,train_loss:0.2575509, Train: 1.0000, Val: 0.6160, Test: 0.6070\n",
      "Epoch: 527,train_loss:0.2132068, Train: 1.0000, Val: 0.5860, Test: 0.6070\n",
      "Epoch: 528,train_loss:0.1869111, Train: 1.0000, Val: 0.5740, Test: 0.5660\n",
      "Epoch: 529,train_loss:0.1705537, Train: 1.0000, Val: 0.5460, Test: 0.5340\n",
      "Epoch: 530,train_loss:0.2597823, Train: 1.0000, Val: 0.5580, Test: 0.5430\n",
      "Epoch: 531,train_loss:0.2680199, Train: 1.0000, Val: 0.5920, Test: 0.5860\n",
      "Epoch: 532,train_loss:0.1525580, Train: 1.0000, Val: 0.6040, Test: 0.6120\n",
      "Epoch: 533,train_loss:0.1998257, Train: 1.0000, Val: 0.6280, Test: 0.6200\n",
      "Epoch: 534,train_loss:0.1450954, Train: 1.0000, Val: 0.6280, Test: 0.6110\n",
      "Epoch: 535,train_loss:0.2203598, Train: 1.0000, Val: 0.6300, Test: 0.6110\n",
      "Epoch: 536,train_loss:0.1838196, Train: 1.0000, Val: 0.6260, Test: 0.6250\n",
      "Epoch: 537,train_loss:0.2546456, Train: 1.0000, Val: 0.6340, Test: 0.6400\n",
      "Epoch: 538,train_loss:0.2296484, Train: 1.0000, Val: 0.6300, Test: 0.6420\n",
      "Epoch: 539,train_loss:0.2380973, Train: 1.0000, Val: 0.5960, Test: 0.6190\n",
      "Epoch: 540,train_loss:0.1779970, Train: 1.0000, Val: 0.5800, Test: 0.6020\n",
      "Epoch: 541,train_loss:0.1723265, Train: 1.0000, Val: 0.5640, Test: 0.5860\n",
      "Epoch: 542,train_loss:0.1251024, Train: 1.0000, Val: 0.5880, Test: 0.5980\n",
      "Epoch: 543,train_loss:0.1670571, Train: 1.0000, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 544,train_loss:0.2920581, Train: 1.0000, Val: 0.6320, Test: 0.6380\n",
      "Epoch: 545,train_loss:0.2068926, Train: 1.0000, Val: 0.6160, Test: 0.6130\n",
      "Epoch: 546,train_loss:0.2174843, Train: 1.0000, Val: 0.6180, Test: 0.6080\n",
      "Epoch: 547,train_loss:0.1530710, Train: 1.0000, Val: 0.6200, Test: 0.6120\n",
      "Epoch: 548,train_loss:0.1837555, Train: 1.0000, Val: 0.6180, Test: 0.6160\n",
      "Epoch: 549,train_loss:0.2531394, Train: 1.0000, Val: 0.6340, Test: 0.6260\n",
      "Epoch: 550,train_loss:0.1048159, Train: 1.0000, Val: 0.6080, Test: 0.6160\n",
      "Epoch: 551,train_loss:0.1736495, Train: 1.0000, Val: 0.5740, Test: 0.5890\n",
      "Epoch: 552,train_loss:0.3084700, Train: 1.0000, Val: 0.5260, Test: 0.5570\n",
      "Epoch: 553,train_loss:0.2387599, Train: 1.0000, Val: 0.5160, Test: 0.5410\n",
      "Epoch: 554,train_loss:0.1063590, Train: 1.0000, Val: 0.5300, Test: 0.5390\n",
      "Epoch: 555,train_loss:0.1375173, Train: 1.0000, Val: 0.5520, Test: 0.5680\n",
      "Epoch: 556,train_loss:0.1421868, Train: 1.0000, Val: 0.5660, Test: 0.5830\n",
      "Epoch: 557,train_loss:0.2439877, Train: 1.0000, Val: 0.6040, Test: 0.5910\n",
      "Epoch: 558,train_loss:0.2217736, Train: 1.0000, Val: 0.5980, Test: 0.5560\n",
      "Epoch: 559,train_loss:0.2168784, Train: 1.0000, Val: 0.5820, Test: 0.5390\n",
      "Epoch: 560,train_loss:0.2340686, Train: 1.0000, Val: 0.6120, Test: 0.5680\n",
      "Epoch: 561,train_loss:0.2905547, Train: 1.0000, Val: 0.6140, Test: 0.6010\n",
      "Epoch: 562,train_loss:0.1286310, Train: 1.0000, Val: 0.5900, Test: 0.5900\n",
      "Epoch: 563,train_loss:0.3137317, Train: 1.0000, Val: 0.5800, Test: 0.6020\n",
      "Epoch: 564,train_loss:0.2653123, Train: 1.0000, Val: 0.5640, Test: 0.5890\n",
      "Epoch: 565,train_loss:0.2243291, Train: 1.0000, Val: 0.5680, Test: 0.5920\n",
      "Epoch: 566,train_loss:0.2268167, Train: 1.0000, Val: 0.5740, Test: 0.6180\n",
      "Epoch: 567,train_loss:0.2456613, Train: 1.0000, Val: 0.6020, Test: 0.6510\n",
      "Epoch: 568,train_loss:0.2665717, Train: 1.0000, Val: 0.6280, Test: 0.6460\n",
      "Epoch: 569,train_loss:0.1717086, Train: 1.0000, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 570,train_loss:0.3011098, Train: 1.0000, Val: 0.6180, Test: 0.6250\n",
      "Epoch: 571,train_loss:0.3253644, Train: 1.0000, Val: 0.6260, Test: 0.6360\n",
      "Epoch: 572,train_loss:0.3294584, Train: 1.0000, Val: 0.6240, Test: 0.6350\n",
      "Epoch: 573,train_loss:0.2424254, Train: 1.0000, Val: 0.6100, Test: 0.6320\n",
      "Epoch: 574,train_loss:0.1592628, Train: 1.0000, Val: 0.5800, Test: 0.5920\n",
      "Epoch: 575,train_loss:0.3258697, Train: 1.0000, Val: 0.5800, Test: 0.5970\n",
      "Epoch: 576,train_loss:0.1911600, Train: 1.0000, Val: 0.5700, Test: 0.6010\n",
      "Epoch: 577,train_loss:0.1147692, Train: 1.0000, Val: 0.5960, Test: 0.6040\n",
      "Epoch: 578,train_loss:0.2700195, Train: 1.0000, Val: 0.6140, Test: 0.6210\n",
      "Epoch: 579,train_loss:0.1130187, Train: 1.0000, Val: 0.6360, Test: 0.6270\n",
      "Epoch: 580,train_loss:0.1741746, Train: 1.0000, Val: 0.6280, Test: 0.6330\n",
      "Epoch: 581,train_loss:0.1386334, Train: 1.0000, Val: 0.6220, Test: 0.6210\n",
      "Epoch: 582,train_loss:0.3050900, Train: 1.0000, Val: 0.6140, Test: 0.6180\n",
      "Epoch: 583,train_loss:0.1845232, Train: 1.0000, Val: 0.6240, Test: 0.6360\n",
      "Epoch: 584,train_loss:0.1316929, Train: 1.0000, Val: 0.6080, Test: 0.6370\n",
      "Epoch: 585,train_loss:0.1508240, Train: 1.0000, Val: 0.5920, Test: 0.6250\n",
      "Epoch: 586,train_loss:0.1568982, Train: 1.0000, Val: 0.5760, Test: 0.6030\n",
      "Epoch: 587,train_loss:0.2547714, Train: 1.0000, Val: 0.5500, Test: 0.5900\n",
      "Epoch: 588,train_loss:0.1801460, Train: 1.0000, Val: 0.5540, Test: 0.5900\n",
      "Epoch: 589,train_loss:0.1862784, Train: 1.0000, Val: 0.5720, Test: 0.6030\n",
      "Epoch: 590,train_loss:0.1159303, Train: 1.0000, Val: 0.5880, Test: 0.6090\n",
      "Epoch: 591,train_loss:0.2904029, Train: 1.0000, Val: 0.6180, Test: 0.6200\n",
      "Epoch: 592,train_loss:0.1448830, Train: 1.0000, Val: 0.6120, Test: 0.6100\n",
      "Epoch: 593,train_loss:0.2472331, Train: 1.0000, Val: 0.6100, Test: 0.5760\n",
      "Epoch: 594,train_loss:0.1943171, Train: 1.0000, Val: 0.5720, Test: 0.5650\n",
      "Epoch: 595,train_loss:0.2112407, Train: 1.0000, Val: 0.5880, Test: 0.5710\n",
      "Epoch: 596,train_loss:0.2485489, Train: 1.0000, Val: 0.5940, Test: 0.5920\n",
      "Epoch: 597,train_loss:0.2234529, Train: 1.0000, Val: 0.6120, Test: 0.6130\n",
      "Epoch: 598,train_loss:0.2136210, Train: 1.0000, Val: 0.5920, Test: 0.6130\n",
      "Epoch: 599,train_loss:0.2101956, Train: 1.0000, Val: 0.5820, Test: 0.6000\n",
      "Epoch: 600,train_loss:0.1423652, Train: 1.0000, Val: 0.5840, Test: 0.6080\n",
      "Epoch: 601,train_loss:0.2228729, Train: 1.0000, Val: 0.5940, Test: 0.6190\n",
      "Epoch: 602,train_loss:0.1215941, Train: 1.0000, Val: 0.6180, Test: 0.6260\n",
      "Epoch: 603,train_loss:0.2245590, Train: 1.0000, Val: 0.6320, Test: 0.6300\n",
      "Epoch: 604,train_loss:0.2958787, Train: 1.0000, Val: 0.6320, Test: 0.6250\n",
      "Epoch: 605,train_loss:0.1415683, Train: 1.0000, Val: 0.6220, Test: 0.6130\n",
      "Epoch: 606,train_loss:0.2523338, Train: 1.0000, Val: 0.6120, Test: 0.5980\n",
      "Epoch: 607,train_loss:0.2735724, Train: 1.0000, Val: 0.5760, Test: 0.5660\n",
      "Epoch: 608,train_loss:0.1025376, Train: 1.0000, Val: 0.5560, Test: 0.5550\n",
      "Epoch: 609,train_loss:0.1467198, Train: 1.0000, Val: 0.5620, Test: 0.5700\n",
      "Epoch: 610,train_loss:0.2738438, Train: 1.0000, Val: 0.6060, Test: 0.6040\n",
      "Epoch: 611,train_loss:0.2424066, Train: 1.0000, Val: 0.6300, Test: 0.6400\n",
      "Epoch: 612,train_loss:0.1480951, Train: 1.0000, Val: 0.6360, Test: 0.6470\n",
      "Epoch: 613,train_loss:0.2121561, Train: 1.0000, Val: 0.6180, Test: 0.6390\n",
      "Epoch: 614,train_loss:0.2835526, Train: 1.0000, Val: 0.5960, Test: 0.6240\n",
      "Epoch: 615,train_loss:0.2848113, Train: 1.0000, Val: 0.6240, Test: 0.6430\n",
      "Epoch: 616,train_loss:0.2373028, Train: 1.0000, Val: 0.6280, Test: 0.6410\n",
      "Epoch: 617,train_loss:0.2095002, Train: 1.0000, Val: 0.6240, Test: 0.6460\n",
      "Epoch: 618,train_loss:0.1813100, Train: 1.0000, Val: 0.6260, Test: 0.6440\n",
      "Epoch: 619,train_loss:0.1309005, Train: 1.0000, Val: 0.6360, Test: 0.6360\n",
      "Epoch: 620,train_loss:0.2495219, Train: 1.0000, Val: 0.6300, Test: 0.6360\n",
      "Epoch: 621,train_loss:0.2207698, Train: 1.0000, Val: 0.6260, Test: 0.6390\n",
      "Epoch: 622,train_loss:0.1761355, Train: 1.0000, Val: 0.6280, Test: 0.6430\n",
      "Epoch: 623,train_loss:0.2352786, Train: 1.0000, Val: 0.6260, Test: 0.6360\n",
      "Epoch: 624,train_loss:0.2672948, Train: 1.0000, Val: 0.6340, Test: 0.6410\n",
      "Epoch: 625,train_loss:0.1781193, Train: 1.0000, Val: 0.6320, Test: 0.6560\n",
      "Epoch: 626,train_loss:0.1742120, Train: 1.0000, Val: 0.6220, Test: 0.6260\n",
      "Epoch: 627,train_loss:0.1618342, Train: 1.0000, Val: 0.6180, Test: 0.6100\n",
      "Epoch: 628,train_loss:0.1188884, Train: 1.0000, Val: 0.6200, Test: 0.6100\n",
      "Epoch: 629,train_loss:0.1783125, Train: 1.0000, Val: 0.6200, Test: 0.6080\n",
      "Epoch: 630,train_loss:0.1542351, Train: 1.0000, Val: 0.6260, Test: 0.6240\n",
      "Epoch: 631,train_loss:0.1492791, Train: 1.0000, Val: 0.6040, Test: 0.6120\n",
      "Epoch: 632,train_loss:0.1827101, Train: 1.0000, Val: 0.6020, Test: 0.6120\n",
      "Epoch: 633,train_loss:0.1481568, Train: 1.0000, Val: 0.6000, Test: 0.6060\n",
      "Epoch: 634,train_loss:0.1620618, Train: 1.0000, Val: 0.6100, Test: 0.6110\n",
      "Epoch: 635,train_loss:0.2627050, Train: 1.0000, Val: 0.6080, Test: 0.6240\n",
      "Epoch: 636,train_loss:0.2651269, Train: 1.0000, Val: 0.6160, Test: 0.6100\n",
      "Epoch: 637,train_loss:0.2016405, Train: 1.0000, Val: 0.6120, Test: 0.6100\n",
      "Epoch: 638,train_loss:0.2969382, Train: 1.0000, Val: 0.6120, Test: 0.5950\n",
      "Epoch: 639,train_loss:0.1586135, Train: 1.0000, Val: 0.6080, Test: 0.5840\n",
      "Epoch: 640,train_loss:0.1505103, Train: 1.0000, Val: 0.5980, Test: 0.5810\n",
      "Epoch: 641,train_loss:0.2240301, Train: 1.0000, Val: 0.6200, Test: 0.6090\n",
      "Epoch: 642,train_loss:0.1476820, Train: 1.0000, Val: 0.6240, Test: 0.6330\n",
      "Epoch: 643,train_loss:0.2129843, Train: 1.0000, Val: 0.6160, Test: 0.6230\n",
      "Epoch: 644,train_loss:0.1548425, Train: 1.0000, Val: 0.6120, Test: 0.6120\n",
      "Epoch: 645,train_loss:0.2387509, Train: 1.0000, Val: 0.5940, Test: 0.6140\n",
      "Epoch: 646,train_loss:0.2986665, Train: 1.0000, Val: 0.6060, Test: 0.6260\n",
      "Epoch: 647,train_loss:0.1900591, Train: 1.0000, Val: 0.5980, Test: 0.5700\n",
      "Epoch: 648,train_loss:0.1872303, Train: 1.0000, Val: 0.5220, Test: 0.5040\n",
      "Epoch: 649,train_loss:0.2022352, Train: 1.0000, Val: 0.5080, Test: 0.4970\n",
      "Epoch: 650,train_loss:0.2785594, Train: 1.0000, Val: 0.5640, Test: 0.5510\n",
      "Epoch: 651,train_loss:0.1525112, Train: 1.0000, Val: 0.5920, Test: 0.5840\n",
      "Epoch: 652,train_loss:0.1744823, Train: 1.0000, Val: 0.5920, Test: 0.6160\n",
      "Epoch: 653,train_loss:0.1594976, Train: 1.0000, Val: 0.6060, Test: 0.6110\n",
      "Epoch: 654,train_loss:0.2497917, Train: 1.0000, Val: 0.6120, Test: 0.6240\n",
      "Epoch: 655,train_loss:0.2558894, Train: 1.0000, Val: 0.6200, Test: 0.6190\n",
      "Epoch: 656,train_loss:0.1899152, Train: 1.0000, Val: 0.5980, Test: 0.6170\n",
      "Epoch: 657,train_loss:0.1806506, Train: 1.0000, Val: 0.6060, Test: 0.6160\n",
      "Epoch: 658,train_loss:0.2678888, Train: 1.0000, Val: 0.5960, Test: 0.6170\n",
      "Epoch: 659,train_loss:0.2672944, Train: 1.0000, Val: 0.5900, Test: 0.5970\n",
      "Epoch: 660,train_loss:0.2650321, Train: 1.0000, Val: 0.5840, Test: 0.5860\n",
      "Epoch: 661,train_loss:0.1948390, Train: 1.0000, Val: 0.5860, Test: 0.5970\n",
      "Epoch: 662,train_loss:0.3227540, Train: 1.0000, Val: 0.5960, Test: 0.6170\n",
      "Epoch: 663,train_loss:0.1171600, Train: 1.0000, Val: 0.6080, Test: 0.6210\n",
      "Epoch: 664,train_loss:0.2640177, Train: 1.0000, Val: 0.6000, Test: 0.6170\n",
      "Epoch: 665,train_loss:0.1813684, Train: 1.0000, Val: 0.5980, Test: 0.6190\n",
      "Epoch: 666,train_loss:0.1944719, Train: 1.0000, Val: 0.6020, Test: 0.6210\n",
      "Epoch: 667,train_loss:0.2416379, Train: 1.0000, Val: 0.5880, Test: 0.6120\n",
      "Epoch: 668,train_loss:0.2057994, Train: 1.0000, Val: 0.5800, Test: 0.5990\n",
      "Epoch: 669,train_loss:0.2236970, Train: 1.0000, Val: 0.5420, Test: 0.5630\n",
      "Epoch: 670,train_loss:0.2463826, Train: 1.0000, Val: 0.5500, Test: 0.5750\n",
      "Epoch: 671,train_loss:0.2800219, Train: 1.0000, Val: 0.5780, Test: 0.5820\n",
      "Epoch: 672,train_loss:0.2764847, Train: 1.0000, Val: 0.5460, Test: 0.5600\n",
      "Epoch: 673,train_loss:0.2550834, Train: 1.0000, Val: 0.5580, Test: 0.5690\n",
      "Epoch: 674,train_loss:0.2264879, Train: 1.0000, Val: 0.5700, Test: 0.5910\n",
      "Epoch: 675,train_loss:0.2196739, Train: 1.0000, Val: 0.5840, Test: 0.6000\n",
      "Epoch: 676,train_loss:0.2748743, Train: 1.0000, Val: 0.5880, Test: 0.5890\n",
      "Epoch: 677,train_loss:0.2177062, Train: 1.0000, Val: 0.5740, Test: 0.5780\n",
      "Epoch: 678,train_loss:0.2345789, Train: 1.0000, Val: 0.5680, Test: 0.5710\n",
      "Epoch: 679,train_loss:0.1839161, Train: 1.0000, Val: 0.5880, Test: 0.5840\n",
      "Epoch: 680,train_loss:0.2740687, Train: 1.0000, Val: 0.5740, Test: 0.5780\n",
      "Epoch: 681,train_loss:0.2196826, Train: 1.0000, Val: 0.5900, Test: 0.5940\n",
      "Epoch: 682,train_loss:0.2637830, Train: 1.0000, Val: 0.6040, Test: 0.6040\n",
      "Epoch: 683,train_loss:0.2547131, Train: 1.0000, Val: 0.5980, Test: 0.5960\n",
      "Epoch: 684,train_loss:0.2378505, Train: 1.0000, Val: 0.5720, Test: 0.5760\n",
      "Epoch: 685,train_loss:0.1656050, Train: 1.0000, Val: 0.5880, Test: 0.5780\n",
      "Epoch: 686,train_loss:0.2605708, Train: 1.0000, Val: 0.6060, Test: 0.6070\n",
      "Epoch: 687,train_loss:0.2503738, Train: 1.0000, Val: 0.6040, Test: 0.6100\n",
      "Epoch: 688,train_loss:0.1426433, Train: 1.0000, Val: 0.5960, Test: 0.6080\n",
      "Epoch: 689,train_loss:0.1914938, Train: 1.0000, Val: 0.5680, Test: 0.5980\n",
      "Epoch: 690,train_loss:0.2168264, Train: 1.0000, Val: 0.5520, Test: 0.5670\n",
      "Epoch: 691,train_loss:0.2029275, Train: 1.0000, Val: 0.5880, Test: 0.5960\n",
      "Epoch: 692,train_loss:0.1791999, Train: 1.0000, Val: 0.6200, Test: 0.6230\n",
      "Epoch: 693,train_loss:0.2277139, Train: 1.0000, Val: 0.6040, Test: 0.6100\n",
      "Epoch: 694,train_loss:0.2179705, Train: 1.0000, Val: 0.5980, Test: 0.6070\n",
      "Epoch: 695,train_loss:0.1805307, Train: 1.0000, Val: 0.5960, Test: 0.6010\n",
      "Epoch: 696,train_loss:0.2442942, Train: 1.0000, Val: 0.6020, Test: 0.6050\n",
      "Epoch: 697,train_loss:0.2366613, Train: 1.0000, Val: 0.5980, Test: 0.6060\n",
      "Epoch: 698,train_loss:0.2374344, Train: 1.0000, Val: 0.6020, Test: 0.6070\n",
      "Epoch: 699,train_loss:0.0999602, Train: 1.0000, Val: 0.5980, Test: 0.6090\n",
      "Epoch: 700,train_loss:0.1781814, Train: 1.0000, Val: 0.6120, Test: 0.6030\n",
      "Epoch: 701,train_loss:0.2253035, Train: 1.0000, Val: 0.5960, Test: 0.6130\n",
      "Epoch: 702,train_loss:0.2808853, Train: 1.0000, Val: 0.5920, Test: 0.6130\n",
      "Epoch: 703,train_loss:0.1852904, Train: 1.0000, Val: 0.5980, Test: 0.6080\n",
      "Epoch: 704,train_loss:0.1231892, Train: 1.0000, Val: 0.5860, Test: 0.6060\n",
      "Epoch: 705,train_loss:0.2495377, Train: 1.0000, Val: 0.5980, Test: 0.5930\n",
      "Epoch: 706,train_loss:0.2310397, Train: 1.0000, Val: 0.5920, Test: 0.5850\n",
      "Epoch: 707,train_loss:0.2719217, Train: 1.0000, Val: 0.5800, Test: 0.5640\n",
      "Epoch: 708,train_loss:0.2621142, Train: 1.0000, Val: 0.5880, Test: 0.5640\n",
      "Epoch: 709,train_loss:0.2540120, Train: 1.0000, Val: 0.6060, Test: 0.5740\n",
      "Epoch: 710,train_loss:0.1965573, Train: 1.0000, Val: 0.6140, Test: 0.5910\n",
      "Epoch: 711,train_loss:0.2157467, Train: 1.0000, Val: 0.6200, Test: 0.5970\n",
      "Epoch: 712,train_loss:0.1839075, Train: 1.0000, Val: 0.6200, Test: 0.6050\n",
      "Epoch: 713,train_loss:0.1524728, Train: 1.0000, Val: 0.6200, Test: 0.6160\n",
      "Epoch: 714,train_loss:0.1756237, Train: 1.0000, Val: 0.6000, Test: 0.6200\n",
      "Epoch: 715,train_loss:0.2683039, Train: 1.0000, Val: 0.5980, Test: 0.6200\n",
      "Epoch: 716,train_loss:0.1401899, Train: 1.0000, Val: 0.6080, Test: 0.6170\n",
      "Epoch: 717,train_loss:0.1959608, Train: 1.0000, Val: 0.6140, Test: 0.6110\n",
      "Epoch: 718,train_loss:0.1928741, Train: 1.0000, Val: 0.6200, Test: 0.6030\n",
      "Epoch: 719,train_loss:0.2166774, Train: 1.0000, Val: 0.6100, Test: 0.5900\n",
      "Epoch: 720,train_loss:0.2018161, Train: 1.0000, Val: 0.6080, Test: 0.5900\n",
      "Epoch: 721,train_loss:0.2216521, Train: 1.0000, Val: 0.6100, Test: 0.5920\n",
      "Epoch: 722,train_loss:0.2223015, Train: 1.0000, Val: 0.6020, Test: 0.5960\n",
      "Epoch: 723,train_loss:0.2545695, Train: 1.0000, Val: 0.6000, Test: 0.5930\n",
      "Epoch: 724,train_loss:0.2407462, Train: 1.0000, Val: 0.6020, Test: 0.6030\n",
      "Epoch: 725,train_loss:0.2853772, Train: 1.0000, Val: 0.6160, Test: 0.6120\n",
      "Epoch: 726,train_loss:0.1975337, Train: 1.0000, Val: 0.6060, Test: 0.6150\n",
      "Epoch: 727,train_loss:0.1605351, Train: 1.0000, Val: 0.5860, Test: 0.6100\n",
      "Epoch: 728,train_loss:0.2328939, Train: 1.0000, Val: 0.6140, Test: 0.6150\n",
      "Epoch: 729,train_loss:0.2039027, Train: 1.0000, Val: 0.6260, Test: 0.6150\n",
      "Epoch: 730,train_loss:0.2030181, Train: 1.0000, Val: 0.6260, Test: 0.6120\n",
      "Epoch: 731,train_loss:0.1173306, Train: 1.0000, Val: 0.6100, Test: 0.6120\n",
      "Epoch: 732,train_loss:0.1871435, Train: 1.0000, Val: 0.6020, Test: 0.6070\n",
      "Epoch: 733,train_loss:0.1746524, Train: 1.0000, Val: 0.6000, Test: 0.5970\n",
      "Epoch: 734,train_loss:0.0514518, Train: 1.0000, Val: 0.6020, Test: 0.5860\n",
      "Epoch: 735,train_loss:0.2325000, Train: 1.0000, Val: 0.6060, Test: 0.5980\n",
      "Epoch: 736,train_loss:0.1682126, Train: 1.0000, Val: 0.6120, Test: 0.5970\n",
      "Epoch: 737,train_loss:0.1918013, Train: 1.0000, Val: 0.6000, Test: 0.6010\n",
      "Epoch: 738,train_loss:0.2217537, Train: 1.0000, Val: 0.5940, Test: 0.5800\n",
      "Epoch: 739,train_loss:0.1972980, Train: 1.0000, Val: 0.5900, Test: 0.5690\n",
      "Epoch: 740,train_loss:0.2414552, Train: 1.0000, Val: 0.5920, Test: 0.5740\n",
      "Epoch: 741,train_loss:0.2230188, Train: 1.0000, Val: 0.5920, Test: 0.5910\n",
      "Epoch: 742,train_loss:0.1449686, Train: 1.0000, Val: 0.6060, Test: 0.6030\n",
      "Epoch: 743,train_loss:0.2190628, Train: 1.0000, Val: 0.6140, Test: 0.5990\n",
      "Epoch: 744,train_loss:0.1732185, Train: 1.0000, Val: 0.6080, Test: 0.5900\n",
      "Epoch: 745,train_loss:0.1687489, Train: 1.0000, Val: 0.6220, Test: 0.5960\n",
      "Epoch: 746,train_loss:0.2005671, Train: 1.0000, Val: 0.6080, Test: 0.5970\n",
      "Epoch: 747,train_loss:0.1901791, Train: 1.0000, Val: 0.6160, Test: 0.6110\n",
      "Epoch: 748,train_loss:0.2890931, Train: 1.0000, Val: 0.6100, Test: 0.6140\n",
      "Epoch: 749,train_loss:0.1929297, Train: 1.0000, Val: 0.6180, Test: 0.6140\n",
      "Epoch: 750,train_loss:0.2920090, Train: 1.0000, Val: 0.6300, Test: 0.6250\n",
      "Epoch: 751,train_loss:0.2279582, Train: 1.0000, Val: 0.6140, Test: 0.6210\n",
      "Epoch: 752,train_loss:0.2198667, Train: 1.0000, Val: 0.6140, Test: 0.6260\n",
      "Epoch: 753,train_loss:0.1966390, Train: 1.0000, Val: 0.6200, Test: 0.6130\n",
      "Epoch: 754,train_loss:0.1696713, Train: 1.0000, Val: 0.6020, Test: 0.5730\n",
      "Epoch: 755,train_loss:0.2653684, Train: 1.0000, Val: 0.5800, Test: 0.5510\n",
      "Epoch: 756,train_loss:0.2697457, Train: 1.0000, Val: 0.5620, Test: 0.5380\n",
      "Epoch: 757,train_loss:0.1363084, Train: 1.0000, Val: 0.5840, Test: 0.5780\n",
      "Epoch: 758,train_loss:0.2366496, Train: 1.0000, Val: 0.6020, Test: 0.6060\n",
      "Epoch: 759,train_loss:0.1630839, Train: 1.0000, Val: 0.6140, Test: 0.6070\n",
      "Epoch: 760,train_loss:0.2297796, Train: 1.0000, Val: 0.5980, Test: 0.5860\n",
      "Epoch: 761,train_loss:0.2390270, Train: 1.0000, Val: 0.6040, Test: 0.6000\n",
      "Epoch: 762,train_loss:0.3106699, Train: 1.0000, Val: 0.6220, Test: 0.6270\n",
      "Epoch: 763,train_loss:0.1937880, Train: 1.0000, Val: 0.5640, Test: 0.5840\n",
      "Epoch: 764,train_loss:0.1612361, Train: 1.0000, Val: 0.5020, Test: 0.5150\n",
      "Epoch: 765,train_loss:0.3723671, Train: 1.0000, Val: 0.5680, Test: 0.5500\n",
      "Epoch: 766,train_loss:0.2030509, Train: 1.0000, Val: 0.5860, Test: 0.5730\n",
      "Epoch: 767,train_loss:0.1768798, Train: 1.0000, Val: 0.6120, Test: 0.5680\n",
      "Epoch: 768,train_loss:0.1337541, Train: 1.0000, Val: 0.5900, Test: 0.5410\n",
      "Epoch: 769,train_loss:0.1996625, Train: 1.0000, Val: 0.5820, Test: 0.5520\n",
      "Epoch: 770,train_loss:0.2450435, Train: 1.0000, Val: 0.6080, Test: 0.5870\n",
      "Epoch: 771,train_loss:0.1791798, Train: 1.0000, Val: 0.6100, Test: 0.6240\n",
      "Epoch: 772,train_loss:0.2903285, Train: 1.0000, Val: 0.6200, Test: 0.6240\n",
      "Epoch: 773,train_loss:0.2684135, Train: 1.0000, Val: 0.5900, Test: 0.6020\n",
      "Epoch: 774,train_loss:0.2528623, Train: 1.0000, Val: 0.5880, Test: 0.5890\n",
      "Epoch: 775,train_loss:0.1188500, Train: 1.0000, Val: 0.5780, Test: 0.5780\n",
      "Epoch: 776,train_loss:0.1934803, Train: 1.0000, Val: 0.5760, Test: 0.5730\n",
      "Epoch: 777,train_loss:0.1867606, Train: 1.0000, Val: 0.5860, Test: 0.5710\n",
      "Epoch: 778,train_loss:0.2127924, Train: 1.0000, Val: 0.6020, Test: 0.5700\n",
      "Epoch: 779,train_loss:0.2204782, Train: 1.0000, Val: 0.5880, Test: 0.5620\n",
      "Epoch: 780,train_loss:0.1358607, Train: 1.0000, Val: 0.5760, Test: 0.5610\n",
      "Epoch: 781,train_loss:0.1064731, Train: 1.0000, Val: 0.5980, Test: 0.5960\n",
      "Epoch: 782,train_loss:0.2678495, Train: 1.0000, Val: 0.6240, Test: 0.6210\n",
      "Epoch: 783,train_loss:0.1825316, Train: 1.0000, Val: 0.6080, Test: 0.5920\n",
      "Epoch: 784,train_loss:0.2723553, Train: 1.0000, Val: 0.5880, Test: 0.5850\n",
      "Epoch: 785,train_loss:0.2387149, Train: 1.0000, Val: 0.5760, Test: 0.5780\n",
      "Epoch: 786,train_loss:0.2262857, Train: 1.0000, Val: 0.5800, Test: 0.5770\n",
      "Epoch: 787,train_loss:0.1749331, Train: 1.0000, Val: 0.5760, Test: 0.5850\n",
      "Epoch: 788,train_loss:0.2005394, Train: 1.0000, Val: 0.6040, Test: 0.6070\n",
      "Epoch: 789,train_loss:0.1641319, Train: 1.0000, Val: 0.5940, Test: 0.6120\n",
      "Epoch: 790,train_loss:0.2725006, Train: 1.0000, Val: 0.5960, Test: 0.5990\n",
      "Epoch: 791,train_loss:0.2458730, Train: 1.0000, Val: 0.5840, Test: 0.5990\n",
      "Epoch: 792,train_loss:0.2082604, Train: 1.0000, Val: 0.5820, Test: 0.5900\n",
      "Epoch: 793,train_loss:0.1565373, Train: 1.0000, Val: 0.5840, Test: 0.5750\n",
      "Epoch: 794,train_loss:0.1601188, Train: 1.0000, Val: 0.6060, Test: 0.5770\n",
      "Epoch: 795,train_loss:0.2813656, Train: 1.0000, Val: 0.5980, Test: 0.5820\n",
      "Epoch: 796,train_loss:0.2965266, Train: 1.0000, Val: 0.5940, Test: 0.5730\n",
      "Epoch: 797,train_loss:0.3680063, Train: 1.0000, Val: 0.5980, Test: 0.5840\n",
      "Epoch: 798,train_loss:0.1576346, Train: 1.0000, Val: 0.5900, Test: 0.6000\n",
      "Epoch: 799,train_loss:0.0980796, Train: 1.0000, Val: 0.5960, Test: 0.6130\n",
      "Epoch: 800,train_loss:0.1657603, Train: 1.0000, Val: 0.5880, Test: 0.6190\n",
      "Epoch: 801,train_loss:0.2856755, Train: 1.0000, Val: 0.5800, Test: 0.6070\n",
      "Epoch: 802,train_loss:0.2345427, Train: 1.0000, Val: 0.5840, Test: 0.6080\n",
      "Epoch: 803,train_loss:0.2365914, Train: 1.0000, Val: 0.5780, Test: 0.5990\n",
      "Epoch: 804,train_loss:0.2531795, Train: 1.0000, Val: 0.5880, Test: 0.6180\n",
      "Epoch: 805,train_loss:0.2137052, Train: 1.0000, Val: 0.5940, Test: 0.6260\n",
      "Epoch: 806,train_loss:0.1904828, Train: 1.0000, Val: 0.6000, Test: 0.6090\n",
      "Epoch: 807,train_loss:0.1025911, Train: 1.0000, Val: 0.5780, Test: 0.5750\n",
      "Epoch: 808,train_loss:0.2575419, Train: 1.0000, Val: 0.5680, Test: 0.5430\n",
      "Epoch: 809,train_loss:0.1922567, Train: 1.0000, Val: 0.5740, Test: 0.5430\n",
      "Epoch: 810,train_loss:0.1532728, Train: 1.0000, Val: 0.5700, Test: 0.5350\n",
      "Epoch: 811,train_loss:0.1473017, Train: 1.0000, Val: 0.5800, Test: 0.5600\n",
      "Epoch: 812,train_loss:0.1338485, Train: 1.0000, Val: 0.5980, Test: 0.5760\n",
      "Epoch: 813,train_loss:0.1831646, Train: 1.0000, Val: 0.6100, Test: 0.6030\n",
      "Epoch: 814,train_loss:0.3033685, Train: 1.0000, Val: 0.5880, Test: 0.6050\n",
      "Epoch: 815,train_loss:0.1986757, Train: 1.0000, Val: 0.5720, Test: 0.6040\n",
      "Epoch: 816,train_loss:0.1723920, Train: 1.0000, Val: 0.5720, Test: 0.5920\n",
      "Epoch: 817,train_loss:0.2048008, Train: 1.0000, Val: 0.5460, Test: 0.5620\n",
      "Epoch: 818,train_loss:0.2822438, Train: 1.0000, Val: 0.5420, Test: 0.5370\n",
      "Epoch: 819,train_loss:0.2988048, Train: 1.0000, Val: 0.5440, Test: 0.5270\n",
      "Epoch: 820,train_loss:0.2194330, Train: 1.0000, Val: 0.5940, Test: 0.5800\n",
      "Epoch: 821,train_loss:0.2831437, Train: 1.0000, Val: 0.6180, Test: 0.6140\n",
      "Epoch: 822,train_loss:0.1974852, Train: 1.0000, Val: 0.5760, Test: 0.5830\n",
      "Epoch: 823,train_loss:0.1449593, Train: 1.0000, Val: 0.5720, Test: 0.5730\n",
      "Epoch: 824,train_loss:0.1370087, Train: 1.0000, Val: 0.5840, Test: 0.5830\n",
      "Epoch: 825,train_loss:0.1855694, Train: 1.0000, Val: 0.5980, Test: 0.6030\n",
      "Epoch: 826,train_loss:0.2326263, Train: 1.0000, Val: 0.6020, Test: 0.6230\n",
      "Epoch: 827,train_loss:0.1109749, Train: 1.0000, Val: 0.6060, Test: 0.6240\n",
      "Epoch: 828,train_loss:0.2342556, Train: 1.0000, Val: 0.5900, Test: 0.5990\n",
      "Epoch: 829,train_loss:0.1683289, Train: 1.0000, Val: 0.5800, Test: 0.5690\n",
      "Epoch: 830,train_loss:0.1620350, Train: 1.0000, Val: 0.5880, Test: 0.5810\n",
      "Epoch: 831,train_loss:0.1539852, Train: 1.0000, Val: 0.5900, Test: 0.6140\n",
      "Epoch: 832,train_loss:0.1776498, Train: 1.0000, Val: 0.6020, Test: 0.6130\n",
      "Epoch: 833,train_loss:0.2659739, Train: 1.0000, Val: 0.6100, Test: 0.6120\n",
      "Epoch: 834,train_loss:0.1229788, Train: 1.0000, Val: 0.6260, Test: 0.6190\n",
      "Epoch: 835,train_loss:0.1179980, Train: 1.0000, Val: 0.6260, Test: 0.6210\n",
      "Epoch: 836,train_loss:0.1956249, Train: 1.0000, Val: 0.6240, Test: 0.6240\n",
      "Epoch: 837,train_loss:0.1900133, Train: 1.0000, Val: 0.6240, Test: 0.6180\n",
      "Epoch: 838,train_loss:0.1962617, Train: 1.0000, Val: 0.6240, Test: 0.6170\n",
      "Epoch: 839,train_loss:0.3305937, Train: 1.0000, Val: 0.6220, Test: 0.6010\n",
      "Epoch: 840,train_loss:0.1498848, Train: 1.0000, Val: 0.6200, Test: 0.5890\n",
      "Epoch: 841,train_loss:0.1511388, Train: 1.0000, Val: 0.6020, Test: 0.5750\n",
      "Epoch: 842,train_loss:0.2397921, Train: 1.0000, Val: 0.6000, Test: 0.5760\n",
      "Epoch: 843,train_loss:0.2136524, Train: 1.0000, Val: 0.6100, Test: 0.5880\n",
      "Epoch: 844,train_loss:0.2753035, Train: 1.0000, Val: 0.6160, Test: 0.6030\n",
      "Epoch: 845,train_loss:0.2079875, Train: 1.0000, Val: 0.6260, Test: 0.6280\n",
      "Epoch: 846,train_loss:0.1742744, Train: 1.0000, Val: 0.6000, Test: 0.6140\n",
      "Epoch: 847,train_loss:0.1430847, Train: 1.0000, Val: 0.5760, Test: 0.5980\n",
      "Epoch: 848,train_loss:0.1648083, Train: 1.0000, Val: 0.5720, Test: 0.5880\n",
      "Epoch: 849,train_loss:0.1747570, Train: 1.0000, Val: 0.5680, Test: 0.5840\n",
      "Epoch: 850,train_loss:0.2467210, Train: 1.0000, Val: 0.5780, Test: 0.6070\n",
      "Epoch: 851,train_loss:0.1696784, Train: 1.0000, Val: 0.6020, Test: 0.6280\n",
      "Epoch: 852,train_loss:0.1597248, Train: 1.0000, Val: 0.6100, Test: 0.6120\n",
      "Epoch: 853,train_loss:0.2400516, Train: 1.0000, Val: 0.6000, Test: 0.6040\n",
      "Epoch: 854,train_loss:0.2645972, Train: 1.0000, Val: 0.5760, Test: 0.5880\n",
      "Epoch: 855,train_loss:0.1633554, Train: 1.0000, Val: 0.5720, Test: 0.5820\n",
      "Epoch: 856,train_loss:0.2103517, Train: 1.0000, Val: 0.6100, Test: 0.5860\n",
      "Epoch: 857,train_loss:0.2378685, Train: 1.0000, Val: 0.6140, Test: 0.5920\n",
      "Epoch: 858,train_loss:0.1839433, Train: 1.0000, Val: 0.5940, Test: 0.5890\n",
      "Epoch: 859,train_loss:0.1802006, Train: 1.0000, Val: 0.5860, Test: 0.5870\n",
      "Epoch: 860,train_loss:0.2504795, Train: 1.0000, Val: 0.5960, Test: 0.6050\n",
      "Epoch: 861,train_loss:0.2316950, Train: 1.0000, Val: 0.6000, Test: 0.6180\n",
      "Epoch: 862,train_loss:0.2762360, Train: 1.0000, Val: 0.6040, Test: 0.6190\n",
      "Epoch: 863,train_loss:0.1484032, Train: 1.0000, Val: 0.5920, Test: 0.5870\n",
      "Epoch: 864,train_loss:0.1862979, Train: 1.0000, Val: 0.5600, Test: 0.5390\n",
      "Epoch: 865,train_loss:0.2302291, Train: 1.0000, Val: 0.5660, Test: 0.5310\n",
      "Epoch: 866,train_loss:0.3153875, Train: 1.0000, Val: 0.6000, Test: 0.5650\n",
      "Epoch: 867,train_loss:0.1861088, Train: 1.0000, Val: 0.6140, Test: 0.6120\n",
      "Epoch: 868,train_loss:0.2381703, Train: 1.0000, Val: 0.6200, Test: 0.6070\n",
      "Epoch: 869,train_loss:0.2774818, Train: 1.0000, Val: 0.6080, Test: 0.5900\n",
      "Epoch: 870,train_loss:0.1406339, Train: 1.0000, Val: 0.6020, Test: 0.5830\n",
      "Epoch: 871,train_loss:0.1427958, Train: 1.0000, Val: 0.5980, Test: 0.5920\n",
      "Epoch: 872,train_loss:0.1759858, Train: 1.0000, Val: 0.6000, Test: 0.6000\n",
      "Epoch: 873,train_loss:0.2383105, Train: 1.0000, Val: 0.6080, Test: 0.6050\n",
      "Epoch: 874,train_loss:0.1874202, Train: 1.0000, Val: 0.6020, Test: 0.6070\n",
      "Epoch: 875,train_loss:0.1555750, Train: 1.0000, Val: 0.6100, Test: 0.6020\n",
      "Epoch: 876,train_loss:0.3335306, Train: 1.0000, Val: 0.5900, Test: 0.5930\n",
      "Epoch: 877,train_loss:0.1810960, Train: 1.0000, Val: 0.5920, Test: 0.5870\n",
      "Epoch: 878,train_loss:0.2865035, Train: 1.0000, Val: 0.5820, Test: 0.5780\n",
      "Epoch: 879,train_loss:0.2053464, Train: 1.0000, Val: 0.5960, Test: 0.5950\n",
      "Epoch: 880,train_loss:0.1608896, Train: 1.0000, Val: 0.6160, Test: 0.6200\n",
      "Epoch: 881,train_loss:0.1650462, Train: 1.0000, Val: 0.6100, Test: 0.6290\n",
      "Epoch: 882,train_loss:0.2536193, Train: 1.0000, Val: 0.6180, Test: 0.6300\n",
      "Epoch: 883,train_loss:0.2101639, Train: 1.0000, Val: 0.6300, Test: 0.6250\n",
      "Epoch: 884,train_loss:0.2019846, Train: 1.0000, Val: 0.6200, Test: 0.6270\n",
      "Epoch: 885,train_loss:0.2354631, Train: 1.0000, Val: 0.6340, Test: 0.6100\n",
      "Epoch: 886,train_loss:0.2359661, Train: 1.0000, Val: 0.6180, Test: 0.5880\n",
      "Epoch: 887,train_loss:0.1445689, Train: 1.0000, Val: 0.6140, Test: 0.5780\n",
      "Epoch: 888,train_loss:0.1640143, Train: 1.0000, Val: 0.6040, Test: 0.5710\n",
      "Epoch: 889,train_loss:0.1832012, Train: 1.0000, Val: 0.6160, Test: 0.5810\n",
      "Epoch: 890,train_loss:0.2645402, Train: 1.0000, Val: 0.6180, Test: 0.5910\n",
      "Epoch: 891,train_loss:0.2345761, Train: 1.0000, Val: 0.6180, Test: 0.5940\n",
      "Epoch: 892,train_loss:0.2336151, Train: 1.0000, Val: 0.6200, Test: 0.6020\n",
      "Epoch: 893,train_loss:0.2326316, Train: 1.0000, Val: 0.6140, Test: 0.6000\n",
      "Epoch: 894,train_loss:0.1622443, Train: 1.0000, Val: 0.6420, Test: 0.6210\n",
      "Epoch: 895,train_loss:0.1817820, Train: 1.0000, Val: 0.6380, Test: 0.6140\n",
      "Epoch: 896,train_loss:0.2221291, Train: 1.0000, Val: 0.6180, Test: 0.6080\n",
      "Epoch: 897,train_loss:0.1977438, Train: 1.0000, Val: 0.6120, Test: 0.6150\n",
      "Epoch: 898,train_loss:0.1462779, Train: 1.0000, Val: 0.6160, Test: 0.6070\n",
      "Epoch: 899,train_loss:0.1473455, Train: 1.0000, Val: 0.6020, Test: 0.6130\n",
      "Epoch: 900,train_loss:0.2250978, Train: 1.0000, Val: 0.6100, Test: 0.6160\n",
      "Epoch: 901,train_loss:0.1670029, Train: 1.0000, Val: 0.6120, Test: 0.6100\n",
      "Epoch: 902,train_loss:0.2438713, Train: 1.0000, Val: 0.6200, Test: 0.6060\n",
      "Epoch: 903,train_loss:0.1725721, Train: 1.0000, Val: 0.6280, Test: 0.6050\n",
      "Epoch: 904,train_loss:0.1453577, Train: 1.0000, Val: 0.6160, Test: 0.6100\n",
      "Epoch: 905,train_loss:0.1733860, Train: 1.0000, Val: 0.6200, Test: 0.6140\n",
      "Epoch: 906,train_loss:0.1422068, Train: 1.0000, Val: 0.6180, Test: 0.6080\n",
      "Epoch: 907,train_loss:0.2124617, Train: 1.0000, Val: 0.6180, Test: 0.6030\n",
      "Epoch: 908,train_loss:0.1748985, Train: 1.0000, Val: 0.6120, Test: 0.6050\n",
      "Epoch: 909,train_loss:0.1855516, Train: 1.0000, Val: 0.6020, Test: 0.6030\n",
      "Epoch: 910,train_loss:0.2409618, Train: 1.0000, Val: 0.6020, Test: 0.6060\n",
      "Epoch: 911,train_loss:0.1128917, Train: 1.0000, Val: 0.6040, Test: 0.6060\n",
      "Epoch: 912,train_loss:0.2164099, Train: 1.0000, Val: 0.6100, Test: 0.6020\n",
      "Epoch: 913,train_loss:0.1653201, Train: 1.0000, Val: 0.6240, Test: 0.6060\n",
      "Epoch: 914,train_loss:0.1769017, Train: 1.0000, Val: 0.6200, Test: 0.5990\n",
      "Epoch: 915,train_loss:0.1953020, Train: 1.0000, Val: 0.6100, Test: 0.5900\n",
      "Epoch: 916,train_loss:0.2128935, Train: 1.0000, Val: 0.6180, Test: 0.5800\n",
      "Epoch: 917,train_loss:0.1249808, Train: 1.0000, Val: 0.6140, Test: 0.5700\n",
      "Epoch: 918,train_loss:0.2466626, Train: 1.0000, Val: 0.5940, Test: 0.5630\n",
      "Epoch: 919,train_loss:0.2846050, Train: 1.0000, Val: 0.6020, Test: 0.5700\n",
      "Epoch: 920,train_loss:0.1462786, Train: 1.0000, Val: 0.6200, Test: 0.5960\n",
      "Epoch: 921,train_loss:0.1880812, Train: 1.0000, Val: 0.6180, Test: 0.5980\n",
      "Epoch: 922,train_loss:0.1555246, Train: 1.0000, Val: 0.6220, Test: 0.6080\n",
      "Epoch: 923,train_loss:0.1751338, Train: 1.0000, Val: 0.6220, Test: 0.6030\n",
      "Epoch: 924,train_loss:0.1086936, Train: 1.0000, Val: 0.6140, Test: 0.6040\n",
      "Epoch: 925,train_loss:0.2557944, Train: 1.0000, Val: 0.6100, Test: 0.6020\n",
      "Epoch: 926,train_loss:0.2693242, Train: 1.0000, Val: 0.6120, Test: 0.6010\n",
      "Epoch: 927,train_loss:0.1882146, Train: 1.0000, Val: 0.6120, Test: 0.5930\n",
      "Epoch: 928,train_loss:0.3280299, Train: 1.0000, Val: 0.6120, Test: 0.5990\n",
      "Epoch: 929,train_loss:0.1981526, Train: 1.0000, Val: 0.6040, Test: 0.5940\n",
      "Epoch: 930,train_loss:0.2266262, Train: 1.0000, Val: 0.6060, Test: 0.5950\n",
      "Epoch: 931,train_loss:0.1506980, Train: 1.0000, Val: 0.6000, Test: 0.5850\n",
      "Epoch: 932,train_loss:0.2020212, Train: 1.0000, Val: 0.6060, Test: 0.5920\n",
      "Epoch: 933,train_loss:0.1313085, Train: 1.0000, Val: 0.6080, Test: 0.6050\n",
      "Epoch: 934,train_loss:0.1216509, Train: 1.0000, Val: 0.6160, Test: 0.5990\n",
      "Epoch: 935,train_loss:0.1853949, Train: 1.0000, Val: 0.6160, Test: 0.6040\n",
      "Epoch: 936,train_loss:0.2763492, Train: 1.0000, Val: 0.6040, Test: 0.5860\n",
      "Epoch: 937,train_loss:0.2801144, Train: 1.0000, Val: 0.5660, Test: 0.5560\n",
      "Epoch: 938,train_loss:0.1689227, Train: 1.0000, Val: 0.5520, Test: 0.5520\n",
      "Epoch: 939,train_loss:0.1572359, Train: 1.0000, Val: 0.5860, Test: 0.5750\n",
      "Epoch: 940,train_loss:0.2122822, Train: 1.0000, Val: 0.6120, Test: 0.5950\n",
      "Epoch: 941,train_loss:0.1990798, Train: 1.0000, Val: 0.6120, Test: 0.5980\n",
      "Epoch: 942,train_loss:0.2590850, Train: 1.0000, Val: 0.6200, Test: 0.6110\n",
      "Epoch: 943,train_loss:0.1511071, Train: 1.0000, Val: 0.6200, Test: 0.6120\n",
      "Epoch: 944,train_loss:0.1880734, Train: 1.0000, Val: 0.6220, Test: 0.6100\n",
      "Epoch: 945,train_loss:0.2330659, Train: 1.0000, Val: 0.6220, Test: 0.6160\n",
      "Epoch: 946,train_loss:0.2792343, Train: 1.0000, Val: 0.6260, Test: 0.6220\n",
      "Epoch: 947,train_loss:0.1538407, Train: 1.0000, Val: 0.6240, Test: 0.6240\n",
      "Epoch: 948,train_loss:0.1665970, Train: 1.0000, Val: 0.6180, Test: 0.6210\n",
      "Epoch: 949,train_loss:0.2164417, Train: 1.0000, Val: 0.6200, Test: 0.6110\n",
      "Epoch: 950,train_loss:0.1716558, Train: 1.0000, Val: 0.6040, Test: 0.6020\n",
      "Epoch: 951,train_loss:0.2495352, Train: 1.0000, Val: 0.5940, Test: 0.6010\n",
      "Epoch: 952,train_loss:0.2523742, Train: 1.0000, Val: 0.6040, Test: 0.6060\n",
      "Epoch: 953,train_loss:0.1146980, Train: 1.0000, Val: 0.5880, Test: 0.5890\n",
      "Epoch: 954,train_loss:0.2036441, Train: 1.0000, Val: 0.5780, Test: 0.5840\n",
      "Epoch: 955,train_loss:0.0816659, Train: 1.0000, Val: 0.5900, Test: 0.5900\n",
      "Epoch: 956,train_loss:0.2161162, Train: 1.0000, Val: 0.6100, Test: 0.5960\n",
      "Epoch: 957,train_loss:0.2362694, Train: 1.0000, Val: 0.6200, Test: 0.6120\n",
      "Epoch: 958,train_loss:0.2305035, Train: 1.0000, Val: 0.6100, Test: 0.5990\n",
      "Epoch: 959,train_loss:0.1611413, Train: 1.0000, Val: 0.6100, Test: 0.5890\n",
      "Epoch: 960,train_loss:0.1740200, Train: 1.0000, Val: 0.5900, Test: 0.5810\n",
      "Epoch: 961,train_loss:0.2103579, Train: 1.0000, Val: 0.5800, Test: 0.5970\n",
      "Epoch: 962,train_loss:0.2345811, Train: 1.0000, Val: 0.5820, Test: 0.5990\n",
      "Epoch: 963,train_loss:0.1060008, Train: 1.0000, Val: 0.5820, Test: 0.5840\n",
      "Epoch: 964,train_loss:0.1650392, Train: 1.0000, Val: 0.5780, Test: 0.5700\n",
      "Epoch: 965,train_loss:0.1681982, Train: 1.0000, Val: 0.5780, Test: 0.5800\n",
      "Epoch: 966,train_loss:0.2415402, Train: 1.0000, Val: 0.5860, Test: 0.5880\n",
      "Epoch: 967,train_loss:0.2745115, Train: 1.0000, Val: 0.6180, Test: 0.6020\n",
      "Epoch: 968,train_loss:0.2728278, Train: 1.0000, Val: 0.6100, Test: 0.6080\n",
      "Epoch: 969,train_loss:0.2165257, Train: 1.0000, Val: 0.5840, Test: 0.5850\n",
      "Epoch: 970,train_loss:0.2231371, Train: 1.0000, Val: 0.5720, Test: 0.5650\n",
      "Epoch: 971,train_loss:0.1928773, Train: 1.0000, Val: 0.5620, Test: 0.5660\n",
      "Epoch: 972,train_loss:0.1921536, Train: 1.0000, Val: 0.5740, Test: 0.5860\n",
      "Epoch: 973,train_loss:0.1633499, Train: 1.0000, Val: 0.5920, Test: 0.6040\n",
      "Epoch: 974,train_loss:0.2434317, Train: 1.0000, Val: 0.5940, Test: 0.5980\n",
      "Epoch: 975,train_loss:0.2128725, Train: 1.0000, Val: 0.5860, Test: 0.5930\n",
      "Epoch: 976,train_loss:0.1803017, Train: 1.0000, Val: 0.5780, Test: 0.6000\n",
      "Epoch: 977,train_loss:0.2891944, Train: 1.0000, Val: 0.5960, Test: 0.6120\n",
      "Epoch: 978,train_loss:0.1334861, Train: 1.0000, Val: 0.6120, Test: 0.6270\n",
      "Epoch: 979,train_loss:0.1486775, Train: 1.0000, Val: 0.6260, Test: 0.6170\n",
      "Epoch: 980,train_loss:0.2290630, Train: 1.0000, Val: 0.6220, Test: 0.6110\n",
      "Epoch: 981,train_loss:0.1690562, Train: 1.0000, Val: 0.6160, Test: 0.5970\n",
      "Epoch: 982,train_loss:0.1423037, Train: 1.0000, Val: 0.6040, Test: 0.5980\n",
      "Epoch: 983,train_loss:0.2569373, Train: 1.0000, Val: 0.6020, Test: 0.6080\n",
      "Epoch: 984,train_loss:0.1012984, Train: 1.0000, Val: 0.5960, Test: 0.6170\n",
      "Epoch: 985,train_loss:0.2106341, Train: 1.0000, Val: 0.5940, Test: 0.6170\n",
      "Epoch: 986,train_loss:0.2085717, Train: 1.0000, Val: 0.6040, Test: 0.6140\n",
      "Epoch: 987,train_loss:0.1906267, Train: 1.0000, Val: 0.6120, Test: 0.6130\n",
      "Epoch: 988,train_loss:0.2895022, Train: 1.0000, Val: 0.6180, Test: 0.6160\n",
      "Epoch: 989,train_loss:0.3075648, Train: 1.0000, Val: 0.6120, Test: 0.6120\n",
      "Epoch: 990,train_loss:0.1515796, Train: 1.0000, Val: 0.6100, Test: 0.6210\n",
      "Epoch: 991,train_loss:0.2588817, Train: 1.0000, Val: 0.6120, Test: 0.6170\n",
      "Epoch: 992,train_loss:0.2375860, Train: 1.0000, Val: 0.6300, Test: 0.6140\n",
      "Epoch: 993,train_loss:0.1345582, Train: 1.0000, Val: 0.6320, Test: 0.6120\n",
      "Epoch: 994,train_loss:0.2621080, Train: 1.0000, Val: 0.6140, Test: 0.6040\n",
      "Epoch: 995,train_loss:0.2391218, Train: 1.0000, Val: 0.5980, Test: 0.5940\n",
      "Epoch: 996,train_loss:0.2283990, Train: 1.0000, Val: 0.5880, Test: 0.5840\n",
      "Epoch: 997,train_loss:0.2300951, Train: 1.0000, Val: 0.5940, Test: 0.5820\n",
      "Epoch: 998,train_loss:0.2538833, Train: 1.0000, Val: 0.6080, Test: 0.5830\n",
      "Epoch: 999,train_loss:0.1521887, Train: 1.0000, Val: 0.6000, Test: 0.5870\n",
      "Epoch: 1000,train_loss:0.2698490, Train: 1.0000, Val: 0.5840, Test: 0.5720\n",
      "CPU times: user 20.1 s, sys: 1 s, total: 21.1 s\n",
      "Wall time: 18.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "622726"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv,AGNNConv\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='val')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset = 'Citeseer'\n",
    "path = osp.join('./', '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# dim = dataset.num_features\n",
    "# lstm_hidden = dataset.num_features\n",
    "dim = 128\n",
    "lstm_hidden = 128\n",
    "layer_num = 1  #pubmed3cora2,Citeseer1\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        # self.gatconv = AGNNConv(requires_grad=True)\n",
    "        self.gatconv = GATConv(in_dim, out_dim,dropout=0.4, heads=1)#in_dimout_dim=dim=256\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x=F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "# model = kwargs[args.model](train_dataset.num_features,train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = GeniePath(dataset.num_features,dataset.num_classes).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "    loss = F.nll_loss(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    # loss = loss_op(model(data.x, data.edge_index)[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        a=logits[mask]\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "losslist_Citeseer_geniepath,testacclist_Citeseer_geniepath=[],[]\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    losslist_Citeseer_geniepath.append(loss)\n",
    "    testacclist_Citeseer_geniepath.append(test()[2])\n",
    "    # val_f1 = test(val_loader)\n",
    "    # test_f1 = test(test_loader)\n",
    "    # print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "    #     epoch, loss, val_f1, test_f1))\n",
    "    log = 'Epoch: {:03d},train_loss:{:.7f}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, loss,*test()))\n",
    "# from matplotlib import pyplot as plt \n",
    "# %matplotlib inline\n",
    "# f, ax = plt.subplots(1,2)\n",
    "\n",
    "# ax[0][0].plot(losslist_cora_geniepath,label=\"losslist_cora_geniepath\")\n",
    "# ax[0][1].plot(testacclist_cora_geniepath,label=\"testacclist_cora_geniepath\")\n",
    "# plt.legend(loc=0, ncol=1) \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1000Citeseer\n",
    "Epoch: 987,train_loss:0.1055440, Train: 1.0000, Val: 0.6000, Test: 0.6270\n",
    "Epoch: 988,train_loss:0.2827685, Train: 1.0000, Val: 0.6100, Test: 0.6190\n",
    "Epoch: 989,train_loss:0.1982046, Train: 1.0000, Val: 0.6020, Test: 0.6260\n",
    "Epoch: 990,train_loss:0.1708948, Train: 1.0000, Val: 0.6120, Test: 0.6150\n",
    "Epoch: 991,train_loss:0.2397249, Train: 1.0000, Val: 0.6000, Test: 0.6070\n",
    "Epoch: 992,train_loss:0.2047505, Train: 1.0000, Val: 0.6000, Test: 0.5970\n",
    "Epoch: 993,train_loss:0.1261923, Train: 1.0000, Val: 0.5800, Test: 0.5860\n",
    "Epoch: 994,train_loss:0.2088980, Train: 1.0000, Val: 0.5820, Test: 0.5920\n",
    "Epoch: 995,train_loss:0.1436900, Train: 1.0000, Val: 0.5940, Test: 0.5970\n",
    "Epoch: 996,train_loss:0.2564604, Train: 1.0000, Val: 0.6160, Test: 0.6130\n",
    "Epoch: 997,train_loss:0.1209909, Train: 1.0000, Val: 0.6340, Test: 0.6390\n",
    "Epoch: 998,train_loss:0.2647962, Train: 1.0000, Val: 0.6320, Test: 0.6460\n",
    "Epoch: 999,train_loss:0.1605675, Train: 1.0000, Val: 0.6340, Test: 0.6450\n",
    "Epoch: 1000,train_loss:0.1401642, Train: 1.0000, Val: 0.6240, Test: 0.6340\n",
    "CPU times: user 20.9 s, sys: 967 ms, total: 21.9 s\n",
    "Wall time: 19.1 s\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sum([torch.numel(param) for param in model.parameters()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "f, ax = plt.subplots(1,2, figsize=(10,5), squeeze=False)\n",
    "\n",
    "ax[0][0].plot(losslist_Citeseer_mymodel,label=\"losslist_Citeseer_mymodel\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_Citeseer_mymodel,label=\"testacclist_Citeseer_mymodel\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "ax[0][0].plot(losslist_Citeseer_geniepath,label=\"losslist_pubmed_geniepath\")\n",
    "ax[0][0].legend(loc=0, ncol=1) \n",
    "ax[0][1].plot(testacclist_Citeseer_geniepath,label=\"testacclist_Citeseer_geniepath\")\n",
    "ax[0][1].legend(loc=0, ncol=1) \n",
    "# plt.savefig(\"gen_sgenCiteseer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
