{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppigeniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5654, Val: 0.4738, Test: 0.4750\n",
      "Epoch: 02, Loss: 0.5252, Val: 0.5341, Test: 0.5384\n",
      "Epoch: 03, Loss: 0.5121, Val: 0.5104, Test: 0.5169\n",
      "Epoch: 04, Loss: 0.4911, Val: 0.5489, Test: 0.5567\n",
      "Epoch: 05, Loss: 0.4697, Val: 0.5833, Test: 0.5936\n",
      "Epoch: 06, Loss: 0.4424, Val: 0.6215, Test: 0.6351\n",
      "Epoch: 07, Loss: 0.4154, Val: 0.6511, Test: 0.6691\n",
      "Epoch: 08, Loss: 0.3841, Val: 0.6382, Test: 0.6561\n",
      "Epoch: 09, Loss: 0.3650, Val: 0.6866, Test: 0.7067\n",
      "Epoch: 10, Loss: 0.3293, Val: 0.7163, Test: 0.7384\n",
      "Epoch: 11, Loss: 0.2928, Val: 0.7477, Test: 0.7719\n",
      "Epoch: 12, Loss: 0.2829, Val: 0.7603, Test: 0.7806\n",
      "Epoch: 13, Loss: 0.2603, Val: 0.8044, Test: 0.8272\n",
      "Epoch: 14, Loss: 0.2238, Val: 0.8225, Test: 0.8452\n",
      "Epoch: 15, Loss: 0.2050, Val: 0.8291, Test: 0.8524\n",
      "Epoch: 16, Loss: 0.1960, Val: 0.8458, Test: 0.8689\n",
      "Epoch: 17, Loss: 0.1966, Val: 0.8306, Test: 0.8567\n",
      "Epoch: 18, Loss: 0.1870, Val: 0.8486, Test: 0.8697\n",
      "Epoch: 19, Loss: 0.1956, Val: 0.8590, Test: 0.8820\n",
      "Epoch: 20, Loss: 0.1762, Val: 0.8398, Test: 0.8618\n",
      "Epoch: 21, Loss: 0.1799, Val: 0.8534, Test: 0.8752\n",
      "Epoch: 22, Loss: 0.1783, Val: 0.8653, Test: 0.8877\n",
      "Epoch: 23, Loss: 0.1549, Val: 0.8782, Test: 0.8987\n",
      "Epoch: 24, Loss: 0.1494, Val: 0.8877, Test: 0.9059\n",
      "Epoch: 25, Loss: 0.1333, Val: 0.8981, Test: 0.9191\n",
      "Epoch: 26, Loss: 0.1202, Val: 0.9047, Test: 0.9240\n",
      "Epoch: 27, Loss: 0.1164, Val: 0.8969, Test: 0.9181\n",
      "Epoch: 28, Loss: 0.1298, Val: 0.8848, Test: 0.9076\n",
      "Epoch: 29, Loss: 0.1419, Val: 0.8787, Test: 0.9007\n",
      "Epoch: 30, Loss: 0.1392, Val: 0.8875, Test: 0.9107\n",
      "Epoch: 31, Loss: 0.1258, Val: 0.8926, Test: 0.9148\n",
      "Epoch: 32, Loss: 0.1150, Val: 0.9113, Test: 0.9299\n",
      "Epoch: 33, Loss: 0.1210, Val: 0.8955, Test: 0.9156\n",
      "Epoch: 34, Loss: 0.1224, Val: 0.8964, Test: 0.9176\n",
      "Epoch: 35, Loss: 0.1167, Val: 0.8995, Test: 0.9185\n",
      "Epoch: 36, Loss: 0.1096, Val: 0.9062, Test: 0.9247\n",
      "Epoch: 37, Loss: 0.1117, Val: 0.8910, Test: 0.9090\n",
      "Epoch: 38, Loss: 0.1195, Val: 0.8981, Test: 0.9159\n",
      "Epoch: 39, Loss: 0.1330, Val: 0.8801, Test: 0.8964\n",
      "Epoch: 40, Loss: 0.1478, Val: 0.8838, Test: 0.9045\n",
      "Epoch: 41, Loss: 0.1248, Val: 0.9005, Test: 0.9204\n",
      "Epoch: 42, Loss: 0.1147, Val: 0.9018, Test: 0.9207\n",
      "Epoch: 43, Loss: 0.1194, Val: 0.9043, Test: 0.9229\n",
      "Epoch: 44, Loss: 0.1113, Val: 0.8952, Test: 0.9146\n",
      "Epoch: 45, Loss: 0.1185, Val: 0.9054, Test: 0.9230\n",
      "Epoch: 46, Loss: 0.1091, Val: 0.9120, Test: 0.9300\n",
      "Epoch: 47, Loss: 0.1049, Val: 0.9072, Test: 0.9261\n",
      "Epoch: 48, Loss: 0.1079, Val: 0.9059, Test: 0.9253\n",
      "Epoch: 49, Loss: 0.1053, Val: 0.9143, Test: 0.9326\n",
      "Epoch: 50, Loss: 0.1107, Val: 0.8989, Test: 0.9145\n",
      "Epoch: 51, Loss: 0.1163, Val: 0.9125, Test: 0.9292\n",
      "Epoch: 52, Loss: 0.0988, Val: 0.9165, Test: 0.9351\n",
      "Epoch: 53, Loss: 0.0934, Val: 0.9129, Test: 0.9307\n",
      "Epoch: 54, Loss: 0.1127, Val: 0.8921, Test: 0.9087\n",
      "Epoch: 55, Loss: 0.1329, Val: 0.8852, Test: 0.9049\n",
      "Epoch: 56, Loss: 0.1351, Val: 0.8882, Test: 0.9059\n",
      "Epoch: 57, Loss: 0.1271, Val: 0.8913, Test: 0.9107\n",
      "Epoch: 58, Loss: 0.1219, Val: 0.8997, Test: 0.9195\n",
      "Epoch: 59, Loss: 0.1108, Val: 0.9097, Test: 0.9276\n",
      "Epoch: 60, Loss: 0.1026, Val: 0.9088, Test: 0.9261\n",
      "Epoch: 61, Loss: 0.0946, Val: 0.9160, Test: 0.9328\n",
      "Epoch: 62, Loss: 0.1062, Val: 0.9053, Test: 0.9248\n",
      "Epoch: 63, Loss: 0.1070, Val: 0.9034, Test: 0.9215\n",
      "Epoch: 64, Loss: 0.1120, Val: 0.9019, Test: 0.9215\n",
      "Epoch: 65, Loss: 0.1102, Val: 0.9111, Test: 0.9273\n",
      "Epoch: 66, Loss: 0.1143, Val: 0.8828, Test: 0.9025\n",
      "Epoch: 67, Loss: 0.1307, Val: 0.8888, Test: 0.9059\n",
      "Epoch: 68, Loss: 0.1349, Val: 0.8814, Test: 0.9028\n",
      "Epoch: 69, Loss: 0.1231, Val: 0.9075, Test: 0.9248\n",
      "Epoch: 70, Loss: 0.1019, Val: 0.9164, Test: 0.9344\n",
      "Epoch: 71, Loss: 0.0869, Val: 0.9227, Test: 0.9386\n",
      "Epoch: 72, Loss: 0.0874, Val: 0.9194, Test: 0.9369\n",
      "Epoch: 73, Loss: 0.0867, Val: 0.9153, Test: 0.9340\n",
      "Epoch: 74, Loss: 0.0913, Val: 0.9200, Test: 0.9384\n",
      "Epoch: 75, Loss: 0.0886, Val: 0.9216, Test: 0.9387\n",
      "Epoch: 76, Loss: 0.0913, Val: 0.9197, Test: 0.9367\n",
      "Epoch: 77, Loss: 0.0820, Val: 0.9286, Test: 0.9448\n",
      "Epoch: 78, Loss: 0.0729, Val: 0.9316, Test: 0.9471\n",
      "Epoch: 79, Loss: 0.0702, Val: 0.9335, Test: 0.9501\n",
      "Epoch: 80, Loss: 0.0667, Val: 0.9368, Test: 0.9532\n",
      "Epoch: 81, Loss: 0.0659, Val: 0.9345, Test: 0.9497\n",
      "Epoch: 82, Loss: 0.0792, Val: 0.9144, Test: 0.9330\n",
      "Epoch: 83, Loss: 0.0991, Val: 0.9067, Test: 0.9221\n",
      "Epoch: 84, Loss: 0.1181, Val: 0.9003, Test: 0.9192\n",
      "Epoch: 85, Loss: 0.1189, Val: 0.9004, Test: 0.9196\n",
      "Epoch: 86, Loss: 0.1322, Val: 0.8941, Test: 0.9133\n",
      "Epoch: 87, Loss: 0.1217, Val: 0.8910, Test: 0.9114\n",
      "Epoch: 88, Loss: 0.1103, Val: 0.9120, Test: 0.9290\n",
      "Epoch: 89, Loss: 0.0931, Val: 0.9198, Test: 0.9354\n",
      "Epoch: 90, Loss: 0.0865, Val: 0.9214, Test: 0.9379\n",
      "Epoch: 91, Loss: 0.0829, Val: 0.9218, Test: 0.9369\n",
      "Epoch: 92, Loss: 0.0987, Val: 0.8895, Test: 0.9055\n",
      "Epoch: 93, Loss: 0.1434, Val: 0.8733, Test: 0.8940\n",
      "Epoch: 94, Loss: 0.1527, Val: 0.8222, Test: 0.8449\n",
      "Epoch: 95, Loss: 0.2356, Val: 0.8372, Test: 0.8577\n",
      "Epoch: 96, Loss: 0.2391, Val: 0.7930, Test: 0.8127\n",
      "Epoch: 97, Loss: 0.2456, Val: 0.8218, Test: 0.8426\n",
      "Epoch: 98, Loss: 0.2101, Val: 0.8497, Test: 0.8696\n",
      "Epoch: 99, Loss: 0.1800, Val: 0.8557, Test: 0.8764\n",
      "Epoch: 100, Loss: 0.1664, Val: 0.8685, Test: 0.8881\n",
      "CPU times: user 46min 49s, sys: 3min 52s, total: 50min 41s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run ./examples/geniepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgeniepath v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5771, Val: 0.4285, Test: 0.4302\n",
      "Epoch: 02, Loss: 0.5351, Val: 0.5369, Test: 0.5420\n",
      "Epoch: 03, Loss: 0.5103, Val: 0.5219, Test: 0.5301\n",
      "Epoch: 04, Loss: 0.4663, Val: 0.6091, Test: 0.6199\n",
      "Epoch: 05, Loss: 0.4234, Val: 0.6461, Test: 0.6605\n",
      "Epoch: 06, Loss: 0.3701, Val: 0.7052, Test: 0.7213\n",
      "Epoch: 07, Loss: 0.3181, Val: 0.7397, Test: 0.7589\n",
      "Epoch: 08, Loss: 0.2631, Val: 0.8044, Test: 0.8243\n",
      "Epoch: 09, Loss: 0.2118, Val: 0.8390, Test: 0.8605\n",
      "Epoch: 10, Loss: 0.1693, Val: 0.8770, Test: 0.8967\n",
      "Epoch: 11, Loss: 0.1351, Val: 0.8980, Test: 0.9176\n",
      "Epoch: 12, Loss: 0.1087, Val: 0.9171, Test: 0.9364\n",
      "Epoch: 13, Loss: 0.0885, Val: 0.9284, Test: 0.9466\n",
      "Epoch: 14, Loss: 0.0747, Val: 0.9383, Test: 0.9557\n",
      "Epoch: 15, Loss: 0.0627, Val: 0.9445, Test: 0.9599\n",
      "Epoch: 16, Loss: 0.0552, Val: 0.9497, Test: 0.9645\n",
      "Epoch: 17, Loss: 0.0488, Val: 0.9538, Test: 0.9680\n",
      "Epoch: 18, Loss: 0.0445, Val: 0.9539, Test: 0.9681\n",
      "Epoch: 19, Loss: 0.0402, Val: 0.9572, Test: 0.9708\n",
      "Epoch: 20, Loss: 0.0370, Val: 0.9595, Test: 0.9722\n",
      "Epoch: 21, Loss: 0.0343, Val: 0.9610, Test: 0.9734\n",
      "Epoch: 22, Loss: 0.0320, Val: 0.9614, Test: 0.9737\n",
      "Epoch: 23, Loss: 0.0302, Val: 0.9624, Test: 0.9744\n",
      "Epoch: 24, Loss: 0.0286, Val: 0.9632, Test: 0.9748\n",
      "Epoch: 25, Loss: 0.0279, Val: 0.9636, Test: 0.9756\n",
      "Epoch: 26, Loss: 0.0268, Val: 0.9646, Test: 0.9756\n",
      "Epoch: 27, Loss: 0.0257, Val: 0.9643, Test: 0.9756\n",
      "Epoch: 28, Loss: 0.0248, Val: 0.9652, Test: 0.9762\n",
      "Epoch: 29, Loss: 0.0241, Val: 0.9646, Test: 0.9763\n",
      "Epoch: 30, Loss: 0.0234, Val: 0.9654, Test: 0.9766\n",
      "Epoch: 31, Loss: 0.0231, Val: 0.9650, Test: 0.9764\n",
      "Epoch: 32, Loss: 0.0228, Val: 0.9652, Test: 0.9764\n",
      "Epoch: 33, Loss: 0.0227, Val: 0.9655, Test: 0.9768\n",
      "Epoch: 34, Loss: 0.0222, Val: 0.9657, Test: 0.9769\n",
      "Epoch: 35, Loss: 0.0218, Val: 0.9655, Test: 0.9766\n",
      "Epoch: 36, Loss: 0.0216, Val: 0.9656, Test: 0.9769\n",
      "Epoch: 37, Loss: 0.0212, Val: 0.9656, Test: 0.9765\n",
      "Epoch: 38, Loss: 0.0210, Val: 0.9659, Test: 0.9769\n",
      "Epoch: 39, Loss: 0.0207, Val: 0.9661, Test: 0.9773\n",
      "Epoch: 40, Loss: 0.0203, Val: 0.9663, Test: 0.9770\n",
      "Epoch: 41, Loss: 0.0201, Val: 0.9660, Test: 0.9774\n",
      "Epoch: 42, Loss: 0.0208, Val: 0.9658, Test: 0.9772\n",
      "Epoch: 43, Loss: 0.0211, Val: 0.9652, Test: 0.9767\n",
      "Epoch: 44, Loss: 0.0217, Val: 0.9652, Test: 0.9760\n",
      "Epoch: 45, Loss: 0.0229, Val: 0.9642, Test: 0.9751\n",
      "Epoch: 46, Loss: 0.0255, Val: 0.9584, Test: 0.9721\n",
      "Epoch: 47, Loss: 0.0342, Val: 0.9513, Test: 0.9651\n",
      "Epoch: 48, Loss: 0.0454, Val: 0.9447, Test: 0.9595\n",
      "Epoch: 49, Loss: 0.0553, Val: 0.9319, Test: 0.9507\n",
      "Epoch: 50, Loss: 0.0583, Val: 0.9416, Test: 0.9593\n",
      "Epoch: 51, Loss: 0.0452, Val: 0.9535, Test: 0.9669\n",
      "Epoch: 52, Loss: 0.0342, Val: 0.9590, Test: 0.9723\n",
      "Epoch: 53, Loss: 0.0279, Val: 0.9637, Test: 0.9747\n",
      "Epoch: 54, Loss: 0.0238, Val: 0.9654, Test: 0.9761\n",
      "Epoch: 55, Loss: 0.0217, Val: 0.9659, Test: 0.9770\n",
      "Epoch: 56, Loss: 0.0204, Val: 0.9669, Test: 0.9781\n",
      "Epoch: 57, Loss: 0.0198, Val: 0.9672, Test: 0.9780\n",
      "Epoch: 58, Loss: 0.0192, Val: 0.9668, Test: 0.9776\n",
      "Epoch: 59, Loss: 0.0191, Val: 0.9672, Test: 0.9780\n",
      "Epoch: 60, Loss: 0.0188, Val: 0.9675, Test: 0.9780\n",
      "Epoch: 61, Loss: 0.0187, Val: 0.9675, Test: 0.9781\n",
      "Epoch: 62, Loss: 0.0186, Val: 0.9676, Test: 0.9782\n",
      "Epoch: 63, Loss: 0.0184, Val: 0.9680, Test: 0.9784\n",
      "Epoch: 64, Loss: 0.0181, Val: 0.9677, Test: 0.9784\n",
      "Epoch: 65, Loss: 0.0180, Val: 0.9680, Test: 0.9783\n",
      "Epoch: 66, Loss: 0.0178, Val: 0.9682, Test: 0.9786\n",
      "Epoch: 67, Loss: 0.0178, Val: 0.9679, Test: 0.9783\n",
      "Epoch: 68, Loss: 0.0176, Val: 0.9680, Test: 0.9785\n",
      "Epoch: 69, Loss: 0.0176, Val: 0.9681, Test: 0.9783\n",
      "Epoch: 70, Loss: 0.0175, Val: 0.9683, Test: 0.9786\n",
      "Epoch: 71, Loss: 0.0174, Val: 0.9681, Test: 0.9787\n",
      "Epoch: 72, Loss: 0.0175, Val: 0.9677, Test: 0.9784\n",
      "Epoch: 73, Loss: 0.0176, Val: 0.9677, Test: 0.9784\n",
      "Epoch: 74, Loss: 0.0174, Val: 0.9680, Test: 0.9785\n",
      "Epoch: 75, Loss: 0.0173, Val: 0.9678, Test: 0.9783\n",
      "Epoch: 76, Loss: 0.0174, Val: 0.9680, Test: 0.9784\n",
      "Epoch: 77, Loss: 0.0174, Val: 0.9683, Test: 0.9786\n",
      "Epoch: 78, Loss: 0.0173, Val: 0.9680, Test: 0.9786\n",
      "Epoch: 79, Loss: 0.0171, Val: 0.9680, Test: 0.9786\n",
      "Epoch: 80, Loss: 0.0171, Val: 0.9676, Test: 0.9785\n",
      "Epoch: 81, Loss: 0.0170, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 82, Loss: 0.0170, Val: 0.9682, Test: 0.9788\n",
      "Epoch: 83, Loss: 0.0171, Val: 0.9680, Test: 0.9786\n",
      "Epoch: 84, Loss: 0.0169, Val: 0.9680, Test: 0.9785\n",
      "Epoch: 85, Loss: 0.0169, Val: 0.9677, Test: 0.9787\n",
      "Epoch: 86, Loss: 0.0170, Val: 0.9680, Test: 0.9785\n",
      "Epoch: 87, Loss: 0.0169, Val: 0.9679, Test: 0.9785\n",
      "Epoch: 88, Loss: 0.0168, Val: 0.9680, Test: 0.9787\n",
      "Epoch: 89, Loss: 0.0168, Val: 0.9680, Test: 0.9784\n",
      "Epoch: 90, Loss: 0.0168, Val: 0.9682, Test: 0.9784\n",
      "Epoch: 91, Loss: 0.0168, Val: 0.9678, Test: 0.9788\n",
      "Epoch: 92, Loss: 0.0169, Val: 0.9683, Test: 0.9789\n",
      "Epoch: 93, Loss: 0.0167, Val: 0.9681, Test: 0.9785\n",
      "Epoch: 94, Loss: 0.0169, Val: 0.9678, Test: 0.9783\n",
      "Epoch: 95, Loss: 0.0170, Val: 0.9678, Test: 0.9784\n",
      "Epoch: 96, Loss: 0.0169, Val: 0.9682, Test: 0.9787\n",
      "Epoch: 97, Loss: 0.0170, Val: 0.9679, Test: 0.9785\n",
      "Epoch: 98, Loss: 0.0169, Val: 0.9679, Test: 0.9784\n",
      "Epoch: 99, Loss: 0.0169, Val: 0.9678, Test: 0.9782\n",
      "Epoch: 100, Loss: 0.0168, Val: 0.9678, Test: 0.9787\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 5s, sys: 3min 9s, total: 42min 14s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "class agnnn(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(agnnn, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.dropout(data.x, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.prop1(x, edge_index)\n",
    "        x = self.prop2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist=[]\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[1,2,3,4]\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgeniepath v2 不加dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5754, Val: 0.3693, Test: 0.3691\n",
      "Epoch: 02, Loss: 0.5515, Val: 0.4225, Test: 0.4259\n",
      "Epoch: 03, Loss: 0.5340, Val: 0.4705, Test: 0.4771\n",
      "Epoch: 04, Loss: 0.5181, Val: 0.4762, Test: 0.4842\n",
      "Epoch: 05, Loss: 0.5011, Val: 0.5297, Test: 0.5377\n",
      "Epoch: 06, Loss: 0.4807, Val: 0.5737, Test: 0.5830\n",
      "Epoch: 07, Loss: 0.4634, Val: 0.5210, Test: 0.5364\n",
      "Epoch: 08, Loss: 0.4387, Val: 0.5965, Test: 0.6092\n",
      "Epoch: 09, Loss: 0.4083, Val: 0.6591, Test: 0.6727\n",
      "Epoch: 10, Loss: 0.3888, Val: 0.6698, Test: 0.6854\n",
      "Epoch: 11, Loss: 0.3549, Val: 0.6855, Test: 0.7049\n",
      "Epoch: 12, Loss: 0.3241, Val: 0.7219, Test: 0.7368\n",
      "Epoch: 13, Loss: 0.2899, Val: 0.7478, Test: 0.7628\n",
      "Epoch: 14, Loss: 0.2704, Val: 0.7702, Test: 0.7937\n",
      "Epoch: 15, Loss: 0.2489, Val: 0.7724, Test: 0.7983\n",
      "Epoch: 16, Loss: 0.2345, Val: 0.7863, Test: 0.8124\n",
      "Epoch: 17, Loss: 0.2083, Val: 0.8249, Test: 0.8463\n",
      "Epoch: 18, Loss: 0.1951, Val: 0.8296, Test: 0.8562\n",
      "Epoch: 19, Loss: 0.1877, Val: 0.8346, Test: 0.8581\n",
      "Epoch: 20, Loss: 0.1729, Val: 0.8514, Test: 0.8735\n",
      "Epoch: 21, Loss: 0.1565, Val: 0.8512, Test: 0.8748\n",
      "Epoch: 22, Loss: 0.1478, Val: 0.8680, Test: 0.8928\n",
      "Epoch: 23, Loss: 0.1327, Val: 0.8838, Test: 0.9047\n",
      "Epoch: 24, Loss: 0.1243, Val: 0.8717, Test: 0.8929\n",
      "Epoch: 25, Loss: 0.1232, Val: 0.8857, Test: 0.9058\n",
      "Epoch: 26, Loss: 0.1144, Val: 0.8821, Test: 0.8964\n",
      "Epoch: 27, Loss: 0.1036, Val: 0.9025, Test: 0.9196\n",
      "Epoch: 28, Loss: 0.0990, Val: 0.8979, Test: 0.9160\n",
      "Epoch: 29, Loss: 0.0913, Val: 0.9169, Test: 0.9352\n",
      "Epoch: 30, Loss: 0.0819, Val: 0.9208, Test: 0.9376\n",
      "Epoch: 31, Loss: 0.0786, Val: 0.9064, Test: 0.9301\n",
      "Epoch: 32, Loss: 0.0755, Val: 0.9195, Test: 0.9362\n",
      "Epoch: 33, Loss: 0.0720, Val: 0.9284, Test: 0.9421\n",
      "Epoch: 34, Loss: 0.0667, Val: 0.9300, Test: 0.9440\n",
      "Epoch: 35, Loss: 0.0621, Val: 0.9332, Test: 0.9478\n",
      "Epoch: 36, Loss: 0.0592, Val: 0.9328, Test: 0.9485\n",
      "Epoch: 37, Loss: 0.0523, Val: 0.9327, Test: 0.9505\n",
      "Epoch: 38, Loss: 0.0537, Val: 0.9307, Test: 0.9494\n",
      "Epoch: 39, Loss: 0.0633, Val: 0.9201, Test: 0.9409\n",
      "Epoch: 40, Loss: 0.0684, Val: 0.9160, Test: 0.9334\n",
      "Epoch: 41, Loss: 0.0626, Val: 0.9308, Test: 0.9473\n",
      "Epoch: 42, Loss: 0.0595, Val: 0.9294, Test: 0.9475\n",
      "Epoch: 43, Loss: 0.0511, Val: 0.9379, Test: 0.9526\n",
      "Epoch: 44, Loss: 0.0486, Val: 0.9356, Test: 0.9530\n",
      "Epoch: 45, Loss: 0.0452, Val: 0.9330, Test: 0.9486\n",
      "Epoch: 46, Loss: 0.0494, Val: 0.9326, Test: 0.9471\n",
      "Epoch: 47, Loss: 0.0457, Val: 0.9419, Test: 0.9545\n",
      "Epoch: 48, Loss: 0.0399, Val: 0.9361, Test: 0.9545\n",
      "Epoch: 49, Loss: 0.0453, Val: 0.9378, Test: 0.9563\n",
      "Epoch: 50, Loss: 0.0477, Val: 0.9350, Test: 0.9499\n",
      "Epoch: 51, Loss: 0.0527, Val: 0.9372, Test: 0.9499\n",
      "Epoch: 52, Loss: 0.0393, Val: 0.9497, Test: 0.9619\n",
      "Epoch: 53, Loss: 0.0309, Val: 0.9522, Test: 0.9669\n",
      "Epoch: 54, Loss: 0.0278, Val: 0.9534, Test: 0.9679\n",
      "Epoch: 55, Loss: 0.0240, Val: 0.9564, Test: 0.9668\n",
      "Epoch: 56, Loss: 0.0225, Val: 0.9570, Test: 0.9694\n",
      "Epoch: 57, Loss: 0.0212, Val: 0.9562, Test: 0.9702\n",
      "Epoch: 58, Loss: 0.0191, Val: 0.9600, Test: 0.9719\n",
      "Epoch: 59, Loss: 0.0173, Val: 0.9606, Test: 0.9731\n",
      "Epoch: 60, Loss: 0.0159, Val: 0.9608, Test: 0.9730\n",
      "Epoch: 61, Loss: 0.0157, Val: 0.9554, Test: 0.9684\n",
      "Epoch: 62, Loss: 0.0178, Val: 0.9599, Test: 0.9707\n",
      "Epoch: 63, Loss: 0.0172, Val: 0.9562, Test: 0.9669\n",
      "Epoch: 64, Loss: 0.0188, Val: 0.9475, Test: 0.9542\n",
      "Epoch: 65, Loss: 0.0347, Val: 0.9481, Test: 0.9604\n",
      "Epoch: 66, Loss: 0.0402, Val: 0.9402, Test: 0.9581\n",
      "Epoch: 67, Loss: 0.0436, Val: 0.9308, Test: 0.9462\n",
      "Epoch: 68, Loss: 0.0505, Val: 0.9300, Test: 0.9520\n",
      "Epoch: 69, Loss: 0.0594, Val: 0.9057, Test: 0.9265\n",
      "Epoch: 70, Loss: 0.0778, Val: 0.8941, Test: 0.9130\n",
      "Epoch: 71, Loss: 0.1187, Val: 0.8892, Test: 0.9108\n",
      "Epoch: 72, Loss: 0.1041, Val: 0.8878, Test: 0.9153\n",
      "Epoch: 73, Loss: 0.0975, Val: 0.9066, Test: 0.9263\n",
      "Epoch: 74, Loss: 0.0694, Val: 0.9313, Test: 0.9501\n",
      "Epoch: 75, Loss: 0.0565, Val: 0.9337, Test: 0.9530\n",
      "Epoch: 76, Loss: 0.0377, Val: 0.9465, Test: 0.9611\n",
      "Epoch: 77, Loss: 0.0289, Val: 0.9511, Test: 0.9659\n",
      "Epoch: 78, Loss: 0.0237, Val: 0.9562, Test: 0.9690\n",
      "Epoch: 79, Loss: 0.0204, Val: 0.9589, Test: 0.9717\n",
      "Epoch: 80, Loss: 0.0171, Val: 0.9596, Test: 0.9734\n",
      "Epoch: 81, Loss: 0.0158, Val: 0.9615, Test: 0.9746\n",
      "Epoch: 82, Loss: 0.0139, Val: 0.9629, Test: 0.9736\n",
      "Epoch: 83, Loss: 0.0130, Val: 0.9617, Test: 0.9736\n",
      "Epoch: 84, Loss: 0.0119, Val: 0.9637, Test: 0.9749\n",
      "Epoch: 85, Loss: 0.0112, Val: 0.9643, Test: 0.9753\n",
      "Epoch: 86, Loss: 0.0104, Val: 0.9634, Test: 0.9757\n",
      "Epoch: 87, Loss: 0.0099, Val: 0.9626, Test: 0.9742\n",
      "Epoch: 88, Loss: 0.0100, Val: 0.9635, Test: 0.9755\n",
      "Epoch: 89, Loss: 0.0095, Val: 0.9640, Test: 0.9769\n",
      "Epoch: 90, Loss: 0.0094, Val: 0.9647, Test: 0.9762\n",
      "Epoch: 91, Loss: 0.0088, Val: 0.9644, Test: 0.9759\n",
      "Epoch: 92, Loss: 0.0084, Val: 0.9649, Test: 0.9763\n",
      "Epoch: 93, Loss: 0.0081, Val: 0.9639, Test: 0.9767\n",
      "Epoch: 94, Loss: 0.0080, Val: 0.9644, Test: 0.9766\n",
      "Epoch: 95, Loss: 0.0078, Val: 0.9641, Test: 0.9752\n",
      "Epoch: 96, Loss: 0.0076, Val: 0.9650, Test: 0.9767\n",
      "Epoch: 97, Loss: 0.0085, Val: 0.9618, Test: 0.9745\n",
      "Epoch: 98, Loss: 0.0079, Val: 0.9642, Test: 0.9757\n",
      "Epoch: 99, Loss: 0.0074, Val: 0.9644, Test: 0.9770\n",
      "Epoch: 100, Loss: 0.0073, Val: 0.9647, Test: 0.9765\n",
      "CPU times: user 41min 10s, sys: 3min 13s, total: 44min 24s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "# class agnnn(torch.nn.Module):\n",
    "#     def __init__(self,in_dim,out_dim):\n",
    "#         super(agnnn, self).__init__()\n",
    "#         self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "#         self.prop1 = AGNNConv(requires_grad=False)\n",
    "#         self.prop2 = AGNNConv(requires_grad=True)\n",
    "#         self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x = F.dropout(data.x, training=self.training)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = self.prop1(x, edge_index)\n",
    "#         x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "        # self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.prop1(x, edge_index)\n",
    "        x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        # x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppi_sgeniepathv3_sgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5777, Val: 0.4572, Test: 0.4586\n",
      "Epoch: 02, Loss: 0.5370, Val: 0.4897, Test: 0.4926\n",
      "Epoch: 03, Loss: 0.5182, Val: 0.5154, Test: 0.5222\n",
      "Epoch: 04, Loss: 0.4983, Val: 0.5532, Test: 0.5634\n",
      "Epoch: 05, Loss: 0.4714, Val: 0.5794, Test: 0.5930\n",
      "Epoch: 06, Loss: 0.4415, Val: 0.5730, Test: 0.5897\n",
      "Epoch: 07, Loss: 0.4010, Val: 0.6659, Test: 0.6872\n",
      "Epoch: 08, Loss: 0.3566, Val: 0.7160, Test: 0.7403\n",
      "Epoch: 09, Loss: 0.3040, Val: 0.7634, Test: 0.7876\n",
      "Epoch: 10, Loss: 0.2561, Val: 0.7968, Test: 0.8238\n",
      "Epoch: 11, Loss: 0.2145, Val: 0.8369, Test: 0.8615\n",
      "Epoch: 12, Loss: 0.1761, Val: 0.8632, Test: 0.8869\n",
      "Epoch: 13, Loss: 0.1481, Val: 0.8807, Test: 0.9028\n",
      "Epoch: 14, Loss: 0.1312, Val: 0.8940, Test: 0.9157\n",
      "Epoch: 15, Loss: 0.1132, Val: 0.9081, Test: 0.9314\n",
      "Epoch: 16, Loss: 0.0946, Val: 0.9171, Test: 0.9389\n",
      "Epoch: 17, Loss: 0.0803, Val: 0.9214, Test: 0.9422\n",
      "Epoch: 18, Loss: 0.0749, Val: 0.9303, Test: 0.9495\n",
      "Epoch: 19, Loss: 0.0672, Val: 0.9339, Test: 0.9530\n",
      "Epoch: 20, Loss: 0.0598, Val: 0.9353, Test: 0.9545\n",
      "Epoch: 21, Loss: 0.0547, Val: 0.9390, Test: 0.9579\n",
      "Epoch: 22, Loss: 0.0504, Val: 0.9409, Test: 0.9592\n",
      "Epoch: 23, Loss: 0.0468, Val: 0.9451, Test: 0.9622\n",
      "Epoch: 24, Loss: 0.0430, Val: 0.9476, Test: 0.9633\n",
      "Epoch: 25, Loss: 0.0414, Val: 0.9495, Test: 0.9661\n",
      "Epoch: 26, Loss: 0.0382, Val: 0.9491, Test: 0.9655\n",
      "Epoch: 27, Loss: 0.0365, Val: 0.9498, Test: 0.9661\n",
      "Epoch: 28, Loss: 0.0351, Val: 0.9514, Test: 0.9673\n",
      "Epoch: 29, Loss: 0.0334, Val: 0.9537, Test: 0.9689\n",
      "Epoch: 30, Loss: 0.0327, Val: 0.9515, Test: 0.9675\n",
      "Epoch: 31, Loss: 0.0324, Val: 0.9541, Test: 0.9690\n",
      "Epoch: 32, Loss: 0.0303, Val: 0.9545, Test: 0.9700\n",
      "Epoch: 33, Loss: 0.0287, Val: 0.9553, Test: 0.9700\n",
      "Epoch: 34, Loss: 0.0288, Val: 0.9526, Test: 0.9685\n",
      "Epoch: 35, Loss: 0.0303, Val: 0.9536, Test: 0.9688\n",
      "Epoch: 36, Loss: 0.0283, Val: 0.9528, Test: 0.9678\n",
      "Epoch: 37, Loss: 0.0288, Val: 0.9543, Test: 0.9690\n",
      "Epoch: 38, Loss: 0.0278, Val: 0.9560, Test: 0.9705\n",
      "Epoch: 39, Loss: 0.0268, Val: 0.9569, Test: 0.9719\n",
      "Epoch: 40, Loss: 0.0271, Val: 0.9578, Test: 0.9710\n",
      "Epoch: 41, Loss: 0.0267, Val: 0.9562, Test: 0.9707\n",
      "Epoch: 42, Loss: 0.0268, Val: 0.9558, Test: 0.9703\n",
      "Epoch: 43, Loss: 0.0272, Val: 0.9548, Test: 0.9698\n",
      "Epoch: 44, Loss: 0.0269, Val: 0.9551, Test: 0.9698\n",
      "Epoch: 45, Loss: 0.0270, Val: 0.9559, Test: 0.9707\n",
      "Epoch: 46, Loss: 0.0268, Val: 0.9561, Test: 0.9705\n",
      "Epoch: 47, Loss: 0.0252, Val: 0.9567, Test: 0.9713\n",
      "Epoch: 48, Loss: 0.0249, Val: 0.9562, Test: 0.9708\n",
      "Epoch: 49, Loss: 0.0261, Val: 0.9562, Test: 0.9716\n",
      "Epoch: 50, Loss: 0.0249, Val: 0.9551, Test: 0.9708\n",
      "Epoch: 51, Loss: 0.0237, Val: 0.9590, Test: 0.9734\n",
      "Epoch: 52, Loss: 0.0225, Val: 0.9575, Test: 0.9719\n",
      "Epoch: 53, Loss: 0.0229, Val: 0.9582, Test: 0.9724\n",
      "Epoch: 54, Loss: 0.0219, Val: 0.9575, Test: 0.9721\n",
      "Epoch: 55, Loss: 0.0218, Val: 0.9587, Test: 0.9732\n",
      "Epoch: 56, Loss: 0.0210, Val: 0.9580, Test: 0.9726\n",
      "Epoch: 57, Loss: 0.0210, Val: 0.9591, Test: 0.9730\n",
      "Epoch: 58, Loss: 0.0204, Val: 0.9601, Test: 0.9735\n",
      "Epoch: 59, Loss: 0.0195, Val: 0.9604, Test: 0.9733\n",
      "Epoch: 60, Loss: 0.0197, Val: 0.9606, Test: 0.9741\n",
      "Epoch: 61, Loss: 0.0193, Val: 0.9613, Test: 0.9744\n",
      "Epoch: 62, Loss: 0.0191, Val: 0.9621, Test: 0.9749\n",
      "Epoch: 63, Loss: 0.0187, Val: 0.9589, Test: 0.9721\n",
      "Epoch: 64, Loss: 0.0197, Val: 0.9603, Test: 0.9743\n",
      "Epoch: 65, Loss: 0.0192, Val: 0.9601, Test: 0.9746\n",
      "Epoch: 66, Loss: 0.0193, Val: 0.9605, Test: 0.9741\n",
      "Epoch: 67, Loss: 0.0197, Val: 0.9600, Test: 0.9735\n",
      "Epoch: 68, Loss: 0.0197, Val: 0.9605, Test: 0.9734\n",
      "Epoch: 69, Loss: 0.0195, Val: 0.9609, Test: 0.9740\n",
      "Epoch: 70, Loss: 0.0189, Val: 0.9596, Test: 0.9740\n",
      "Epoch: 71, Loss: 0.0198, Val: 0.9584, Test: 0.9725\n",
      "Epoch: 72, Loss: 0.0213, Val: 0.9551, Test: 0.9696\n",
      "Epoch: 73, Loss: 0.0259, Val: 0.9548, Test: 0.9701\n",
      "Epoch: 74, Loss: 0.0330, Val: 0.9423, Test: 0.9629\n",
      "Epoch: 75, Loss: 0.0592, Val: 0.9254, Test: 0.9453\n",
      "Epoch: 76, Loss: 0.0994, Val: 0.9170, Test: 0.9373\n",
      "Epoch: 77, Loss: 0.1009, Val: 0.9126, Test: 0.9344\n",
      "Epoch: 78, Loss: 0.0765, Val: 0.9236, Test: 0.9448\n",
      "Epoch: 79, Loss: 0.0616, Val: 0.9336, Test: 0.9536\n",
      "Epoch: 80, Loss: 0.0475, Val: 0.9416, Test: 0.9605\n",
      "Epoch: 81, Loss: 0.0379, Val: 0.9491, Test: 0.9657\n",
      "Epoch: 82, Loss: 0.0307, Val: 0.9514, Test: 0.9681\n",
      "Epoch: 83, Loss: 0.0270, Val: 0.9547, Test: 0.9694\n",
      "Epoch: 84, Loss: 0.0242, Val: 0.9569, Test: 0.9715\n",
      "Epoch: 85, Loss: 0.0212, Val: 0.9589, Test: 0.9728\n",
      "Epoch: 86, Loss: 0.0198, Val: 0.9596, Test: 0.9738\n",
      "Epoch: 87, Loss: 0.0191, Val: 0.9603, Test: 0.9738\n",
      "Epoch: 88, Loss: 0.0190, Val: 0.9606, Test: 0.9741\n",
      "Epoch: 89, Loss: 0.0184, Val: 0.9602, Test: 0.9737\n",
      "Epoch: 90, Loss: 0.0185, Val: 0.9604, Test: 0.9741\n",
      "Epoch: 91, Loss: 0.0190, Val: 0.9600, Test: 0.9742\n",
      "Epoch: 92, Loss: 0.0185, Val: 0.9590, Test: 0.9729\n",
      "Epoch: 93, Loss: 0.0187, Val: 0.9600, Test: 0.9736\n",
      "Epoch: 94, Loss: 0.0188, Val: 0.9599, Test: 0.9741\n",
      "Epoch: 95, Loss: 0.0189, Val: 0.9603, Test: 0.9745\n",
      "Epoch: 96, Loss: 0.0176, Val: 0.9608, Test: 0.9742\n",
      "Epoch: 97, Loss: 0.0175, Val: 0.9613, Test: 0.9744\n",
      "Epoch: 98, Loss: 0.0170, Val: 0.9603, Test: 0.9737\n",
      "Epoch: 99, Loss: 0.0194, Val: 0.9586, Test: 0.9725\n",
      "Epoch: 100, Loss: 0.0199, Val: 0.9601, Test: 0.9740\n",
      "CPU times: user 29min 27s, sys: 2min 21s, total: 31min 48s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import SGConv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = SGConv(in_dim, out_dim)#, K=2,cached=True)#这里in_dim和out_dim都=dim=256\n",
    "        # self.gatconv = GATConv(256, 256, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy (train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdafd\n",
      "CPU times: user 316 µs, sys: 0 ns, total: 316 µs\n",
      "Wall time: 253 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"fdafd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
