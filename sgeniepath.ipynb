{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./examples/geniepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgeniepath v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /disk2/zk/.local/share/jupyter/runtime/kernel-a7babdd4-0e58-4479-88c9-b93a67c44227.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk2/zk/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "args = parser.parse_args()\n",
    "assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "class agnnn(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(agnnn, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.dropout(data.x, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.prop1(x, edge_index)\n",
    "        x = self.prop2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = kwargs[args.model](train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgeniepath v2 不加dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5719, Val: 0.4294, Test: 0.4309\n",
      "Epoch: 02, Loss: 0.5425, Val: 0.4253, Test: 0.4313\n",
      "Epoch: 03, Loss: 0.5239, Val: 0.4462, Test: 0.4536\n",
      "Epoch: 04, Loss: 0.5020, Val: 0.5138, Test: 0.5228\n",
      "Epoch: 05, Loss: 0.4771, Val: 0.5448, Test: 0.5591\n",
      "Epoch: 06, Loss: 0.4499, Val: 0.6031, Test: 0.6126\n",
      "Epoch: 07, Loss: 0.4187, Val: 0.6335, Test: 0.6445\n",
      "Epoch: 08, Loss: 0.3823, Val: 0.6853, Test: 0.7022\n",
      "Epoch: 09, Loss: 0.3472, Val: 0.7152, Test: 0.7386\n",
      "Epoch: 10, Loss: 0.3170, Val: 0.7201, Test: 0.7456\n",
      "Epoch: 11, Loss: 0.2802, Val: 0.7506, Test: 0.7769\n",
      "Epoch: 12, Loss: 0.2661, Val: 0.7851, Test: 0.8104\n",
      "Epoch: 13, Loss: 0.2327, Val: 0.8162, Test: 0.8350\n",
      "Epoch: 14, Loss: 0.2038, Val: 0.8219, Test: 0.8468\n",
      "Epoch: 15, Loss: 0.1931, Val: 0.8384, Test: 0.8560\n",
      "Epoch: 16, Loss: 0.1793, Val: 0.8539, Test: 0.8739\n",
      "Epoch: 17, Loss: 0.1619, Val: 0.8553, Test: 0.8747\n",
      "Epoch: 18, Loss: 0.1483, Val: 0.8726, Test: 0.8937\n",
      "Epoch: 19, Loss: 0.1346, Val: 0.8810, Test: 0.9002\n",
      "Epoch: 20, Loss: 0.1223, Val: 0.8897, Test: 0.9044\n",
      "Epoch: 21, Loss: 0.1107, Val: 0.9003, Test: 0.9167\n",
      "Epoch: 22, Loss: 0.1016, Val: 0.9056, Test: 0.9277\n",
      "Epoch: 23, Loss: 0.0882, Val: 0.9050, Test: 0.9265\n",
      "Epoch: 24, Loss: 0.0842, Val: 0.9145, Test: 0.9320\n",
      "Epoch: 25, Loss: 0.0825, Val: 0.9184, Test: 0.9345\n",
      "Epoch: 26, Loss: 0.0756, Val: 0.9271, Test: 0.9421\n",
      "Epoch: 27, Loss: 0.0737, Val: 0.9135, Test: 0.9325\n",
      "Epoch: 28, Loss: 0.0717, Val: 0.9307, Test: 0.9483\n",
      "Epoch: 29, Loss: 0.0644, Val: 0.9316, Test: 0.9441\n",
      "Epoch: 30, Loss: 0.0617, Val: 0.9270, Test: 0.9436\n",
      "Epoch: 31, Loss: 0.0602, Val: 0.9350, Test: 0.9510\n",
      "Epoch: 32, Loss: 0.0523, Val: 0.9411, Test: 0.9566\n",
      "Epoch: 33, Loss: 0.0465, Val: 0.9218, Test: 0.9382\n",
      "Epoch: 34, Loss: 0.0624, Val: 0.9253, Test: 0.9450\n",
      "Epoch: 35, Loss: 0.0532, Val: 0.9398, Test: 0.9577\n",
      "Epoch: 36, Loss: 0.0433, Val: 0.9496, Test: 0.9637\n",
      "Epoch: 37, Loss: 0.0411, Val: 0.9301, Test: 0.9440\n",
      "Epoch: 38, Loss: 0.0462, Val: 0.9473, Test: 0.9623\n",
      "Epoch: 39, Loss: 0.0425, Val: 0.9480, Test: 0.9609\n",
      "Epoch: 40, Loss: 0.0371, Val: 0.9519, Test: 0.9633\n",
      "Epoch: 41, Loss: 0.0314, Val: 0.9546, Test: 0.9673\n",
      "Epoch: 42, Loss: 0.0268, Val: 0.9598, Test: 0.9721\n",
      "Epoch: 43, Loss: 0.0242, Val: 0.9588, Test: 0.9724\n",
      "Epoch: 44, Loss: 0.0224, Val: 0.9603, Test: 0.9744\n",
      "Epoch: 45, Loss: 0.0206, Val: 0.9552, Test: 0.9715\n",
      "Epoch: 46, Loss: 0.0247, Val: 0.9560, Test: 0.9657\n",
      "Epoch: 47, Loss: 0.0240, Val: 0.9547, Test: 0.9699\n",
      "Epoch: 48, Loss: 0.0207, Val: 0.9602, Test: 0.9722\n",
      "Epoch: 49, Loss: 0.0176, Val: 0.9616, Test: 0.9742\n",
      "Epoch: 50, Loss: 0.0158, Val: 0.9641, Test: 0.9752\n",
      "Epoch: 51, Loss: 0.0143, Val: 0.9643, Test: 0.9748\n",
      "Epoch: 52, Loss: 0.0141, Val: 0.9650, Test: 0.9761\n",
      "Epoch: 53, Loss: 0.0128, Val: 0.9660, Test: 0.9753\n",
      "Epoch: 54, Loss: 0.0120, Val: 0.9665, Test: 0.9768\n",
      "Epoch: 55, Loss: 0.0109, Val: 0.9667, Test: 0.9779\n",
      "Epoch: 56, Loss: 0.0107, Val: 0.9668, Test: 0.9782\n",
      "Epoch: 57, Loss: 0.0110, Val: 0.9658, Test: 0.9771\n",
      "Epoch: 58, Loss: 0.0229, Val: 0.9446, Test: 0.9558\n",
      "Epoch: 59, Loss: 0.0375, Val: 0.9393, Test: 0.9532\n",
      "Epoch: 60, Loss: 0.0485, Val: 0.9292, Test: 0.9470\n",
      "Epoch: 61, Loss: 0.0802, Val: 0.8850, Test: 0.9086\n",
      "Epoch: 62, Loss: 0.1263, Val: 0.8629, Test: 0.8935\n",
      "Epoch: 63, Loss: 0.1404, Val: 0.8830, Test: 0.9032\n",
      "Epoch: 64, Loss: 0.0916, Val: 0.9148, Test: 0.9305\n",
      "Epoch: 65, Loss: 0.0643, Val: 0.9339, Test: 0.9498\n",
      "Epoch: 66, Loss: 0.0472, Val: 0.9470, Test: 0.9626\n",
      "Epoch: 67, Loss: 0.0334, Val: 0.9521, Test: 0.9674\n",
      "Epoch: 68, Loss: 0.0250, Val: 0.9535, Test: 0.9659\n",
      "Epoch: 69, Loss: 0.0196, Val: 0.9623, Test: 0.9744\n",
      "Epoch: 70, Loss: 0.0178, Val: 0.9611, Test: 0.9743\n",
      "Epoch: 71, Loss: 0.0154, Val: 0.9635, Test: 0.9739\n",
      "Epoch: 72, Loss: 0.0133, Val: 0.9654, Test: 0.9763\n",
      "Epoch: 73, Loss: 0.0118, Val: 0.9664, Test: 0.9779\n",
      "Epoch: 74, Loss: 0.0108, Val: 0.9666, Test: 0.9766\n",
      "Epoch: 75, Loss: 0.0101, Val: 0.9663, Test: 0.9770\n",
      "Epoch: 76, Loss: 0.0093, Val: 0.9674, Test: 0.9776\n",
      "Epoch: 77, Loss: 0.0087, Val: 0.9679, Test: 0.9779\n",
      "Epoch: 78, Loss: 0.0083, Val: 0.9682, Test: 0.9785\n",
      "Epoch: 79, Loss: 0.0081, Val: 0.9677, Test: 0.9780\n",
      "Epoch: 80, Loss: 0.0079, Val: 0.9672, Test: 0.9780\n",
      "Epoch: 81, Loss: 0.0075, Val: 0.9677, Test: 0.9785\n",
      "Epoch: 82, Loss: 0.0073, Val: 0.9683, Test: 0.9786\n",
      "Epoch: 83, Loss: 0.0071, Val: 0.9683, Test: 0.9779\n",
      "Epoch: 84, Loss: 0.0070, Val: 0.9686, Test: 0.9787\n",
      "Epoch: 85, Loss: 0.0068, Val: 0.9685, Test: 0.9788\n",
      "Epoch: 86, Loss: 0.0065, Val: 0.9682, Test: 0.9784\n",
      "Epoch: 87, Loss: 0.0064, Val: 0.9686, Test: 0.9780\n",
      "Epoch: 88, Loss: 0.0063, Val: 0.9688, Test: 0.9786\n",
      "Epoch: 89, Loss: 0.0064, Val: 0.9677, Test: 0.9780\n",
      "Epoch: 90, Loss: 0.0062, Val: 0.9682, Test: 0.9790\n",
      "Epoch: 91, Loss: 0.0060, Val: 0.9688, Test: 0.9785\n",
      "Epoch: 92, Loss: 0.0058, Val: 0.9688, Test: 0.9788\n",
      "Epoch: 93, Loss: 0.0058, Val: 0.9682, Test: 0.9786\n",
      "Epoch: 94, Loss: 0.0057, Val: 0.9683, Test: 0.9784\n",
      "Epoch: 95, Loss: 0.0056, Val: 0.9688, Test: 0.9787\n",
      "Epoch: 96, Loss: 0.0055, Val: 0.9686, Test: 0.9792\n",
      "Epoch: 97, Loss: 0.0057, Val: 0.9689, Test: 0.9788\n",
      "Epoch: 98, Loss: 0.0061, Val: 0.9676, Test: 0.9782\n",
      "Epoch: 99, Loss: 0.0063, Val: 0.9673, Test: 0.9785\n",
      "Epoch: 100, Loss: 0.0060, Val: 0.9688, Test: 0.9775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import AGNNConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePathLazy')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "# class agnnn(torch.nn.Module):\n",
    "#     def __init__(self,in_dim,out_dim):\n",
    "#         super(agnnn, self).__init__()\n",
    "#         self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "#         self.prop1 = AGNNConv(requires_grad=False)\n",
    "#         self.prop2 = AGNNConv(requires_grad=True)\n",
    "#         self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x = F.dropout(data.x, training=self.training)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = self.prop1(x, edge_index)\n",
    "#         x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "        \n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, 16)\n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, out_dim)\n",
    "        # self.gatconv = AGNNConv(requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.prop1(x, edge_index)\n",
    "        x = self.prop2(x, edge_index)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        # x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
