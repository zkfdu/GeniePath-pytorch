{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.8480, Acc: 0.5114\n",
      "Epoch: 02, Loss: 0.5329, Acc: 0.5341\n",
      "Epoch: 03, Loss: 0.4896, Acc: 0.5811\n",
      "Epoch: 04, Loss: 0.4669, Acc: 0.6126\n",
      "Epoch: 05, Loss: 0.4260, Acc: 0.6444\n",
      "Epoch: 06, Loss: 0.3788, Acc: 0.7256\n",
      "Epoch: 07, Loss: 0.3422, Acc: 0.7430\n",
      "Epoch: 08, Loss: 0.3021, Acc: 0.7859\n",
      "Epoch: 09, Loss: 0.2565, Acc: 0.8366\n",
      "Epoch: 10, Loss: 0.2214, Acc: 0.8636\n",
      "Epoch: 11, Loss: 0.1873, Acc: 0.8869\n",
      "Epoch: 12, Loss: 0.1640, Acc: 0.9007\n",
      "Epoch: 13, Loss: 0.1397, Acc: 0.9145\n",
      "Epoch: 14, Loss: 0.1149, Acc: 0.9262\n",
      "Epoch: 15, Loss: 0.1034, Acc: 0.9368\n",
      "Epoch: 16, Loss: 0.0955, Acc: 0.9450\n",
      "Epoch: 17, Loss: 0.0797, Acc: 0.9514\n",
      "Epoch: 18, Loss: 0.0681, Acc: 0.9578\n",
      "Epoch: 19, Loss: 0.0582, Acc: 0.9638\n",
      "Epoch: 20, Loss: 0.0540, Acc: 0.9642\n",
      "Epoch: 21, Loss: 0.0450, Acc: 0.9650\n",
      "Epoch: 22, Loss: 0.0476, Acc: 0.9670\n",
      "Epoch: 23, Loss: 0.0461, Acc: 0.9675\n",
      "Epoch: 24, Loss: 0.0396, Acc: 0.9701\n",
      "Epoch: 25, Loss: 0.0317, Acc: 0.9766\n",
      "Epoch: 26, Loss: 0.0292, Acc: 0.9773\n",
      "Epoch: 27, Loss: 0.0261, Acc: 0.9768\n",
      "Epoch: 28, Loss: 0.0230, Acc: 0.9798\n",
      "Epoch: 29, Loss: 0.0251, Acc: 0.9754\n",
      "Epoch: 30, Loss: 0.0224, Acc: 0.9775\n",
      "Epoch: 31, Loss: 0.0215, Acc: 0.9791\n",
      "Epoch: 32, Loss: 0.0199, Acc: 0.9794\n",
      "Epoch: 33, Loss: 0.0202, Acc: 0.9760\n",
      "Epoch: 34, Loss: 0.0309, Acc: 0.9754\n",
      "Epoch: 35, Loss: 0.0289, Acc: 0.9760\n",
      "Epoch: 36, Loss: 0.0244, Acc: 0.9764\n",
      "Epoch: 37, Loss: 0.0246, Acc: 0.9787\n",
      "Epoch: 38, Loss: 0.0218, Acc: 0.9794\n",
      "Epoch: 39, Loss: 0.0164, Acc: 0.9810\n",
      "Epoch: 40, Loss: 0.0135, Acc: 0.9829\n",
      "Epoch: 41, Loss: 0.0119, Acc: 0.9837\n",
      "Epoch: 42, Loss: 0.0122, Acc: 0.9839\n",
      "Epoch: 43, Loss: 0.0119, Acc: 0.9840\n",
      "Epoch: 44, Loss: 0.0104, Acc: 0.9841\n",
      "Epoch: 45, Loss: 0.0102, Acc: 0.9844\n",
      "Epoch: 46, Loss: 0.0112, Acc: 0.9848\n",
      "Epoch: 47, Loss: 0.0098, Acc: 0.9853\n",
      "Epoch: 48, Loss: 0.0085, Acc: 0.9853\n",
      "Epoch: 49, Loss: 0.0098, Acc: 0.9852\n",
      "Epoch: 50, Loss: 0.0087, Acc: 0.9856\n",
      "Epoch: 51, Loss: 0.0083, Acc: 0.9844\n",
      "Epoch: 52, Loss: 0.0095, Acc: 0.9852\n",
      "Epoch: 53, Loss: 0.0078, Acc: 0.9857\n",
      "Epoch: 54, Loss: 0.0075, Acc: 0.9856\n",
      "Epoch: 55, Loss: 0.0079, Acc: 0.9855\n",
      "Epoch: 56, Loss: 0.0079, Acc: 0.9860\n",
      "Epoch: 57, Loss: 0.0065, Acc: 0.9859\n",
      "Epoch: 58, Loss: 0.0072, Acc: 0.9862\n",
      "Epoch: 59, Loss: 0.0062, Acc: 0.9865\n",
      "Epoch: 60, Loss: 0.0069, Acc: 0.9859\n",
      "Epoch: 61, Loss: 0.0073, Acc: 0.9865\n",
      "Epoch: 62, Loss: 0.0068, Acc: 0.9862\n",
      "Epoch: 63, Loss: 0.0066, Acc: 0.9862\n",
      "Epoch: 64, Loss: 0.0070, Acc: 0.9860\n",
      "Epoch: 65, Loss: 0.0067, Acc: 0.9860\n",
      "Epoch: 66, Loss: 0.0065, Acc: 0.9856\n",
      "Epoch: 67, Loss: 0.0063, Acc: 0.9867\n",
      "Epoch: 68, Loss: 0.0070, Acc: 0.9863\n",
      "Epoch: 69, Loss: 0.0063, Acc: 0.9863\n",
      "Epoch: 70, Loss: 0.0059, Acc: 0.9870\n",
      "Epoch: 71, Loss: 0.0056, Acc: 0.9865\n",
      "Epoch: 72, Loss: 0.0064, Acc: 0.9866\n",
      "Epoch: 73, Loss: 0.0063, Acc: 0.9865\n",
      "Epoch: 74, Loss: 0.0076, Acc: 0.9852\n",
      "Epoch: 75, Loss: 0.0109, Acc: 0.9816\n",
      "Epoch: 76, Loss: 0.0300, Acc: 0.9626\n",
      "Epoch: 77, Loss: 0.0670, Acc: 0.9490\n",
      "Epoch: 78, Loss: 0.0951, Acc: 0.9473\n",
      "Epoch: 79, Loss: 0.0716, Acc: 0.9569\n",
      "Epoch: 80, Loss: 0.0545, Acc: 0.9658\n",
      "Epoch: 81, Loss: 0.0378, Acc: 0.9714\n",
      "Epoch: 82, Loss: 0.0237, Acc: 0.9798\n",
      "Epoch: 83, Loss: 0.0178, Acc: 0.9819\n",
      "Epoch: 84, Loss: 0.0135, Acc: 0.9853\n",
      "Epoch: 85, Loss: 0.0090, Acc: 0.9863\n",
      "Epoch: 86, Loss: 0.0072, Acc: 0.9870\n",
      "Epoch: 87, Loss: 0.0064, Acc: 0.9865\n",
      "Epoch: 88, Loss: 0.0079, Acc: 0.9875\n",
      "Epoch: 89, Loss: 0.0059, Acc: 0.9877\n",
      "Epoch: 90, Loss: 0.0064, Acc: 0.9878\n",
      "Epoch: 91, Loss: 0.0070, Acc: 0.9867\n",
      "Epoch: 92, Loss: 0.0062, Acc: 0.9876\n",
      "Epoch: 93, Loss: 0.0056, Acc: 0.9884\n",
      "Epoch: 94, Loss: 0.0049, Acc: 0.9878\n",
      "Epoch: 95, Loss: 0.0049, Acc: 0.9881\n",
      "Epoch: 96, Loss: 0.0047, Acc: 0.9882\n",
      "Epoch: 97, Loss: 0.0054, Acc: 0.9879\n",
      "Epoch: 98, Loss: 0.0054, Acc: 0.9869\n",
      "Epoch: 99, Loss: 0.0081, Acc: 0.9865\n",
      "Epoch: 100, Loss: 0.0062, Acc: 0.9863\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn import metrics\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2,3'\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='test')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GATConv(train_dataset.num_features, 256, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(train_dataset.num_features, 4 * 256)\n",
    "        self.conv2 = GATConv(4 * 256, 256, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 256, 4 * 256)\n",
    "        self.conv3 = GATConv(\n",
    "            4 * 256, train_dataset.num_classes, heads=6, concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 256, train_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_micro_f1 = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        pred = (out > 0).float().cpu()\n",
    "        micro_f1 = metrics.f1_score(data.y, pred, average='micro')\n",
    "        total_micro_f1 += micro_f1 * data.num_graphs\n",
    "    return total_micro_f1 / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    acc = test(val_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}'.format(epoch, loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc04995ad923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "# from model_geniepath import GeniePath as Net\n",
    "from model_geniepath import GeniePathLazy as Net\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='test')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(train_dataset.num_features, train_dataset.num_classes, device).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_micro_f1 = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        pred = (out > 0).float().cpu()\n",
    "        micro_f1 = metrics.f1_score(data.y, pred, average='micro')\n",
    "        total_micro_f1 += micro_f1 * data.num_graphs\n",
    "    return total_micro_f1 / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    acc = test(val_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}'.format(epoch, loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Epoch: 001, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 002, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 003, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 004, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 005, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 006, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 007, Train: 0.1429, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 008, Train: 0.1643, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 009, Train: 0.1929, Val: 0.1620, Test: 0.1490\n",
      "Epoch: 010, Train: 0.2071, Val: 0.1640, Test: 0.1610\n",
      "Epoch: 011, Train: 0.2571, Val: 0.1840, Test: 0.1690\n",
      "Epoch: 012, Train: 0.3214, Val: 0.2060, Test: 0.1950\n",
      "Epoch: 013, Train: 0.4929, Val: 0.2820, Test: 0.2940\n",
      "Epoch: 014, Train: 0.5071, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 015, Train: 0.4857, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 016, Train: 0.4643, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 017, Train: 0.5000, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 018, Train: 0.5357, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 019, Train: 0.6071, Val: 0.2860, Test: 0.3180\n",
      "Epoch: 020, Train: 0.6500, Val: 0.2940, Test: 0.3350\n",
      "Epoch: 021, Train: 0.7071, Val: 0.3220, Test: 0.3730\n",
      "Epoch: 022, Train: 0.7357, Val: 0.3640, Test: 0.4140\n",
      "Epoch: 023, Train: 0.7643, Val: 0.4000, Test: 0.4530\n",
      "Epoch: 024, Train: 0.8000, Val: 0.4620, Test: 0.5100\n",
      "Epoch: 025, Train: 0.8429, Val: 0.5140, Test: 0.5560\n",
      "Epoch: 026, Train: 0.8500, Val: 0.5560, Test: 0.5980\n",
      "Epoch: 027, Train: 0.9000, Val: 0.6020, Test: 0.6390\n",
      "Epoch: 028, Train: 0.9071, Val: 0.6260, Test: 0.6590\n",
      "Epoch: 029, Train: 0.9000, Val: 0.6400, Test: 0.6730\n",
      "Epoch: 030, Train: 0.8929, Val: 0.6420, Test: 0.6830\n",
      "Epoch: 031, Train: 0.9000, Val: 0.6420, Test: 0.6830\n",
      "Epoch: 032, Train: 0.8929, Val: 0.6420, Test: 0.6830\n",
      "Epoch: 033, Train: 0.8857, Val: 0.6420, Test: 0.6830\n",
      "Epoch: 034, Train: 0.8786, Val: 0.6420, Test: 0.6830\n",
      "Epoch: 035, Train: 0.8786, Val: 0.6440, Test: 0.6710\n",
      "Epoch: 036, Train: 0.8786, Val: 0.6480, Test: 0.6760\n",
      "Epoch: 037, Train: 0.8857, Val: 0.6480, Test: 0.6760\n",
      "Epoch: 038, Train: 0.8929, Val: 0.6640, Test: 0.6770\n",
      "Epoch: 039, Train: 0.8929, Val: 0.6700, Test: 0.6710\n",
      "Epoch: 040, Train: 0.8929, Val: 0.6800, Test: 0.6850\n",
      "Epoch: 041, Train: 0.9143, Val: 0.6860, Test: 0.6900\n",
      "Epoch: 042, Train: 0.9214, Val: 0.6860, Test: 0.6900\n",
      "Epoch: 043, Train: 0.9143, Val: 0.7080, Test: 0.7130\n",
      "Epoch: 044, Train: 0.9214, Val: 0.7220, Test: 0.7230\n",
      "Epoch: 045, Train: 0.9214, Val: 0.7320, Test: 0.7350\n",
      "Epoch: 046, Train: 0.9286, Val: 0.7380, Test: 0.7400\n",
      "Epoch: 047, Train: 0.9429, Val: 0.7540, Test: 0.7600\n",
      "Epoch: 048, Train: 0.9500, Val: 0.7680, Test: 0.7690\n",
      "Epoch: 049, Train: 0.9500, Val: 0.7680, Test: 0.7690\n",
      "Epoch: 050, Train: 0.9500, Val: 0.7680, Test: 0.7690\n",
      "Epoch: 051, Train: 0.9500, Val: 0.7700, Test: 0.7880\n",
      "Epoch: 052, Train: 0.9500, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 053, Train: 0.9500, Val: 0.7760, Test: 0.7870\n",
      "Epoch: 054, Train: 0.9500, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 055, Train: 0.9500, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 056, Train: 0.9500, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 057, Train: 0.9500, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 058, Train: 0.9571, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 059, Train: 0.9571, Val: 0.7780, Test: 0.7820\n",
      "Epoch: 060, Train: 0.9500, Val: 0.7880, Test: 0.7930\n",
      "Epoch: 061, Train: 0.9500, Val: 0.7880, Test: 0.7930\n",
      "Epoch: 062, Train: 0.9571, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 063, Train: 0.9571, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 064, Train: 0.9571, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 065, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 066, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 067, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 068, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 069, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 070, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 071, Train: 0.9571, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 072, Train: 0.9500, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 073, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 074, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 075, Train: 0.9571, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 076, Train: 0.9571, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 077, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 078, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 079, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 080, Train: 0.9500, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 081, Train: 0.9571, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 082, Train: 0.9643, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 083, Train: 0.9714, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 084, Train: 0.9714, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 085, Train: 0.9643, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 086, Train: 0.9643, Val: 0.8020, Test: 0.8090\n",
      "Epoch: 087, Train: 0.9714, Val: 0.8080, Test: 0.8200\n",
      "Epoch: 088, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 089, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 090, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 091, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 092, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 093, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 094, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 095, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 096, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 097, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 098, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 099, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 100, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 101, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 102, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 103, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 104, Train: 0.9786, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 105, Train: 0.9857, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 106, Train: 0.9857, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 107, Train: 0.9857, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 108, Train: 0.9929, Val: 0.8100, Test: 0.8190\n",
      "Epoch: 109, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 110, Train: 0.9786, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 111, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 112, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 113, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 114, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 115, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 116, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 117, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 118, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 119, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 120, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 121, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 122, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 123, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 124, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 125, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 126, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 127, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 128, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 129, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 130, Train: 0.9857, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 131, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 132, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 133, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 134, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 135, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 136, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 137, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 138, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 139, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 140, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 141, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 142, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 143, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 144, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 145, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 146, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 147, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 148, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 149, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 150, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 151, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 152, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 153, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 154, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 155, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 156, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 157, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 158, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 159, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 160, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 161, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 162, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 163, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 164, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 165, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 166, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 167, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 168, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 169, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 170, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 171, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 172, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 173, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 174, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 175, Train: 1.0000, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 176, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 177, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 178, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 179, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 180, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 181, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 182, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 183, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 184, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 185, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 186, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 187, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 188, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 189, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 190, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 191, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 192, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 193, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 194, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 195, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 196, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 197, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 198, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 199, Train: 0.9929, Val: 0.8120, Test: 0.8330\n",
      "Epoch: 200, Train: 0.9929, Val: 0.8120, Test: 0.8330\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join( './', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# path = osp.join('./', 'data', 'PPI')\n",
    "# train_dataset = PPI(path, split='train')\n",
    "# val_dataset = PPI(path, split='test')\n",
    "# test_dataset = PPI(path, split='test')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(dataset.num_features, 16)\n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.dropout(data.x, training=self.training)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.prop1(x, data.edge_index)\n",
    "        x = self.prop2(x, data.edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
