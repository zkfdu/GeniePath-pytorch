{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.8480, Acc: 0.5114\n",
      "Epoch: 02, Loss: 0.5329, Acc: 0.5341\n",
      "Epoch: 03, Loss: 0.4896, Acc: 0.5811\n",
      "Epoch: 04, Loss: 0.4669, Acc: 0.6126\n",
      "Epoch: 05, Loss: 0.4260, Acc: 0.6444\n",
      "Epoch: 06, Loss: 0.3788, Acc: 0.7256\n",
      "Epoch: 07, Loss: 0.3422, Acc: 0.7430\n",
      "Epoch: 08, Loss: 0.3021, Acc: 0.7859\n",
      "Epoch: 09, Loss: 0.2565, Acc: 0.8366\n",
      "Epoch: 10, Loss: 0.2214, Acc: 0.8636\n",
      "Epoch: 11, Loss: 0.1873, Acc: 0.8869\n",
      "Epoch: 12, Loss: 0.1640, Acc: 0.9007\n",
      "Epoch: 13, Loss: 0.1397, Acc: 0.9145\n",
      "Epoch: 14, Loss: 0.1149, Acc: 0.9262\n",
      "Epoch: 15, Loss: 0.1034, Acc: 0.9368\n",
      "Epoch: 16, Loss: 0.0955, Acc: 0.9450\n",
      "Epoch: 17, Loss: 0.0797, Acc: 0.9514\n",
      "Epoch: 18, Loss: 0.0681, Acc: 0.9578\n",
      "Epoch: 19, Loss: 0.0582, Acc: 0.9638\n",
      "Epoch: 20, Loss: 0.0540, Acc: 0.9642\n",
      "Epoch: 21, Loss: 0.0450, Acc: 0.9650\n",
      "Epoch: 22, Loss: 0.0476, Acc: 0.9670\n",
      "Epoch: 23, Loss: 0.0461, Acc: 0.9675\n",
      "Epoch: 24, Loss: 0.0396, Acc: 0.9701\n",
      "Epoch: 25, Loss: 0.0317, Acc: 0.9766\n",
      "Epoch: 26, Loss: 0.0292, Acc: 0.9773\n",
      "Epoch: 27, Loss: 0.0261, Acc: 0.9768\n",
      "Epoch: 28, Loss: 0.0230, Acc: 0.9798\n",
      "Epoch: 29, Loss: 0.0251, Acc: 0.9754\n",
      "Epoch: 30, Loss: 0.0224, Acc: 0.9775\n",
      "Epoch: 31, Loss: 0.0215, Acc: 0.9791\n",
      "Epoch: 32, Loss: 0.0199, Acc: 0.9794\n",
      "Epoch: 33, Loss: 0.0202, Acc: 0.9760\n",
      "Epoch: 34, Loss: 0.0309, Acc: 0.9754\n",
      "Epoch: 35, Loss: 0.0289, Acc: 0.9760\n",
      "Epoch: 36, Loss: 0.0244, Acc: 0.9764\n",
      "Epoch: 37, Loss: 0.0246, Acc: 0.9787\n",
      "Epoch: 38, Loss: 0.0218, Acc: 0.9794\n",
      "Epoch: 39, Loss: 0.0164, Acc: 0.9810\n",
      "Epoch: 40, Loss: 0.0135, Acc: 0.9829\n",
      "Epoch: 41, Loss: 0.0119, Acc: 0.9837\n",
      "Epoch: 42, Loss: 0.0122, Acc: 0.9839\n",
      "Epoch: 43, Loss: 0.0119, Acc: 0.9840\n",
      "Epoch: 44, Loss: 0.0104, Acc: 0.9841\n",
      "Epoch: 45, Loss: 0.0102, Acc: 0.9844\n",
      "Epoch: 46, Loss: 0.0112, Acc: 0.9848\n",
      "Epoch: 47, Loss: 0.0098, Acc: 0.9853\n",
      "Epoch: 48, Loss: 0.0085, Acc: 0.9853\n",
      "Epoch: 49, Loss: 0.0098, Acc: 0.9852\n",
      "Epoch: 50, Loss: 0.0087, Acc: 0.9856\n",
      "Epoch: 51, Loss: 0.0083, Acc: 0.9844\n",
      "Epoch: 52, Loss: 0.0095, Acc: 0.9852\n",
      "Epoch: 53, Loss: 0.0078, Acc: 0.9857\n",
      "Epoch: 54, Loss: 0.0075, Acc: 0.9856\n",
      "Epoch: 55, Loss: 0.0079, Acc: 0.9855\n",
      "Epoch: 56, Loss: 0.0079, Acc: 0.9860\n",
      "Epoch: 57, Loss: 0.0065, Acc: 0.9859\n",
      "Epoch: 58, Loss: 0.0072, Acc: 0.9862\n",
      "Epoch: 59, Loss: 0.0062, Acc: 0.9865\n",
      "Epoch: 60, Loss: 0.0069, Acc: 0.9859\n",
      "Epoch: 61, Loss: 0.0073, Acc: 0.9865\n",
      "Epoch: 62, Loss: 0.0068, Acc: 0.9862\n",
      "Epoch: 63, Loss: 0.0066, Acc: 0.9862\n",
      "Epoch: 64, Loss: 0.0070, Acc: 0.9860\n",
      "Epoch: 65, Loss: 0.0067, Acc: 0.9860\n",
      "Epoch: 66, Loss: 0.0065, Acc: 0.9856\n",
      "Epoch: 67, Loss: 0.0063, Acc: 0.9867\n",
      "Epoch: 68, Loss: 0.0070, Acc: 0.9863\n",
      "Epoch: 69, Loss: 0.0063, Acc: 0.9863\n",
      "Epoch: 70, Loss: 0.0059, Acc: 0.9870\n",
      "Epoch: 71, Loss: 0.0056, Acc: 0.9865\n",
      "Epoch: 72, Loss: 0.0064, Acc: 0.9866\n",
      "Epoch: 73, Loss: 0.0063, Acc: 0.9865\n",
      "Epoch: 74, Loss: 0.0076, Acc: 0.9852\n",
      "Epoch: 75, Loss: 0.0109, Acc: 0.9816\n",
      "Epoch: 76, Loss: 0.0300, Acc: 0.9626\n",
      "Epoch: 77, Loss: 0.0670, Acc: 0.9490\n",
      "Epoch: 78, Loss: 0.0951, Acc: 0.9473\n",
      "Epoch: 79, Loss: 0.0716, Acc: 0.9569\n",
      "Epoch: 80, Loss: 0.0545, Acc: 0.9658\n",
      "Epoch: 81, Loss: 0.0378, Acc: 0.9714\n",
      "Epoch: 82, Loss: 0.0237, Acc: 0.9798\n",
      "Epoch: 83, Loss: 0.0178, Acc: 0.9819\n",
      "Epoch: 84, Loss: 0.0135, Acc: 0.9853\n",
      "Epoch: 85, Loss: 0.0090, Acc: 0.9863\n",
      "Epoch: 86, Loss: 0.0072, Acc: 0.9870\n",
      "Epoch: 87, Loss: 0.0064, Acc: 0.9865\n",
      "Epoch: 88, Loss: 0.0079, Acc: 0.9875\n",
      "Epoch: 89, Loss: 0.0059, Acc: 0.9877\n",
      "Epoch: 90, Loss: 0.0064, Acc: 0.9878\n",
      "Epoch: 91, Loss: 0.0070, Acc: 0.9867\n",
      "Epoch: 92, Loss: 0.0062, Acc: 0.9876\n",
      "Epoch: 93, Loss: 0.0056, Acc: 0.9884\n",
      "Epoch: 94, Loss: 0.0049, Acc: 0.9878\n",
      "Epoch: 95, Loss: 0.0049, Acc: 0.9881\n",
      "Epoch: 96, Loss: 0.0047, Acc: 0.9882\n",
      "Epoch: 97, Loss: 0.0054, Acc: 0.9879\n",
      "Epoch: 98, Loss: 0.0054, Acc: 0.9869\n",
      "Epoch: 99, Loss: 0.0081, Acc: 0.9865\n",
      "Epoch: 100, Loss: 0.0062, Acc: 0.9863\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn import metrics\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2,3'\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='test')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GATConv(train_dataset.num_features, 256, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(train_dataset.num_features, 4 * 256)\n",
    "        self.conv2 = GATConv(4 * 256, 256, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 256, 4 * 256)\n",
    "        self.conv3 = GATConv(\n",
    "            4 * 256, train_dataset.num_classes, heads=6, concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 256, train_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_micro_f1 = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        pred = (out > 0).float().cpu()\n",
    "        micro_f1 = metrics.f1_score(data.y, pred, average='micro')\n",
    "        total_micro_f1 += micro_f1 * data.num_graphs\n",
    "    return total_micro_f1 / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    acc = test(val_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}'.format(epoch, loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geniepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5796, Val: 0.3726, Test: 0.3727\n",
      "Epoch: 02, Loss: 0.5505, Val: 0.4225, Test: 0.4226\n",
      "Epoch: 03, Loss: 0.5436, Val: 0.4197, Test: 0.4198\n",
      "Epoch: 04, Loss: 0.5390, Val: 0.4408, Test: 0.4422\n",
      "Epoch: 05, Loss: 0.5395, Val: 0.4512, Test: 0.4516\n",
      "Epoch: 06, Loss: 0.5347, Val: 0.4398, Test: 0.4411\n",
      "Epoch: 07, Loss: 0.5354, Val: 0.4734, Test: 0.4750\n",
      "Epoch: 08, Loss: 0.5328, Val: 0.4612, Test: 0.4622\n",
      "Epoch: 09, Loss: 0.5287, Val: 0.5067, Test: 0.5108\n",
      "Epoch: 10, Loss: 0.5286, Val: 0.4869, Test: 0.4887\n",
      "Epoch: 11, Loss: 0.5252, Val: 0.4782, Test: 0.4800\n",
      "Epoch: 12, Loss: 0.5222, Val: 0.5073, Test: 0.5099\n",
      "Epoch: 13, Loss: 0.5256, Val: 0.4872, Test: 0.4897\n",
      "Epoch: 14, Loss: 0.5218, Val: 0.4888, Test: 0.4923\n",
      "Epoch: 15, Loss: 0.5209, Val: 0.4841, Test: 0.4873\n",
      "Epoch: 16, Loss: 0.5202, Val: 0.4875, Test: 0.4926\n",
      "Epoch: 17, Loss: 0.5213, Val: 0.4509, Test: 0.4559\n",
      "Epoch: 18, Loss: 0.5230, Val: 0.4731, Test: 0.4776\n",
      "Epoch: 19, Loss: 0.5178, Val: 0.5229, Test: 0.5252\n",
      "Epoch: 20, Loss: 0.5172, Val: 0.4859, Test: 0.4896\n",
      "Epoch: 21, Loss: 0.5151, Val: 0.4961, Test: 0.5002\n",
      "Epoch: 22, Loss: 0.5122, Val: 0.4968, Test: 0.5021\n",
      "Epoch: 23, Loss: 0.5155, Val: 0.4753, Test: 0.4809\n",
      "Epoch: 24, Loss: 0.5210, Val: 0.4920, Test: 0.4968\n",
      "Epoch: 25, Loss: 0.5174, Val: 0.4739, Test: 0.4801\n",
      "Epoch: 26, Loss: 0.5142, Val: 0.4971, Test: 0.5037\n",
      "Epoch: 27, Loss: 0.5127, Val: 0.4962, Test: 0.5031\n",
      "Epoch: 28, Loss: 0.5107, Val: 0.4963, Test: 0.5039\n",
      "Epoch: 29, Loss: 0.5125, Val: 0.4943, Test: 0.5011\n",
      "Epoch: 30, Loss: 0.5142, Val: 0.5002, Test: 0.5064\n",
      "Epoch: 31, Loss: 0.5106, Val: 0.5320, Test: 0.5370\n",
      "Epoch: 32, Loss: 0.5083, Val: 0.5105, Test: 0.5145\n",
      "Epoch: 33, Loss: 0.5098, Val: 0.4778, Test: 0.4846\n",
      "Epoch: 34, Loss: 0.5075, Val: 0.5135, Test: 0.5218\n",
      "Epoch: 35, Loss: 0.5035, Val: 0.5288, Test: 0.5359\n",
      "Epoch: 36, Loss: 0.5011, Val: 0.5018, Test: 0.5102\n",
      "Epoch: 37, Loss: 0.5052, Val: 0.4754, Test: 0.4789\n",
      "Epoch: 38, Loss: 0.5085, Val: 0.5051, Test: 0.5131\n",
      "Epoch: 39, Loss: 0.5027, Val: 0.5239, Test: 0.5304\n",
      "Epoch: 40, Loss: 0.4992, Val: 0.5269, Test: 0.5303\n",
      "Epoch: 41, Loss: 0.4936, Val: 0.5218, Test: 0.5304\n",
      "Epoch: 42, Loss: 0.4910, Val: 0.5287, Test: 0.5371\n",
      "Epoch: 43, Loss: 0.4921, Val: 0.5464, Test: 0.5508\n",
      "Epoch: 44, Loss: 0.4925, Val: 0.5334, Test: 0.5405\n",
      "Epoch: 45, Loss: 0.4915, Val: 0.5540, Test: 0.5579\n",
      "Epoch: 46, Loss: 0.4888, Val: 0.5397, Test: 0.5455\n",
      "Epoch: 47, Loss: 0.4877, Val: 0.5465, Test: 0.5529\n",
      "Epoch: 48, Loss: 0.4825, Val: 0.5359, Test: 0.5418\n",
      "Epoch: 49, Loss: 0.4870, Val: 0.5593, Test: 0.5666\n",
      "Epoch: 50, Loss: 0.4930, Val: 0.5352, Test: 0.5389\n",
      "Epoch: 51, Loss: 0.4869, Val: 0.5630, Test: 0.5683\n",
      "Epoch: 52, Loss: 0.4865, Val: 0.5405, Test: 0.5478\n",
      "Epoch: 53, Loss: 0.4774, Val: 0.5661, Test: 0.5728\n",
      "Epoch: 54, Loss: 0.4730, Val: 0.5632, Test: 0.5722\n",
      "Epoch: 55, Loss: 0.4692, Val: 0.5608, Test: 0.5718\n",
      "Epoch: 56, Loss: 0.4659, Val: 0.5597, Test: 0.5687\n",
      "Epoch: 57, Loss: 0.4665, Val: 0.5511, Test: 0.5576\n",
      "Epoch: 58, Loss: 0.4741, Val: 0.5512, Test: 0.5597\n",
      "Epoch: 59, Loss: 0.4711, Val: 0.5566, Test: 0.5634\n",
      "Epoch: 60, Loss: 0.4713, Val: 0.5598, Test: 0.5712\n",
      "Epoch: 61, Loss: 0.4710, Val: 0.5773, Test: 0.5884\n",
      "Epoch: 62, Loss: 0.4817, Val: 0.5610, Test: 0.5677\n",
      "Epoch: 63, Loss: 0.4819, Val: 0.5706, Test: 0.5778\n",
      "Epoch: 64, Loss: 0.4830, Val: 0.5218, Test: 0.5332\n",
      "Epoch: 65, Loss: 0.4782, Val: 0.5519, Test: 0.5631\n",
      "Epoch: 66, Loss: 0.4763, Val: 0.5307, Test: 0.5439\n",
      "Epoch: 67, Loss: 0.4955, Val: 0.5255, Test: 0.5331\n",
      "Epoch: 68, Loss: 0.5064, Val: 0.5118, Test: 0.5145\n",
      "Epoch: 69, Loss: 0.5381, Val: 0.4883, Test: 0.4892\n",
      "Epoch: 70, Loss: 0.5385, Val: 0.4556, Test: 0.4543\n",
      "Epoch: 71, Loss: 0.5372, Val: 0.4863, Test: 0.4817\n",
      "Epoch: 72, Loss: 0.5300, Val: 0.5159, Test: 0.5152\n",
      "Epoch: 73, Loss: 0.5225, Val: 0.5252, Test: 0.5215\n",
      "Epoch: 74, Loss: 0.5178, Val: 0.5052, Test: 0.5044\n",
      "Epoch: 75, Loss: 0.5137, Val: 0.4776, Test: 0.4771\n",
      "Epoch: 76, Loss: 0.5123, Val: 0.5139, Test: 0.5181\n",
      "Epoch: 77, Loss: 0.5132, Val: 0.5401, Test: 0.5446\n",
      "Epoch: 78, Loss: 0.5119, Val: 0.4991, Test: 0.5031\n",
      "Epoch: 79, Loss: 0.5066, Val: 0.5232, Test: 0.5233\n",
      "Epoch: 80, Loss: 0.5028, Val: 0.5192, Test: 0.5228\n",
      "Epoch: 81, Loss: 0.4986, Val: 0.5073, Test: 0.5111\n",
      "Epoch: 82, Loss: 0.4939, Val: 0.5258, Test: 0.5343\n",
      "Epoch: 83, Loss: 0.4928, Val: 0.5342, Test: 0.5413\n",
      "Epoch: 84, Loss: 0.4922, Val: 0.5345, Test: 0.5396\n",
      "Epoch: 85, Loss: 0.5002, Val: 0.5010, Test: 0.5022\n",
      "Epoch: 86, Loss: 0.5016, Val: 0.5102, Test: 0.5170\n",
      "Epoch: 87, Loss: 0.4972, Val: 0.5504, Test: 0.5557\n",
      "Epoch: 88, Loss: 0.4920, Val: 0.5402, Test: 0.5476\n",
      "Epoch: 89, Loss: 0.4929, Val: 0.5475, Test: 0.5568\n",
      "Epoch: 90, Loss: 0.4918, Val: 0.5318, Test: 0.5407\n",
      "Epoch: 91, Loss: 0.4885, Val: 0.5272, Test: 0.5353\n",
      "Epoch: 92, Loss: 0.4907, Val: 0.5184, Test: 0.5243\n",
      "Epoch: 93, Loss: 0.4965, Val: 0.5278, Test: 0.5348\n",
      "Epoch: 94, Loss: 0.5089, Val: 0.5082, Test: 0.5106\n",
      "Epoch: 95, Loss: 0.5227, Val: 0.4758, Test: 0.4730\n",
      "Epoch: 96, Loss: 0.5170, Val: 0.5052, Test: 0.5078\n",
      "Epoch: 97, Loss: 0.5054, Val: 0.5337, Test: 0.5357\n",
      "Epoch: 98, Loss: 0.5011, Val: 0.5321, Test: 0.5348\n",
      "Epoch: 99, Loss: 0.5010, Val: 0.4872, Test: 0.4918\n",
      "Epoch: 100, Loss: 0.5022, Val: 0.5202, Test: 0.5201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54min 47s, sys: 4min 38s, total: 59min 25s\n",
      "Wall time: 4min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2406521"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, default='GeniePath')\n",
    "# args = parser.parse_args()\n",
    "# assert args.model in ['GeniePath', 'GeniePathLazy']\n",
    "\n",
    "path = osp.join('./', 'data', 'PPI')\n",
    "train_dataset = PPI(path, split='train')\n",
    "val_dataset = PPI(path, split='val')\n",
    "test_dataset = PPI(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4\n",
    "\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = GATConv(in_dim, out_dim, heads=1)#这里in_dim和out_dim都=dim=256\n",
    "        # self.gatconv = GATConv(256, 256, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePathLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(GeniePathLayer, self).__init__()\n",
    "        self.breadth_func = Breadth(in_dim, dim)\n",
    "        self.depth_func = Depth(dim, lstm_hidden)\n",
    "\n",
    "    def forward(self, x, edge_index, h, c):\n",
    "        x = self.breadth_func(x, edge_index)\n",
    "        x = x[None, :]\n",
    "        x, (h, c) = self.depth_func(x, h, c)\n",
    "        x = x[0]\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "class GeniePath(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePath, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.gplayers = torch.nn.ModuleList(\n",
    "            [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        for i, l in enumerate(self.gplayers):\n",
    "            x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePath(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hhh=model(data.x, data.edge_index)\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n",
    "\n",
    "losslist=[]\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    losslist.append(loss)\n",
    "    val_f1 = test(val_loader)\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "        epoch, loss, val_f1, test_f1))\n",
    "from matplotlib import pyplot as plt \n",
    "# %matplotlib inline\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()\n",
    "sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
